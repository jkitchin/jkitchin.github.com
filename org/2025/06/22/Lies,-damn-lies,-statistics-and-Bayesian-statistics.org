* Lies, damn lies, statistics and Bayesian statistics
:PROPERTIES:
:categories: machine-learning
:date:     2025/06/22 11:14:23
:updated:  2025/06/22 11:14:23
:org-url:  https://kitchingroup.cheme.cmu.edu/org/2025/06/22/Lies,-damn-lies,-statistics-and-Bayesian-statistics.org
:permalink: https://kitchingroup.cheme.cmu.edu/blog/2025/06/22/Lies,-damn-lies,-statistics-and-Bayesian-statistics/index.html
:END:

This post on LinkedIn (https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI) reminded me of a quip I often make of "Lies, damn lies, statistics, and Bayesian statistics". I am frequently skeptical of claims about "Bayesian something something", especially when the claim is about uncertainty quantification. That skepticism comes from practical experience of mine that "Bayesian something something" is rarely as well behaved and informative as advertised (in my hands of course).

To illustrate, I will use some noisy, 1d data from a Lennard-Jones function and Gaussian process regression to fit the data.

** The data

We get our data by sampling a Lennard-Jones function, adding some noise, and removing a gap in the data. The gap in the middle might be classically considered an interpolation region.

#+BEGIN_SRC jupyter-python
import numpy as np
import matplotlib.pyplot as plt

r = np.linspace(0.95, 3, 200)

eps, sig = 1, 1
y = 4 * eps * ((1 / r)**12 - (1 / r)**6) + np.random.normal(0, 0.03, size=r.shape)


ind = ((r > 1) & (r < 1.25)) | ((r > 2) & (r < 2.5))
_R = r[ind][:, None]
_y = y[ind]
plt.plot(_R, _y, '.')
plt.xlabel('R')
plt.ylabel('E');
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/d53ba27c2f085871cd2832e3cf7a8256b00185dc.png]]

*** GPR with a RBF kernel

The RBF kernel is the most standard kernel. It does an ok job fitting here, although I see evidence of overfitting (the wiggles are caused by the noise). You can reduce the overfitting by using a larger ~alpha~ value in the gpr, but that requires you to know in advance how smooth it should be.

#+BEGIN_SRC jupyter-python
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, WhiteKernel
kernel = RBF() + WhiteKernel()
gpr = GaussianProcessRegressor(kernel=kernel,
                               random_state=0, normalize_y=True).fit(_R, _y)

plt.plot(_R, _y, 'b.')
plt.plot(r, y, 'b.', alpha=0.2)

yp, se = gpr.predict(r[:, None], return_std=True)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, 'k--', r, yp - 2 * se, 'k--');
plt.plot(_R, _y, '.')
plt.xlabel('R')
plt.ylabel('E');

gpr.kernel_
#+END_SRC

#+RESULTS:
:RESULTS:
: RBF(length_scale=0.0773) + WhiteKernel(noise_level=0.00374)
[[file:./.ob-jupyter/2157ad1d8cabe934b169557f729ee9af1d0d22e9.png]]
:END:

The uncertainty here is primarily related to the model, i.e. it is constrained to be correct where there is data, but with no data, the model is not the right one.

The model does well in the region where there is data, but is qualitatively wrong in the gap (even though classically this would be considered interpolation), and overestimates the uncertainty in this region. The problem is the covariance kernel decays to 0 about two length scales away from the last point, which means there is no data to inform what the weights in that region should look like.  That causes the model to revert to the mean of the data.

#+BEGIN_SRC jupyter-python
gpr.predict([[1.8]]), gpr.predict([[3.0]]), np.mean(_y)
#+END_SRC

#+RESULTS:
| array | ((-0.28266893)) | array | ((-0.28791865)) | -0.2879186522487767 |

This kernel simply cannot be used for extrapolation, or any predictions more than about two length scales away from the nearest point. Calling it Bayesian doesn't make it better. For similar reasons, this model will not work well outside the data range.

A practical person would still consider using this model, and might even rely on the uncertainty being too large to identify regions of low reliability.

*** a better kernel solves these issues

Not all is lost, if we know more. In this case we can construct a kernel that reflects our understanding that the data came from a Lennard Jones like interaction model. You can construct kernels by adding and multiplying kernels. Here we consider a linear kernel, the =DotProduct= kernel, and construct a new kernel that is a sum of the linear kernel to the 12^{th} power, a linear kernel to the 6^{th} power, and a ~WhiteKernel~ for the noise. It is a little subtle that this kernel should work in $1 / r$ space, so in addition to kernel engineering, we also do feature engineering.

#+BEGIN_SRC jupyter-python
from sklearn.gaussian_process.kernels import DotProduct

kernel = DotProduct()**12 + DotProduct()**6 +  WhiteKernel()
gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True).fit(1 / _R, _y)

plt.plot(_R, _y, 'b.')
plt.plot(r, y, 'b.', alpha=0.2)


yp, se = gpr.predict(1 / r[:, None], return_std=True)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, 'k--', r, yp - 2 * se, 'k--');

plt.xlabel('R')
plt.ylabel('E');

gpr.kernel_
#+END_SRC

#+RESULTS:
:RESULTS:
: DotProduct(sigma_0=0.157) ** 12 + DotProduct(sigma_0=0.991) ** 6 + WhiteKernel(noise_level=0.00641)
[[file:./.ob-jupyter/366a4d22ea40b1ec3b8fec4411230af7db8713a2.png]]
:END:

Note that this GPR does fine in the gap, including the right level of uncertainty there. This model is better because we used the kernel to constrain what forms the model can have. This model actually extrapolates correctly outside the data. It is worth noting that although this model has great predictive and UQ properties, it does not tell us anything about the values of \epsilon and \sigma in the Lennard Jones model. Although we might say the kernel is physics-based, i.e. it is based on the relevant features and equation, it does not have physical parameters in it.



** Summary

Naive use of GPR can provide useful models when you have enough data, but these models likely do not accurately capture uncertainty outside that data, nor is it likely they are reliable in extrapolation. It is possible to do better than this, when you know what to do. Through feature and kernel engineering, you can sometimes create situations where the problem essentially becomes linear regression, where a simple linear kernel is what you want, or you develop a kernel that represents the underlying model. Kernel engineering is generally hard, with limited opportunities to be flexible. See https://www.cs.toronto.edu/~duvenaud/cookbook/ for examples of kernels and combining them.

You can see it is not adequate to say "we used Gaussian process regression". That is about as informative as saying linear regression without identifying the features, or nonlinear regression and not saying what model. You have to be specific about the kernel, and thoughtful about how you know if a prediction is reliable or not. Just because you get an uncertainty prediction doesn't mean its right.



