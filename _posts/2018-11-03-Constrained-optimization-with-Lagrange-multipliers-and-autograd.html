---
title: Constrained optimization with Lagrange multipliers and autograd
date: 2018/11/03 09:39:20
updated: 2018/11/03 09:39:20
categories: autograd,optimization
tags: 
---


<p>
Constrained optimization is common in engineering problems solving. A prototypical example (from Greenberg, Advanced Engineering Mathematics, Ch 13.7) is to find the point on a plane that is closest to the origin. The plane is defined by the equation \(2x - y + z = 3\), and we seek to minimize \(x^2 + y^2 + z^2\) subject to the equality constraint defined by the plane. <code>scipy.optimize.minimize</code> provides a pretty convenient interface to solve a problem like this, ans shown here.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">import</span> numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> scipy.optimize <span style="color: #0000FF;">import</span> minimize

<span style="color: #0000FF;">def</span> <span style="color: #006699;">objective</span>(X):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">x</span>, <span style="color: #BA36A5;">y</span>, <span style="color: #BA36A5;">z</span> = X
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> x**2 + y**2 + z**2

<span style="color: #0000FF;">def</span> <span style="color: #006699;">eq</span>(X):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">x</span>, <span style="color: #BA36A5;">y</span>, <span style="color: #BA36A5;">z</span> = X
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> 2 * x - y + z - 3

<span style="color: #BA36A5;">sol</span> = minimize(objective, [1, -0.5, 0.5], constraints={<span style="color: #008000;">'type'</span>: <span style="color: #008000;">'eq'</span>, <span style="color: #008000;">'fun'</span>: eq})
sol
</pre>
</div>

<pre class="example">
    fun: 1.5
    jac: array([ 2.00000001, -0.99999999,  1.00000001])
message: 'Optimization terminated successfully.'
   nfev: 5
    nit: 1
   njev: 1
 status: 0
success: True
      x: array([ 1. , -0.5,  0.5])

</pre>

<p>
I like the minimize function a lot, although I am not crazy for how the constraints are provided. The alternative used to be that there was an argument for equality constraints and another for inequality constraints. Analogous to <code>scipy.integrate.solve_ivp</code> event functions, they could have also used function attributes.
</p>

<p>
Sometimes, it might be desirable to go back to basics though, especially if you are unaware of the <code>minimize</code> function or perhaps suspect it is not working right and want an independent answer. Next we look at how to construct this constrained optimization problem using Lagrange multipliers. This converts the problem into an augmented unconstrained optimization problem we can use <code>fsolve</code> on. The gist of this method is we formulate a new problem:
</p>

<p>
\(F(X) = f(X) - \lambda g(X)\)
</p>

<p>
and then solve the simultaneous resulting equations:
</p>

<p>
\(F_x(X) = F_y(X) = F_z(X) = g(X) = 0\) where \(F_x\) is the derivative of \(f*\) with respect to \(x\), and \(g(X)\) is the equality constraint written so it is equal to zero. Since we end up with four equations that equal zero, we can simply use fsolve to get the solution. Many <a href="http://kitchingroup.cheme.cmu.edu/blog/2013/02/03/Using-Lagrange-multipliers-in-optimization/">years ago</a> I used a finite difference approximation to the derivatives. Today we use autograd to get the desired derivatives. Here it is.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> grad

<span style="color: #0000FF;">def</span> <span style="color: #006699;">F</span>(L):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">'Augmented Lagrange function'</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">x</span>, <span style="color: #BA36A5;">y</span>, <span style="color: #BA36A5;">z</span>, <span style="color: #BA36A5;">_lambda</span> = L
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> objective([x, y, z]) - _lambda * eq([x, y, z])

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Gradients of the Lagrange function</span>
<span style="color: #BA36A5;">dfdL</span> = grad(F, 0)

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Find L that returns all zeros in this function.</span>
<span style="color: #0000FF;">def</span> <span style="color: #006699;">obj</span>(L):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">x</span>, <span style="color: #BA36A5;">y</span>, <span style="color: #BA36A5;">z</span>, <span style="color: #BA36A5;">_lambda</span> = L
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">dFdx</span>, <span style="color: #BA36A5;">dFdy</span>, <span style="color: #BA36A5;">dFdz</span>, <span style="color: #BA36A5;">dFdlam</span> = dfdL(L)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> [dFdx, dFdy, dFdz, eq([x, y, z])]

<span style="color: #0000FF;">from</span> scipy.optimize <span style="color: #0000FF;">import</span> fsolve
<span style="color: #BA36A5;">x</span>, <span style="color: #BA36A5;">y</span>, <span style="color: #BA36A5;">z</span>, <span style="color: #BA36A5;">_lam</span> = fsolve(obj, [0.0, 0.0, 0.0, 1.0])
<span style="color: #0000FF;">print</span>(f<span style="color: #008000;">'The answer is at {x, y, z}'</span>)
</pre>
</div>

<pre class="example">
The answer is at (1.0, -0.5, 0.5)


</pre>

<p>
That is the same answer as before. Note we have still relied on some black box solver inside of fsolve (instead of inside minimize), but it might be more clear what problem we are solving (e.g. finding zeros). It takes a bit more work to set this up, since we have to construct the augmented function, but autograd makes it pretty convenient to set up the final objective function we want to solve.
</p>

<p>
How do we know we are at a minimum? We can check that the Hessian is positive definite in the original function we wanted to minimize. You can see here the array is positive definite, e.g. all the eigenvalues are positive. autograd makes this easy too.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> hessian
<span style="color: #BA36A5;">h</span> = hessian(objective, 0)
h(np.array([x, y, z]))
</pre>
</div>

<pre class="example">
array([[ 2.,  0.,  0.],
       [ 0.,  2.,  0.],
       [ 0.,  0.,  2.]])

</pre>

<p>
In case it isn't evident from that structure that the eigenvalues are all positive, here we compute them:
</p>

<div class="org-src-container">
<pre class="src src-ipython">np.linalg.eig(h(np.array([x, y, z])))[0]
</pre>
</div>

<pre class="example">
array([ 2.,  2.,  2.])

</pre>

<p>
In summary, autograd continues to enable advanced engineering problems to be solved.
</p>
<p>Copyright (C) 2018 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd.org">org-mode source</a></p>
<p>Org-mode version = 9.1.14</p>