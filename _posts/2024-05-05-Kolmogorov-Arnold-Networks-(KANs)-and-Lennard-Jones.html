---
title: Kolmogorov-Arnold Networks (KANs) and Lennard Jones
date: 2024/05/05 11:06:22
updated: 2024/05/05 11:06:22
categories: 
tags: 
---


<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org6ed4d7a">1. Create a dataset</a></li>
<li><a href="#orgca2cb78">2. Create and train the model</a></li>
</ul>
</div>
</div>
<p>
KANs have been a hot topic of discussion recently (<a href="https://arxiv.org/abs/2404.19756">https://arxiv.org/abs/2404.19756</a>). Here I explore using them as an alternative to a neural network for a simple atomistic potential using Lennard Jones data. I adapted this code from  <a href="https://github.com/KindXiaoming/pykan/blob/master/hellokan.ipynb">https://github.com/KindXiaoming/pykan/blob/master/hellokan.ipynb</a>. 
</p>

<p>
TL;DR It was easy to make the model, and it fit this simple data very well. It does not extrapolate in this example, and it is not obvious what the extrapolation behavior should be.
</p>

<div id="outline-container-org6ed4d7a" class="outline-2">
<h2 id="org6ed4d7a"><span class="section-number-2">1.</span> Create a dataset</h2>
<div class="outline-text-2" id="text-1">
<p>
We leverage the <code>create_dataset</code> function to generate the dataset here. I chose a range with some modest nonlinearity, and the minimum.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt
<span style="color: #0000FF;">import</span> torch
<span style="color: #0000FF;">from</span> kan <span style="color: #0000FF;">import</span> create_dataset, KAN

<span style="color: #0000FF;">def</span> <span style="color: #006699;">LJ</span>(r):
    <span style="color: #BA36A5;">r6</span> = r**6
    <span style="color: #0000FF;">return</span> 1 / r6**2 - 1 / r6

<span style="color: #BA36A5;">dataset</span> = create_dataset(LJ, n_var=1, ranges=[0.95, 2.0],
                         train_num=50)

plt.plot(dataset[<span style="color: #008000;">'train_input'</span>], dataset[<span style="color: #008000;">'train_label'</span>], <span style="color: #008000;">'b.'</span>)
plt.xlabel(<span style="color: #008000;">'r'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/0db7627856ef3cacbeb19cba9e64a53fb49bf422.png"></figure> 
</p>
</div>
</div>


<div id="outline-container-orgca2cb78" class="outline-2">
<h2 id="orgca2cb78"><span class="section-number-2">2.</span> Create and train the model</h2>
<div class="outline-text-2" id="text-2">
<p>
We start by making the model. We are going to model a Lennard-Jones potential with one input, the distance between two atoms, and one output. We start with a width of 2 "neurons".
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">model</span> = KAN(width=[1, 2, 1])
</pre>
</div>

<p>
Training is easy. You can even run this cell several times.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">model.train(dataset, opt=<span style="color: #008000;">"LBFGS"</span>, steps=20);

model.plot()
</pre>
</div>

<pre class="example">
train loss: 1.64e-04 | test loss: 1.46e-02 | reg: 6.72e+00 : 100%|██| 20/20 [00:03&lt;00:00,  5.61it/s]

</pre>

<p>
<figure><img src="/media/0cea2b134045cc964f990ac28b524c32d441976b.png"></figure> 
</p>


<p>
We can see here that the fit looks very good.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">X</span> = torch.linspace(dataset[<span style="color: #008000;">'train_input'</span>].<span style="color: #006FE0;">min</span>(),
                   dataset[<span style="color: #008000;">'train_input'</span>].<span style="color: #006FE0;">max</span>(), 100)[:, <span style="color: #D0372D;">None</span>]

plt.plot(dataset[<span style="color: #008000;">'train_input'</span>], dataset[<span style="color: #008000;">'train_label'</span>], <span style="color: #008000;">'b.'</span>, label=<span style="color: #008000;">'data'</span>)

plt.plot(X, model(X).detach().numpy(), <span style="color: #008000;">'r-'</span>, label=<span style="color: #008000;">'fit'</span>)
plt.legend()
plt.xlabel(<span style="color: #008000;">'r'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/24eddff0ce69063a1aaabc80060e78b56ecef0b5.png"></figure> 
</p>

<p>
KANs do not save us from extrapolation issues though. I think a downside of KANs is it is not obvious what extrapolation behavior to expect. I guess it could be related to what happens in the spline representation of the functions. Eventually those have to extrapolate too.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">X</span> = torch.linspace(0, 5, 1000)[:, <span style="color: #D0372D;">None</span>]
plt.plot(dataset[<span style="color: #008000;">'train_input'</span>], dataset[<span style="color: #008000;">'train_label'</span>], <span style="color: #008000;">'b.'</span>)
plt.plot(X, model(X).detach().numpy(), <span style="color: #008000;">'r-'</span>);
</pre>
</div>

<p>
<figure><img src="/media/a16818596b6a60ea026406808143fcddcfae54f9.png"></figure> 
</p>


<p>
It is early days for KANs, so many things we know about MLPs are still unknown for KANs. For example, with MLPs we know they extrapolate like the activation functions. Probably there is some insight like that to be had here, but it needs to be uncovered. With MLPs there are a lot of ways to regularize them for desired behavior. Probably that is true here too, and will be discovered. Similarly, there are many ways people have approached uncertainty quantification in MLPs that probably have some analog in KANs. 
Still, the ease of use suggests it could be promising for some applications.
</p>
</div>
</div>
<p>Copyright (C) 2024 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2024/05/05/Kolmogorov-Arnold-Networks-(KANs)-and-Lennard-Jones.org">org-mode source</a></p>
<p>Org-mode version = 9.7-pre</p>