---
title: Sensitivity analysis with odeint and autograd
date: 2019/09/13 09:56:09
updated: 2019/09/13 09:56:09
categories: autograd,ode
tags: 
---


<p>
In this <a href="http://kitchingroup.cheme.cmu.edu/blog/2018/10/11/A-differentiable-ODE-integrator-for-sensitivity-analysis/">previous post</a> I showed a way to do sensitivity analysis of the solution of a differential equation to parameters in the equation using autograd. The basic approach was to write a differentiable integrator, and then use it in a function so that autograd could take the derivative.
</p>

<p>
Since that time, autograd has added <a href="https://github.com/HIPS/autograd/blob/master/autograd/scipy/integrate.py">derivative support</a> for <code>scipy.integrate.odeint</code>. In this post we examine that. As usual with autograd, we have to import the autograd version of numpy, and the autograd version of odeint. We will find the derivative of the solution to an ODE (which is an array) so we need to also import the jacobian function. Finally, there is a subtle, and non-obvious requirement that we need to import the autograd tuple. That ensures that the variables are differentiable through the tuple we will use for the arguments.
</p>

<p>
The differential equation we solve returns the concentration of a species as a function of time, and the solution depends on two parameters, i.e. \(C = f(t; k_1, k_{-1})\), and we are interested in the time-dependent sensitivity of \(C\) with respect to those parameters. The approach we use is to define a function that has those parameters as arguments. The function will solve the ODE and return the time-dependent solution. First we make that solution, mostly to see that the autograd version of odeint works.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> autograd.scipy.integrate <span style="color: #0000FF;">import</span> odeint
<span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> jacobian
<span style="color: #0000FF;">from</span> autograd.builtins <span style="color: #0000FF;">import</span> <span style="color: #006FE0;">tuple</span>

<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

<span style="color: #BA36A5;">Ca0</span> = 1.0
<span style="color: #BA36A5;">k1</span> = <span style="color: #BA36A5;">k_1</span> = 3.0

<span style="color: #BA36A5;">tspan</span> = np.linspace(0, 0.5)

<span style="color: #0000FF;">def</span> <span style="color: #006699;">C</span>(K):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">k1</span>, <span style="color: #BA36A5;">k_1</span> = K
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">def</span> <span style="color: #006699;">dCdt</span>(Ca, t, k1, k_1):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> -k1 * Ca + k_1 * (Ca0 - Ca)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">sol</span> = odeint(dCdt, Ca0, tspan, <span style="color: #006FE0;">tuple</span>((k1, k_1)))
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> sol

plt.plot(tspan, C([k1, k_1]))
plt.xlim([tspan.<span style="color: #006FE0;">min</span>(), tspan.<span style="color: #006FE0;">max</span>()])
plt.xlabel(<span style="color: #008000;">'t'</span>)
plt.ylabel(<span style="color: #008000;">'C'</span>);
</pre>
</div>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>


<p>
<figure><img src="/media/bca9e95a16f361ce6d92dd6efe90a2e653e014ef.png"></figure> 
</p>


<p>
Now, the solution is an array, and we want the derivative of C with respect to the parameters at each time point. That means we want the jacobian derivative of the output with respect to the input. Here is the autograd approach to doing that. The jacobian function returns a function that we can evaluate to get the derivatives.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">import</span> time
<span style="color: #BA36A5;">t0</span> = time.time()
<span style="color: #BA36A5;">dCdk</span> = jacobian(C, 0)


<span style="color: #BA36A5;">k_sensitivity</span> = dCdk(np.array([k1, k_1]))

<span style="color: #BA36A5;">k1_sensitivity</span> = k_sensitivity[:, 0, 0]
<span style="color: #BA36A5;">k_1_sensitivity</span> = k_sensitivity[:, 0, 1]

plt.plot(tspan, np.<span style="color: #006FE0;">abs</span>(k1_sensitivity), label=<span style="color: #008000;">'dC/dk1'</span>)
plt.plot(tspan, np.<span style="color: #006FE0;">abs</span>(k_1_sensitivity), label=<span style="color: #008000;">'dC/dk_1'</span>)
plt.legend(loc=<span style="color: #008000;">'best'</span>)
plt.xlabel(<span style="color: #008000;">'t'</span>)
plt.ylabel(<span style="color: #008000;">'sensitivity'</span>)
<span style="color: #0000FF;">print</span>(f<span style="color: #008000;">'Elapsed time = {time.time() - t0:1.1f} seconds'</span>)
</pre>
</div>

<p>
Elapsed time = 38.2 seconds
</p>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>


<p>
<figure><img src="/media/3a0a58bb6d4b3e1b215c2918d511f3a8a3a2ca3d.png"></figure> 
</p>

<p>
That looks similar to the results from before. It is pretty slow I think, that took more than half a minute to work out. That is still faster and probably more correct than if I had to do it by hand. In contrast, however, the finite difference code below is comparatively very fast! I don't know what is slow in the autograd implementation. I guess it is an implementation detail.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">import</span> numdifftools <span style="color: #0000FF;">as</span> nd
<span style="color: #BA36A5;">t0</span> = time.time()

<span style="color: #BA36A5;">fdk1</span>, <span style="color: #BA36A5;">fdk_1</span> = nd.Jacobian(C)([k1, k_1]).T
<span style="color: #0000FF;">print</span>(f<span style="color: #008000;">'Elapsed time = {time.time() - t0:1.1f} seconds'</span>)

plt.plot(tspan, np.<span style="color: #006FE0;">abs</span>(fdk1), label=<span style="color: #008000;">'fd dC/dk1'</span>)
plt.plot(tspan, np.<span style="color: #006FE0;">abs</span>(fdk_1), label=<span style="color: #008000;">'fd dC/dk_1'</span>)
plt.plot(tspan, np.<span style="color: #006FE0;">abs</span>(k1_sensitivity), <span style="color: #008000;">'y--'</span>, label=<span style="color: #008000;">'dC/dk1'</span>)
plt.plot(tspan, np.<span style="color: #006FE0;">abs</span>(k_1_sensitivity),<span style="color: #008000;">'m--'</span>, label=<span style="color: #008000;">'dC/dk_1'</span>)
plt.legend(loc=<span style="color: #008000;">'best'</span>);
plt.xlabel(<span style="color: #008000;">'t'</span>);
plt.ylabel(<span style="color: #008000;">'sensitivity'</span>);
</pre>
</div>

<p>
Elapsed time = 0.1 seconds
</p>

<pre class="example">
&lt;Figure size 432x288 with 1 Axes&gt;
</pre>


<p>
<figure><img src="/media/be7bf4798396d6a27938715f6bb0e22b8f3e0b1c.png"></figure> 
</p>

<p>
You can see the two results are visually indistinguishable. Even the code is pretty similar. I would tend to prefer the autograd way since it should be less sensitive to finite difference artifacts, but it is nice to have an independent way to test if it is working.
</p>
<p>Copyright (C) 2019 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2019/09/13/Sensitivity-analysis-with-odeint-and-autograd.org">org-mode source</a></p>
<p>Org-mode version = 9.2.3</p>