

<!doctype html>
<!--[if lt IE 7 ]> <html lang="en" class="no-js ie6"> <![endif]-->
<!--[if IE 7 ]>    <html lang="en" class="no-js ie7"> <![endif]-->
<!--[if IE 8 ]>    <html lang="en" class="no-js ie8"> <![endif]-->
<!--[if IE 9 ]>    <html lang="en" class="no-js ie9"> <![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html lang="en" class="no-js"> <!--<![endif]-->
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Kitchin Research Group</title>
  <meta name="google-site-verification" content="CGcacJdHc2YoZyI0Vey9XRA5qwhhFDzThKJezbRFcJ4" />
  <meta name="description" content="Chemical Engineering at Carnegie Mellon University">
  <meta name="author" content="John Kitchin">
  <link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
  <link rel="alternate" type="application/atom+xml" title="Atom 1.0" href="/blog/feed/atom" />
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">

  <link rel="stylesheet" href="/css/base.css?v=1">
  <link rel="stylesheet" href="/css/grid.css?v=1">
  <link rel="stylesheet" media="handheld" href="/css/handheld.css?v=1">
  <link rel="stylesheet" href="/css/pygments_murphy.css" type="text/css" />

  <script src="/js/libs/modernizr-1.7.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
  <link rel="stylesheet" href="/themes/theme1/style.css?v=1">
<link href='http://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>

</head>
  <body>
    <div id="container" class="container container_12">
      <div id="main" role="main">
        <div id="main_block">
          <header>
<div id="header" class="header_gradient theme_font">
<table><tr><td>
    <h1><a href="/">The Kitchin Research Group</a></h1>
    <h2>Chemical Engineering at Carnegie Mellon University</h2>
</td>
<td colspan=100%><div style="float:right;width:100%;text-align:right;"> <span id='badgeCont737515' style='width:126px'><script src='http://labs.researcherid.com/mashlets?el=badgeCont737515&mashlet=badge&showTitle=false&className=a&rid=A-2363-2010'></script></span></div>
</td></tr>
</table>
</div>
  <div id="navigation" class="grid_12">

    <ul class="theme_font">
      <li><a href="/blog"
             class="">Blog</a></li>

      <li><a href="/blog/archive"
             class="">Archives</a></li>

      <li><a href="/publications.html">Publications</a></li>

      <li><a href="/research.html"
             class="">Research</a></li>

      <li><a href="/categories.html"
             class="">Categories</a></li>

      <li><a href="/about.html"
             class="">About us</a></li>

      <li><a href="/subscribe.html">Subscribe</a></li>

    </ul>
  </div>
</header>

          <div id="prose_block" class="grid_8">
            
  





<article>
  <div class="blog_post">
    <header>
      <div id="Lies-damn-lies-statistics-and-Bayesian-statistics"></div>
      <h2 class="blog_post_title"><a href="/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics/" rel="bookmark" title="Permanent Link to Lies, damn lies, statistics and Bayesian statistics">Lies, damn lies, statistics and Bayesian statistics</a></h2>
      <p><small><span class="blog_post_date">Posted June 22, 2025 at 11:14 AM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/machine-learning/'>machine-learning</a></span> | tags: 
      <p><small><span class="blog_post_date">Updated June 23, 2025 at 01:33 PM</span></small>
      </small></p>
    </header>
    <div class="post_prose">
      



<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org035da70">1. The data</a></li>
<li><a href="#org28ad175">2. GPR with a RBF kernel</a></li>
<li><a href="#orga7fb619">3. a better kernel solves these issues</a></li>
<li><a href="#orgca7200a">4. How about with feature engineering?</a></li>
<li><a href="#orgdd16d18">5. Summary</a></li>
</ul>
</div>
</div>
<p>
This post on LinkedIn (<a href="https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI">https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI</a>) reminded me of a quip I often make of "Lies, damn lies, statistics, and Bayesian statistics". I am frequently skeptical of claims about "Bayesian something something", especially when the claim is about uncertainty quantification. That skepticism comes from practical experience of mine that "Bayesian something something" is rarely as well behaved and informative as advertised (in my hands of course).
</p>

<p>
To illustrate, I will use some noisy, 1d data from a Lennard-Jones function and Gaussian process regression to fit the data.
</p>
<div id="outline-container-org035da70" class="outline-2">
<h2 id="org035da70"><span class="section-number-2">1.</span> The data</h2>
<div class="outline-text-2" id="text-1">
<p>
We get our data by sampling a Lennard-Jones function, adding some noise, and removing a gap in the data. The gap in the middle might be classically considered an interpolation region.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #0000FF; font-weight: bold;">import</span> numpy <span style="color: #0000FF; font-weight: bold;">as</span> np
<span style="color: #0000FF; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #0000FF; font-weight: bold;">as</span> plt

<span style="color: #BA36A5;">r</span> = np.linspace(0.95, 3, 200)

<span style="color: #BA36A5;">eps</span>, <span style="color: #BA36A5;">sig</span> = 1, 1
<span style="color: #BA36A5;">y</span> = 4 * eps * ((1 / r)**12 - (1 / r)**6) + np.random.normal(0, 0.03, size=r.shape)


<span style="color: #BA36A5;">ind</span> = ((r &gt; 1) &amp; (r &lt; 1.25)) | ((r &gt; 2) &amp; (r &lt; 2.5))
<span style="color: #BA36A5;">_R</span> = r[ind][:, <span style="color: #D0372D;">None</span>]
<span style="color: #BA36A5;">_y</span> = y[ind]
plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)
plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/653165863df7654b10ddaca2f7645560768bd870.png"></figure> 
</p>
</div>
</div>
<div id="outline-container-org28ad175" class="outline-2">
<h2 id="org28ad175"><span class="section-number-2">2.</span> GPR with a RBF kernel</h2>
<div class="outline-text-2" id="text-2">
<p>
The RBF kernel is the most standard kernel. It does an ok job fitting here, although I see evidence of overfitting (the wiggles are caused by the noise). You can reduce the overfitting by using a larger <code>alpha</code> value in the gpr, but that requires you to know in advance how smooth it should be.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #0000FF; font-weight: bold;">from</span> sklearn.gaussian_process <span style="color: #0000FF; font-weight: bold;">import</span> GaussianProcessRegressor
<span style="color: #0000FF; font-weight: bold;">from</span> sklearn.gaussian_process.kernels <span style="color: #0000FF; font-weight: bold;">import</span> RBF, WhiteKernel
<span style="color: #BA36A5;">kernel</span> = RBF() + WhiteKernel()
<span style="color: #BA36A5;">gpr</span> = GaussianProcessRegressor(kernel=kernel,
                               random_state=0, normalize_y=<span style="color: #D0372D;">True</span>).fit(_R, _y)

plt.plot(_R, _y, <span style="color: #008000;">'b.'</span>)
plt.plot(r, y, <span style="color: #008000;">'b.'</span>, alpha=0.2)

<span style="color: #BA36A5;">yp</span>, <span style="color: #BA36A5;">se</span> = gpr.predict(r[:, <span style="color: #D0372D;">None</span>], return_std=<span style="color: #D0372D;">True</span>)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, <span style="color: #008000;">'k--'</span>, r, yp - 2 * se, <span style="color: #008000;">'k--'</span>);
plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)
plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);

gpr.kernel_
</pre>
</div>

<pre class="example">
RBF(length_scale=0.0948) + WhiteKernel(noise_level=0.00635)
</pre>

<p>
<figure><img src="/media/6acc7dccc37ee773aeb4c97c62929401733a02f6.png"></figure> 
</p>

<p>
The uncertainty here is primarily related to the model, i.e. it is constrained to be correct where there is data, but with no data, the model is not the right one.
</p>

<p>
The model does well in the region where there is data, but is qualitatively wrong in the gap (even though classically this would be considered interpolation), and overestimates the uncertainty in this region. The problem is the covariance kernel decays to 0 about two length scales away from the last point, which means there is no data to inform what the weights in that region should look like.  That causes the model to revert to the mean of the data.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">gpr.predict([[1.8]]), gpr.predict([[3.0]]), np.mean(_y)
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">array</td>
<td class="org-left">((-0.2452041))</td>
<td class="org-left">array</td>
<td class="org-left">((-0.29363654))</td>
<td class="org-right">-0.2936364964541409</td>
</tr>
</tbody>
</table>

<p>
Why is this happening? It is not that tricky. You can think of the GP as an expansion of the data in basis functions. The kernel trick effectively makes this expansion in the infinite limit. What are those basis functions? We can draw samples of them, which we show here. You can see that where there is no data, the basis functions are "wiggly". That means they are simply not good at making predictions here.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">y_samples</span> = gpr.sample_y(r[:, <span style="color: #D0372D;">None</span>], n_samples=15, random_state=0)

plt.plot(r, yp)
plt.plot(r, yp + 2 * se, <span style="color: #008000;">'k--'</span>, r, yp - 2 * se, <span style="color: #008000;">'k--'</span>);
plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)

plt.plot(r, y_samples, <span style="color: #008000;">'k'</span>, alpha=0.2);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/cff4cfa5cacedcef6ecde8ec2b63dcee659949fb.png"></figure> 
</p>


<p>
This kernel simply cannot be used for extrapolation, or any predictions more than about two length scales away from the nearest point. Calling it Bayesian doesn't make it better. For similar reasons, this model will not work well outside the data range.
</p>

<p>
A practical person would still consider using this model, and might even rely on the uncertainty being too large to identify regions of low reliability.
</p>
</div>
</div>
<div id="outline-container-orga7fb619" class="outline-2">
<h2 id="orga7fb619"><span class="section-number-2">3.</span> a better kernel solves these issues</h2>
<div class="outline-text-2" id="text-3">
<p>
Not all is lost, if we know more. In this case we can construct a kernel that reflects our understanding that the data came from a Lennard Jones like interaction model. You can construct kernels by adding and multiplying kernels. Here we consider a linear kernel, the <code>DotProduct</code> kernel, and construct a new kernel that is a sum of the linear kernel to the 12<sup>th</sup> power, a linear kernel to the 6<sup>th</sup> power, and a <code>WhiteKernel</code> for the noise. It is a little subtle that this kernel should work in \(1 / r\) space, so in addition to kernel engineering, we also do feature engineering.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #0000FF; font-weight: bold;">from</span> sklearn.gaussian_process.kernels <span style="color: #0000FF; font-weight: bold;">import</span> DotProduct

<span style="color: #BA36A5;">kernel</span> = DotProduct()**12 + DotProduct()**6 +  WhiteKernel()
<span style="color: #BA36A5;">gpr</span> = GaussianProcessRegressor(kernel=kernel, normalize_y=<span style="color: #D0372D;">True</span>).fit(1 / _R, _y)

plt.plot(_R, _y, <span style="color: #008000;">'b.'</span>)
plt.plot(r, y, <span style="color: #008000;">'b.'</span>, alpha=0.2)


<span style="color: #BA36A5;">yp</span>, <span style="color: #BA36A5;">se</span> = gpr.predict(1 / r[:, <span style="color: #D0372D;">None</span>], return_std=<span style="color: #D0372D;">True</span>)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, <span style="color: #008000;">'k--'</span>, r, yp - 2 * se, <span style="color: #008000;">'k--'</span>);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);

gpr.kernel_
</pre>
</div>

<pre class="example">
DotProduct(sigma_0=0.0281) ** 12 + DotProduct(sigma_0=0.936) ** 6 + WhiteKernel(noise_level=0.0077)
</pre>

<p>
<figure><img src="/media/70e91f8419a473ed578a14442694e67a3409bd1e.png"></figure> 
</p>

<p>
Note that this GPR does fine in the gap, including the right level of uncertainty there. This model is better because we used the kernel to constrain what forms the model can have. This model actually extrapolates correctly outside the data. It is worth noting that although this model has great predictive and UQ properties, it does not tell us anything about the values of &epsilon; and &sigma; in the Lennard Jones model. Although we might say the kernel is physics-based, i.e. it is based on the relevant features and equation, it does not have physical parameters in it.
</p>

<p>
How about those basis functions here? You can see that all of them basically look like the LJ potential. That means they are good basis functions to expand this data set in.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">y_samples</span> = gpr.sample_y(1 / r[:, <span style="color: #D0372D;">None</span>], n_samples=15, random_state=0)

plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)

plt.plot(r, y_samples, <span style="color: #008000;">'k'</span>, alpha=0.2);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/e7fe34a01c52cb228cbbcde85e5f334e7f8237a1.png"></figure> 
</p>
</div>
</div>
<div id="outline-container-orgca7200a" class="outline-2">
<h2 id="orgca7200a"><span class="section-number-2">4.</span> How about with feature engineering?</h2>
<div class="outline-text-2" id="text-4">
<p>
Can we do even better with feature engineering here? Motivated by <a href="https://www.linkedin.com/feed/update/urn:li:activity:7342573774774386688?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7342573774774386688%2C7342949865590530052%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287342949865590530052%2Curn%3Ali%3Aactivity%3A7342573774774386688%29">this comment</a> by Cory Simon, we cast the problem as a linear regression in [1 / r<sup>6</sup>, 1 / r<sup>12</sup>] feature space. This is also a perfectly reasonable thing to do. Since our output is linear in these features, we simply use a linear kernel (aka the DotProduct kernel in sklearn).
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">r6</span> = 1 / _R**6
<span style="color: #BA36A5;">r12</span> = r6**2

<span style="color: #BA36A5;">kernel</span> = DotProduct() + WhiteKernel()

<span style="color: #BA36A5;">gpr</span> = GaussianProcessRegressor(kernel=kernel, normalize_y=<span style="color: #D0372D;">True</span>).fit(np.hstack([r6, r12]), _y)

plt.plot(_R, _y, <span style="color: #008000;">'b.'</span>)
plt.plot(r, y, <span style="color: #008000;">'b.'</span>, alpha=0.2)

<span style="color: #BA36A5;">fr6</span> = 1 / r[:, <span style="color: #D0372D;">None</span>]**6
<span style="color: #BA36A5;">fr12</span> = fr6**2

<span style="color: #BA36A5;">yp</span>, <span style="color: #BA36A5;">se</span> = gpr.predict(np.hstack([fr6, fr12]), return_std=<span style="color: #D0372D;">True</span>)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, <span style="color: #008000;">'k--'</span>, r, yp - 2 * se, <span style="color: #008000;">'k--'</span>);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);

gpr.kernel_
</pre>
</div>

<pre class="example">
DotProduct(sigma_0=0.74) + WhiteKernel(noise_level=0.00654)
</pre>

<p>
<figure><img src="/media/d8769fe652b9e902e3d349ce26cdbd7d8050b190.png"></figure> 
</p>

<p>
We can't easily plot these basis functions the same way, so we reduce them to a 1-d plot. You can see here that these basis functions practically the same as the one with the advanced kernel.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">y_samples</span> = gpr.sample_y(np.hstack([fr6, fr12]),
                         n_samples=15, random_state=0)

plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)

plt.plot(r, y_samples, <span style="color: #008000;">'k'</span>, alpha=0.2);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/f777586bb8e17bac5ca3dadbfba97119addeb46b.png"></figure> 
</p>



<p>
This also works quite well, and is another way to leverage knowledge about what we are building a model for.
</p>
</div>
</div>
<div id="outline-container-orgdd16d18" class="outline-2">
<h2 id="orgdd16d18"><span class="section-number-2">5.</span> Summary</h2>
<div class="outline-text-2" id="text-5">
<p>
Naive use of GPR can provide useful models when you have enough data, but these models likely do not accurately capture uncertainty outside that data, nor is it likely they are reliable in extrapolation. It is possible to do better than this, when you know what to do. Through feature and kernel engineering, you can sometimes create situations where the problem essentially becomes linear regression, where a simple linear kernel is what you want, or you develop a kernel that represents the underlying model. Kernel engineering is generally hard, with limited opportunities to be flexible. See <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">https://www.cs.toronto.edu/~duvenaud/cookbook/</a> for examples of kernels and combining them.
</p>

<p>
You can see it is not adequate to say "we used Gaussian process regression". That is about as informative as saying linear regression without identifying the features, or nonlinear regression and not saying what model. You have to be specific about the kernel, and thoughtful about how you know if a prediction is reliable or not. Just because you get an uncertainty prediction doesn't mean its right.
</p>
</div>
</div>
<p>Copyright (C) 2025 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2025/06/22/Lies,-damn-lies,-statistics-and-Bayesian-statistics.org">org-mode source</a></p>
<p>Org-mode version = 9.8-pre</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="New-Publication-Solving-an-inverse-problem-with-generative-models"></div>
      <h2 class="blog_post_title"><a href="/blog/2025/06/17/New-Publication-Solving-an-inverse-problem-with-generative-models/" rel="bookmark" title="Permanent Link to New Publication - Solving an inverse problem with generative models">New Publication - Solving an inverse problem with generative models</a></h2>
      <p><small><span class="blog_post_date">Posted June 17, 2025 at 01:24 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/news/'>news</a>, <a href='/blog/category/publication/'>publication</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
Inverse problems—where we aim to find inputs that produce a desired output—are notoriously challenging in science and engineering. In this study, I explore how generative AI models can tackle these problems by comparing four approaches: a forward model combined with nonlinear optimization, a backward model using partial least squares regression, and two generative methods based on Gaussian mixture models and diffusion-based flow transformations. Using data from a simple RGB-controlled light sensor, the paper demonstrates that generative models can accurately and flexibly infer input settings for target outputs, with advantages such as uncertainty quantification and the ability to condition on partial outputs. This work showcases the promise of generative modeling in reshaping how we approach inverse problems across disciplines.
</p>


<iframe width="560" height="315" src="https://www.youtube.com/embed/zx6FHzx8V-Y?si=1OQBQ25Ze8e5mzZl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #006699;">@article</span>{<span style="color: #D0372D;">kitchin-2025-solvin-inver</span>,
  <span style="color: #BA36A5;">author</span> =       "Kitchin, John R.",
  <span style="color: #BA36A5;">title</span> =        {Solving an Inverse Problem With Generative Models},
  <span style="color: #BA36A5;">journal</span> =      "Digital Discovery",
  <span style="color: #BA36A5;">pages</span> =        "-",
  <span style="color: #BA36A5;">year</span> =         2025,
  <span style="color: #BA36A5;">doi</span> =          "<span style="color: #006DAF; text-decoration: underline;">10.1039/D5DD00137D</span>",
  <span style="color: #BA36A5;">url</span> =          "<span style="color: #006DAF; text-decoration: underline;">http://dx.doi.org/10.1039/D5DD00137D</span>",
  <span style="color: #BA36A5;">abstract</span> =     "Inverse problems{,} where we seek the values of inputs to a
                  model that lead to a desired set of outputs{,} are a
                  challenges subset of problems in science and engineering. In
                  this work we demonstrate the use of two generative AI methods
                  to solve inverse problems. We compare this approach to two
                  more conventional approaches that use a forward model with
                  nonlinear programming{,} and the use of a backward model. We
                  illustrate each method on a dataset obtained from a simple
                  remote instrument that has three inputs: the setting of the
                  red{,} green and blue channels of an RGB LED. We focus on
                  several outputs from a light sensor that measures intensity at
                  445 nm{,} 515 nm{,} 590 nm{,} and 630 nm. The speci&#64257;c problem
                  we solve is identifying inputs that lead to a speci&#64257;c
                  intensity in three of those channels. We show that generative
                  models can be used to solve this kind of inverse problem{,}
                  and they have some advantages over the conventional
                  approaches.",
  <span style="color: #BA36A5;">publisher</span> =    "RSC",
}
</pre>
</div>

<p>
<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
<div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1039/D5DD00137D'></div>
</p>
<p>Copyright (C) 2025 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2025/06/17/New-Publication---Solving-an-inverse-problem-with-generative-models.org">org-mode source</a></p>
<p>Org-mode version = 9.8-pre</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2025/06/17/New-Publication-Solving-an-inverse-problem-with-generative-models">Discuss on Twitter</a>

  <hr class="interblog" />

          </div>
          <div id="sidebar" class="grid_4">
            <aside>
<section>
<script>
  (function() {
    var cx = '002533177287215655227:l7uvu35ssbc';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</section>

<section>
    <h1 class="post_header_gradient theme_font">Twitter</h1>
    <a class="twitter-timeline" href="https://twitter.com/johnkitchin" data-widget-id="545217643582881792">Tweets by @johnkitchin</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</section>


  <section>
    <h1 class="post_header_gradient theme_font">Links</h1>
    <ul>
      <li><a href="https://www.continuum.io">Anaconda Python</a></li>
      <li><a href="/pycse">Pycse</a></li>
      <li><a href="/dft-book">DFT-book</a></li>
    </ul>
  </section>

  <section>
    <h1 class="post_header_gradient theme_font">Latest Posts</h1>
    <ul>
      <li><a href="/blog/2025/07/09/New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design/">New publication - Hyperplane decision trees as piecewise linear surrogate models for chemical process design</a></li>
      <li><a href="/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics/">Lies, damn lies, statistics and Bayesian statistics</a></li>
      <li><a href="/blog/2025/06/17/New-Publication-Solving-an-inverse-problem-with-generative-models/">New Publication - Solving an inverse problem with generative models</a></li>
      <li><a href="/blog/2025/05/07/New-publication-The-Evolving-Role-of-Programming-and-Llms-in-the-Development-of-Self-Driving-Laboratories/">New publication - The Evolving Role of Programming and Llms in the Development of Self-Driving Laboratories</a></li>
      <li><a href="/blog/2025/04/11/New-publication-A-Classification-based-Methodology-for-the-Estimation-of-Binary-Surfactant-Critical-Micelle-Concentrations/">New publication - A Classification-based Methodology for the Estimation of Binary Surfactant Critical Micelle Concentrations</a></li>
    </ul>
  </section>

<section>
<h1 class="post_header_gradient theme_font">Latest GitHub Repos</h1>
  <a href="https://github.com/jkitchin">@jkitchin</a> on GitHub.
  <ul id="my-github-projects">
        <li class="loading">Status updating&#8230;</li>
  </ul>

</section>
</aside>

          </div>
          <div class="clear"></div>
        </div>
      </div>
      
<footer>
  <div id="footer" class="grid_12">
    <div class="grid_8">
      <p>
        <a href="/blog/feed/index.xml">RSS</a>
      </p>
    </div>
    <div class="grid_4" id="credits">
      <p>
        Copyright 2025
        John Kitchin
      </p>
      <p>
        Powered by <a href="http://www.blogofile.com">Blogofile</a>
      </p>
    </div>
  </div>
</footer>

    </div>
      <script src="//ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/js/libs/jquery-1.5.1.min.js"%3E%3C/script%3E'))</script>
  <script src="/js/plugins.js"></script>
  <script src="/js/script.js"></script>
  <script src="/js/jquery.tweet.js"></script>  
  <script src="/js/site.js"></script>
  <!--[if lt IE 7 ]>
  <script src="js/libs/dd_belatedpng.js"></script>
  <script> DD_belatedPNG.fix('img, .png_bg');</script>
  <![endif]-->
 
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PH8NF4F0RE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PH8NF4F0RE');
</script>


  </body>
</html>






<script src="http://ajax.microsoft.com/ajax/jquery/jquery-1.4.2.min.js" type="text/javascript"></script>
<script src="/js/git.js" type="text/javascript"></script>
<script type="text/javascript">
    $(function() {
     $("#my-github-projects").loadRepositories("jkitchin");
    });
</script>



