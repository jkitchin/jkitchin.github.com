

<!doctype html>
<!--[if lt IE 7 ]> <html lang="en" class="no-js ie6"> <![endif]-->
<!--[if IE 7 ]>    <html lang="en" class="no-js ie7"> <![endif]-->
<!--[if IE 8 ]>    <html lang="en" class="no-js ie8"> <![endif]-->
<!--[if IE 9 ]>    <html lang="en" class="no-js ie9"> <![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html lang="en" class="no-js"> <!--<![endif]-->
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Kitchin Research Group</title>
  <meta name="google-site-verification" content="CGcacJdHc2YoZyI0Vey9XRA5qwhhFDzThKJezbRFcJ4" />
  <meta name="description" content="Chemical Engineering at Carnegie Mellon University">
  <meta name="author" content="John Kitchin">
  <link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
  <link rel="alternate" type="application/atom+xml" title="Atom 1.0" href="/blog/feed/atom" />
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">

  <link rel="stylesheet" href="/css/base.css?v=1">
  <link rel="stylesheet" href="/css/grid.css?v=1">
  <link rel="stylesheet" media="handheld" href="/css/handheld.css?v=1">
  <link rel="stylesheet" href="/css/pygments_murphy.css" type="text/css" />

  <script src="/js/libs/modernizr-1.7.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
  <link rel="stylesheet" href="/themes/theme1/style.css?v=1">
<link href='http://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>

</head>
  <body>
    <div id="container" class="container container_12">
      <div id="main" role="main">
        <div id="main_block">
          <header>
<div id="header" class="header_gradient theme_font">
<table><tr><td>
    <h1><a href="/">The Kitchin Research Group</a></h1>
    <h2>Chemical Engineering at Carnegie Mellon University</h2>
</td>
<td colspan=100%><div style="float:right;width:100%;text-align:right;"> <span id='badgeCont737515' style='width:126px'><script src='http://labs.researcherid.com/mashlets?el=badgeCont737515&mashlet=badge&showTitle=false&className=a&rid=A-2363-2010'></script></span></div>
</td></tr>
</table>
</div>
  <div id="navigation" class="grid_12">

    <ul class="theme_font">
      <li><a href="/blog"
             class="">Blog</a></li>

      <li><a href="/blog/archive"
             class="">Archives</a></li>

      <li><a href="/publications.html">Publications</a></li>

      <li><a href="/research.html"
             class="">Research</a></li>

      <li><a href="/categories.html"
             class="">Categories</a></li>

      <li><a href="/about.html"
             class="">About us</a></li>

      <li><a href="/subscribe.html">Subscribe</a></li>

    </ul>
  </div>
</header>

          <div id="prose_block" class="grid_8">
            
  





<article>
  <div class="blog_post">
    <header>
      <div id="New-publication-Towards-Agentic-Science-for-Advancing-Scientific-Discovery"></div>
      <h2 class="blog_post_title"><a href="/blog/2025/09/16/New-publication-Towards-Agentic-Science-for-Advancing-Scientific-Discovery/" rel="bookmark" title="Permanent Link to New publication - Towards Agentic Science for Advancing Scientific Discovery">New publication - Towards Agentic Science for Advancing Scientific Discovery</a></h2>
      <p><small><span class="blog_post_date">Posted September 16, 2025 at 01:38 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/news/'>news</a>, <a href='/blog/category/publication/'>publication</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
In our new paper published in Nature Machine Intelligence, my colleagues Hongliang Xin, Heather Kulik, and I explore what we call "agentic science" – a new paradigm where AI agents can (semi-)autonomously conduct scientific research.
</p>

<p>
Scientific discovery has evolved through distinct eras: from early empirical observations and theoretical frameworks like Newtonian mechanics, through the computational modeling revolution, to today's data science approaches. We argue that we're now entering the age of agentic science, where AI systems don't just analyze data but can independently reason, plan experiments, and interact with both digital tools and physical laboratory equipment.
</p>

<p>
What makes these AI agents special is their capacity for independent agency. Built around large language models that can process text, images, and structured data, they can actively learn, integrate with external tools, and think strategically about long-term research goals. Systems like Coscientist can already interpret natural language requests and autonomously operate lab equipment, while A-Lab represents a fully autonomous materials synthesis laboratory.
</p>

<p>
However, we're honest about the challenges. These systems can "hallucinate" – producing convincing but incorrect information – and they're sensitive to how questions are phrased. We also lack standardized ways to evaluate their performance, and they consume significant computational resources.
</p>

<p>
The key to success lies in maintaining human oversight while leveraging AI for high-throughput tasks. With proper safeguards, transparent documentation, and ethical considerations, agentic AI could dramatically accelerate scientific discovery while actually improving reproducibility by systematically analyzing literature and identifying research gaps.
</p>

<p>
We believe this represents a fundamental shift in how science gets done – not replacing human scientists, but creating powerful human-AI partnerships that could unlock new pathways to discovery.
</p>

<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #006699;">@article</span>{<span style="color: #D0372D;">xin-2025-towar-agent</span>,
  <span style="color: #BA36A5;">author</span> =       {Hongliang Xin and John R. Kitchin and Heather J. Kulik},
  <span style="color: #BA36A5;">title</span> =        {Towards Agentic Science for Advancing Scientific Discovery},
  <span style="color: #BA36A5;">journal</span> =      {Nature Machine Intelligence},
  <span style="color: #BA36A5;">volume</span> =       {nil},
  <span style="color: #BA36A5;">number</span> =       {nil},
  <span style="color: #BA36A5;">pages</span> =        {nil},
  <span style="color: #BA36A5;">year</span> =         2025,
  <span style="color: #BA36A5;">doi</span> =          {<span style="color: #006DAF; text-decoration: underline;">10.1038/s42256-025-01110-x</span>},
  <span style="color: #BA36A5;">url</span> =          {<span style="color: #006DAF; text-decoration: underline;">https://doi.org/10.1038/s42256-025-01110-x</span>},
  <span style="color: #BA36A5;">DATE_ADDED</span> =   {Tue Sep 16 13:36:03 2025},
}
</pre>
</div>

<p>
<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
<div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1038/s42256-025-01110-x'></div>
</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/t_A_TAa3t1A?si=mYM2UTghkiL0nUJO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<p>Copyright (C) 2025 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2025/09/16/New-publication---Towards-Agentic-Science-for-Advancing-Scientific-Discovery.org">org-mode source</a></p>
<p>Org-mode version = 9.8-pre</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2025/09/16/New-publication-Towards-Agentic-Science-for-Advancing-Scientific-Discovery">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="New-publication-Mapping-uncertainty-using-differentiable-programming"></div>
      <h2 class="blog_post_title"><a href="/blog/2025/07/31/New-publication-Mapping-uncertainty-using-differentiable-programming/" rel="bookmark" title="Permanent Link to New publication - Mapping uncertainty using differentiable programming">New publication - Mapping uncertainty using differentiable programming</a></h2>
      <p><small><span class="blog_post_date">Posted July 31, 2025 at 01:42 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/news/'>news</a>, <a href='/blog/category/publication/'>publication</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
In our latest work, we set out to tackle a common challenge in engineering and science: understanding how small uncertainties in inputs, like temperature changes or slight variations in pressure, can ripple through complex systems and affect the final outcome. Instead of relying on slow, trial-and-error simulations, we leveraged an emerging computing technique that treats uncertainty like a path we can follow mathematically. By "teaching" our software to calculate these paths directly, we can predict how errors build up in real processes, whether in a chemical reactor, a filtration system, or a bioreactor, 100 times faster than traditional methods. This speed and precision mean we can design safer, more reliable systems and respond more quickly when things don't go exactly as planned.
</p>

<p>
We introduce a differentiable-programming-based framework for uncertainty quantification that leverages the implicit function theorem and path integration to compute both forward and inverse uncertainty maps directly from high-fidelity or surrogate models . Our approach requires only C<sup>1</sup> differentiability and injectivity of the implicit system and avoids expensive Monte Carlo sampling by tracing uncertainty "contours" through the model. We validate it on three chemical-engineering case studies: a CSTR (showing exact agreement with analytical solutions in unimodal and multimodal scenarios), a membrane reactor for natural-gas aromatization (recovering 95% of exhaustive-search samples in 7 min vs. ~10 h), and a fed-batch bioreactor with 3D ellipsoidal uncertainty regions. All code and reproducible Jupyter notebooks are available at github.com/KitchinHUB/Mapping-Uncertainty-Using-Differentiable-Programming.
</p>


<p>
<figure><img src="/media/date-31-07-2025-time-13-25-21.png"></figure> 
</p>


<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #006699;">@article</span>{<span style="color: #D0372D;">alves-2025-mappin-uncer</span>,
  <span style="color: #BA36A5;">author</span> =       {Victor Alves and Carl D. Laird and Fernando V. Lima and John
                  R. Kitchin},
  <span style="color: #BA36A5;">title</span> =        {Mapping Uncertainty Using Differentiable Programming},
  <span style="color: #BA36A5;">journal</span> =      {AIChE Journal},
  <span style="color: #BA36A5;">volume</span> =       {},
  <span style="color: #BA36A5;">number</span> =       {},
  <span style="color: #BA36A5;">pages</span> =        {e18940},
  <span style="color: #BA36A5;">year</span> =         2025,
  <span style="color: #BA36A5;">doi</span> =          {<span style="color: #006DAF; text-decoration: underline;">10.1002/aic.18940</span>},
  <span style="color: #BA36A5;">url</span> =          {<span style="color: #006DAF; text-decoration: underline;">https://doi.org/10.1002/aic.18940</span>},
  <span style="color: #BA36A5;">DATE_ADDED</span> =   {Thu Jul 31 09:53:53 2025},
}
</pre>
</div>


<p>
<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
<div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1002/aic.18940'></div>
</p>
<p>Copyright (C) 2025 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2025/07/31/New-publication---Mapping-uncertainty-using-differentiable-programming.org">org-mode source</a></p>
<p>Org-mode version = 9.8-pre</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2025/07/31/New-publication-Mapping-uncertainty-using-differentiable-programming">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="New-publication-Spin-informed-universal-graph-neural-networks-for-simulating-magnetic-ordering"></div>
      <h2 class="blog_post_title"><a href="/blog/2025/07/17/New-publication-Spin-informed-universal-graph-neural-networks-for-simulating-magnetic-ordering/" rel="bookmark" title="Permanent Link to New publication - Spin-informed universal graph neural networks for simulating magnetic ordering">New publication - Spin-informed universal graph neural networks for simulating magnetic ordering</a></h2>
      <p><small><span class="blog_post_date">Posted July 17, 2025 at 08:07 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/news/'>news</a>, <a href='/blog/category/publication/'>publication</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
In this work, we developed a data-efficient, spin-informed graph neural network framework that augments universal machine-learning interatomic potentials with explicit spin coordinates and initial magnetic-moment guesses, while rigorously preserving the physical symmetries of collinear magnetism. This allows us to predict both the magnitude and direction of atomic spins in bulk and surface materials. By integrating a closed-loop anomaly detection pipeline based on Gaussian mixture models and z-score outlier filtering, we uncovered and corrected mislabeled DFT data, substantially improving dataset quality and model robustness. The resulting SI-GemNet-OC model achieves state-of-the-art accuracy, dramatically speeds up DFT convergence (e.g., reducing SCF cycles for GdDyAl₄ from 211 to 28), and successfully ranks magnetic orderings across hundreds of compounds with a Spearman’s ρ of 0.896. Importantly, we also show that this approach generalizes to complex surface and adsorbate-induced spin configurations, offering a powerful new tool for high-throughput discovery of magnetic materials.
</p>



<p>
<figure><img src="/media/date-01-07-2025-time-16-58-18.png"></figure> 
</p>



<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #006699;">@article</span>{<span style="color: #D0372D;">xu-2025-spin-infor</span>,
  <span style="color: #BA36A5;">author</span> =       {Wenbin Xu and Rohan Yuri Sanspeur and Adeesh Kolluru and Bowen
                  Deng and Peter Harrington and Steven Farrell and Karsten
                  Reuter and John R. Kitchin },
  <span style="color: #BA36A5;">title</span> =        {Spin-Informed Universal Graph Neural Networks for Simulating
                  Magnetic Ordering},
  <span style="color: #BA36A5;">journal</span> =      {Proceedings of the National Academy of Sciences},
  <span style="color: #BA36A5;">volume</span> =       122,
  <span style="color: #BA36A5;">number</span> =       27,
  <span style="color: #BA36A5;">pages</span> =        {e2422973122},
  <span style="color: #BA36A5;">year</span> =         2025,
  <span style="color: #BA36A5;">doi</span> =          {<span style="color: #006DAF; text-decoration: underline;">10.1073/pnas.2422973122</span>},
  <span style="color: #BA36A5;">URL</span> =          {<span style="color: #006DAF; text-decoration: underline;">https://www.pnas.org/doi/abs/10.1073/pnas.2422973122</span>},
  <span style="color: #BA36A5;">eprint</span> =       {https://www.pnas.org/doi/pdf/10.1073/pnas.2422973122},
}
</pre>
</div>

<p>
<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
<div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1073/pnas.2422973122'></div>
</p>
<p>Copyright (C) 2025 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2025/07/17/New-publication---Spin-informed-universal-graph-neural-networks-for-simulating-magnetic-ordering.org">org-mode source</a></p>
<p>Org-mode version = 9.8-pre</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2025/07/17/New-publication-Spin-informed-universal-graph-neural-networks-for-simulating-magnetic-ordering">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design"></div>
      <h2 class="blog_post_title"><a href="/blog/2025/07/09/New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design/" rel="bookmark" title="Permanent Link to New publication - Hyperplane decision trees as piecewise linear surrogate models for chemical process design">New publication - Hyperplane decision trees as piecewise linear surrogate models for chemical process design</a></h2>
      <p><small><span class="blog_post_date">Posted July 09, 2025 at 02:52 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/news/'>news</a>, <a href='/blog/category/publication/'>publication</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
We’ve developed a new kind of decision-tree model that’s both smart and practical for tackling tough engineering problems. First, we take raw data and "lift" it into a richer feature space so we can slice it more cleverly including angular shapes. Next, we grow a friendly “hyperplane” tree that splits data along these angled cuts, fitting simple linear models in each branch. The result is a piecewise-linear surrogate that behaves a lot like the real system but runs orders of magnitude faster. Finally, because each piece is just a linear model, we can plug the whole thing straight into an optimizer that finds the very best solution under complex rules. That means we can design chemical processes, heat exchangers, or any engineering system more reliably and sustainably - saving time, energy, and cost.
</p>


<p>
<figure><img src="/media/date-09-07-2025-time-14-19-14.png"></figure> 
</p>



<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #006699;">@article</span>{<span style="color: #D0372D;">sunshine-2025-hyper-decis</span>,
  <span style="color: #BA36A5;">author</span> =       {Ethan M. Sunshine and Carolina Colombo Tedesco and Sneha A.
                  Akhade and Matthew J. McNenly and John R. Kitchin and Carl D.
                  Laird},
  <span style="color: #BA36A5;">title</span> =        {Hyperplane Decision Trees As Piecewise Linear Surrogate Models
                  for Chemical Process Design},
  <span style="color: #BA36A5;">journal</span> =      {Computers \&amp; Chemical Engineering},
  <span style="color: #BA36A5;">volume</span> =       {},
  <span style="color: #BA36A5;">number</span> =       {},
  <span style="color: #BA36A5;">pages</span> =        109204,
  <span style="color: #BA36A5;">year</span> =         2025,
  <span style="color: #BA36A5;">doi</span> =          {<span style="color: #006DAF; text-decoration: underline;">10.1016/j.compchemeng.2025.109204</span>},
  <span style="color: #BA36A5;">url</span> =          {<span style="color: #006DAF; text-decoration: underline;">https://doi.org/10.1016/j.compchemeng.2025.109204</span>},
  <span style="color: #BA36A5;">DATE_ADDED</span> =   {Wed Jul 9 14:14:17 2025},
}
</pre>
</div>

<p>
<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
<div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1016/j.compchemeng.2025.109204'></div>
</p>
<p>Copyright (C) 2025 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2025/07/09/New-publication---Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design.org">org-mode source</a></p>
<p>Org-mode version = 9.8-pre</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2025/07/09/New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="Lies-damn-lies-statistics-and-Bayesian-statistics"></div>
      <h2 class="blog_post_title"><a href="/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics/" rel="bookmark" title="Permanent Link to Lies, damn lies, statistics and Bayesian statistics">Lies, damn lies, statistics and Bayesian statistics</a></h2>
      <p><small><span class="blog_post_date">Posted June 22, 2025 at 11:14 AM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/machine-learning/'>machine-learning</a></span> | tags: 
      <p><small><span class="blog_post_date">Updated June 23, 2025 at 01:33 PM</span></small>
      </small></p>
    </header>
    <div class="post_prose">
      



<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org035da70">1. The data</a></li>
<li><a href="#org28ad175">2. GPR with a RBF kernel</a></li>
<li><a href="#orga7fb619">3. a better kernel solves these issues</a></li>
<li><a href="#orgca7200a">4. How about with feature engineering?</a></li>
<li><a href="#orgdd16d18">5. Summary</a></li>
</ul>
</div>
</div>
<p>
This post on LinkedIn (<a href="https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI">https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI</a>) reminded me of a quip I often make of "Lies, damn lies, statistics, and Bayesian statistics". I am frequently skeptical of claims about "Bayesian something something", especially when the claim is about uncertainty quantification. That skepticism comes from practical experience of mine that "Bayesian something something" is rarely as well behaved and informative as advertised (in my hands of course).
</p>

<p>
To illustrate, I will use some noisy, 1d data from a Lennard-Jones function and Gaussian process regression to fit the data.
</p>
<div id="outline-container-org035da70" class="outline-2">
<h2 id="org035da70"><span class="section-number-2">1.</span> The data</h2>
<div class="outline-text-2" id="text-1">
<p>
We get our data by sampling a Lennard-Jones function, adding some noise, and removing a gap in the data. The gap in the middle might be classically considered an interpolation region.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #0000FF; font-weight: bold;">import</span> numpy <span style="color: #0000FF; font-weight: bold;">as</span> np
<span style="color: #0000FF; font-weight: bold;">import</span> matplotlib.pyplot <span style="color: #0000FF; font-weight: bold;">as</span> plt

<span style="color: #BA36A5;">r</span> = np.linspace(0.95, 3, 200)

<span style="color: #BA36A5;">eps</span>, <span style="color: #BA36A5;">sig</span> = 1, 1
<span style="color: #BA36A5;">y</span> = 4 * eps * ((1 / r)**12 - (1 / r)**6) + np.random.normal(0, 0.03, size=r.shape)


<span style="color: #BA36A5;">ind</span> = ((r &gt; 1) &amp; (r &lt; 1.25)) | ((r &gt; 2) &amp; (r &lt; 2.5))
<span style="color: #BA36A5;">_R</span> = r[ind][:, <span style="color: #D0372D;">None</span>]
<span style="color: #BA36A5;">_y</span> = y[ind]
plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)
plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/653165863df7654b10ddaca2f7645560768bd870.png"></figure> 
</p>
</div>
</div>
<div id="outline-container-org28ad175" class="outline-2">
<h2 id="org28ad175"><span class="section-number-2">2.</span> GPR with a RBF kernel</h2>
<div class="outline-text-2" id="text-2">
<p>
The RBF kernel is the most standard kernel. It does an ok job fitting here, although I see evidence of overfitting (the wiggles are caused by the noise). You can reduce the overfitting by using a larger <code>alpha</code> value in the gpr, but that requires you to know in advance how smooth it should be.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #0000FF; font-weight: bold;">from</span> sklearn.gaussian_process <span style="color: #0000FF; font-weight: bold;">import</span> GaussianProcessRegressor
<span style="color: #0000FF; font-weight: bold;">from</span> sklearn.gaussian_process.kernels <span style="color: #0000FF; font-weight: bold;">import</span> RBF, WhiteKernel
<span style="color: #BA36A5;">kernel</span> = RBF() + WhiteKernel()
<span style="color: #BA36A5;">gpr</span> = GaussianProcessRegressor(kernel=kernel,
                               random_state=0, normalize_y=<span style="color: #D0372D;">True</span>).fit(_R, _y)

plt.plot(_R, _y, <span style="color: #008000;">'b.'</span>)
plt.plot(r, y, <span style="color: #008000;">'b.'</span>, alpha=0.2)

<span style="color: #BA36A5;">yp</span>, <span style="color: #BA36A5;">se</span> = gpr.predict(r[:, <span style="color: #D0372D;">None</span>], return_std=<span style="color: #D0372D;">True</span>)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, <span style="color: #008000;">'k--'</span>, r, yp - 2 * se, <span style="color: #008000;">'k--'</span>);
plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)
plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);

gpr.kernel_
</pre>
</div>

<pre class="example">
RBF(length_scale=0.0948) + WhiteKernel(noise_level=0.00635)
</pre>

<p>
<figure><img src="/media/6acc7dccc37ee773aeb4c97c62929401733a02f6.png"></figure> 
</p>

<p>
The uncertainty here is primarily related to the model, i.e. it is constrained to be correct where there is data, but with no data, the model is not the right one.
</p>

<p>
The model does well in the region where there is data, but is qualitatively wrong in the gap (even though classically this would be considered interpolation), and overestimates the uncertainty in this region. The problem is the covariance kernel decays to 0 about two length scales away from the last point, which means there is no data to inform what the weights in that region should look like.  That causes the model to revert to the mean of the data.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python">gpr.predict([[1.8]]), gpr.predict([[3.0]]), np.mean(_y)
</pre>
</div>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">array</td>
<td class="org-left">((-0.2452041))</td>
<td class="org-left">array</td>
<td class="org-left">((-0.29363654))</td>
<td class="org-right">-0.2936364964541409</td>
</tr>
</tbody>
</table>

<p>
Why is this happening? It is not that tricky. You can think of the GP as an expansion of the data in basis functions. The kernel trick effectively makes this expansion in the infinite limit. What are those basis functions? We can draw samples of them, which we show here. You can see that where there is no data, the basis functions are "wiggly". That means they are simply not good at making predictions here.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">y_samples</span> = gpr.sample_y(r[:, <span style="color: #D0372D;">None</span>], n_samples=15, random_state=0)

plt.plot(r, yp)
plt.plot(r, yp + 2 * se, <span style="color: #008000;">'k--'</span>, r, yp - 2 * se, <span style="color: #008000;">'k--'</span>);
plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)

plt.plot(r, y_samples, <span style="color: #008000;">'k'</span>, alpha=0.2);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/cff4cfa5cacedcef6ecde8ec2b63dcee659949fb.png"></figure> 
</p>


<p>
This kernel simply cannot be used for extrapolation, or any predictions more than about two length scales away from the nearest point. Calling it Bayesian doesn't make it better. For similar reasons, this model will not work well outside the data range.
</p>

<p>
A practical person would still consider using this model, and might even rely on the uncertainty being too large to identify regions of low reliability.
</p>
</div>
</div>
<div id="outline-container-orga7fb619" class="outline-2">
<h2 id="orga7fb619"><span class="section-number-2">3.</span> a better kernel solves these issues</h2>
<div class="outline-text-2" id="text-3">
<p>
Not all is lost, if we know more. In this case we can construct a kernel that reflects our understanding that the data came from a Lennard Jones like interaction model. You can construct kernels by adding and multiplying kernels. Here we consider a linear kernel, the <code>DotProduct</code> kernel, and construct a new kernel that is a sum of the linear kernel to the 12<sup>th</sup> power, a linear kernel to the 6<sup>th</sup> power, and a <code>WhiteKernel</code> for the noise. It is a little subtle that this kernel should work in \(1 / r\) space, so in addition to kernel engineering, we also do feature engineering.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #0000FF; font-weight: bold;">from</span> sklearn.gaussian_process.kernels <span style="color: #0000FF; font-weight: bold;">import</span> DotProduct

<span style="color: #BA36A5;">kernel</span> = DotProduct()**12 + DotProduct()**6 +  WhiteKernel()
<span style="color: #BA36A5;">gpr</span> = GaussianProcessRegressor(kernel=kernel, normalize_y=<span style="color: #D0372D;">True</span>).fit(1 / _R, _y)

plt.plot(_R, _y, <span style="color: #008000;">'b.'</span>)
plt.plot(r, y, <span style="color: #008000;">'b.'</span>, alpha=0.2)


<span style="color: #BA36A5;">yp</span>, <span style="color: #BA36A5;">se</span> = gpr.predict(1 / r[:, <span style="color: #D0372D;">None</span>], return_std=<span style="color: #D0372D;">True</span>)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, <span style="color: #008000;">'k--'</span>, r, yp - 2 * se, <span style="color: #008000;">'k--'</span>);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);

gpr.kernel_
</pre>
</div>

<pre class="example">
DotProduct(sigma_0=0.0281) ** 12 + DotProduct(sigma_0=0.936) ** 6 + WhiteKernel(noise_level=0.0077)
</pre>

<p>
<figure><img src="/media/70e91f8419a473ed578a14442694e67a3409bd1e.png"></figure> 
</p>

<p>
Note that this GPR does fine in the gap, including the right level of uncertainty there. This model is better because we used the kernel to constrain what forms the model can have. This model actually extrapolates correctly outside the data. It is worth noting that although this model has great predictive and UQ properties, it does not tell us anything about the values of &epsilon; and &sigma; in the Lennard Jones model. Although we might say the kernel is physics-based, i.e. it is based on the relevant features and equation, it does not have physical parameters in it.
</p>

<p>
How about those basis functions here? You can see that all of them basically look like the LJ potential. That means they are good basis functions to expand this data set in.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">y_samples</span> = gpr.sample_y(1 / r[:, <span style="color: #D0372D;">None</span>], n_samples=15, random_state=0)

plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)

plt.plot(r, y_samples, <span style="color: #008000;">'k'</span>, alpha=0.2);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/e7fe34a01c52cb228cbbcde85e5f334e7f8237a1.png"></figure> 
</p>
</div>
</div>
<div id="outline-container-orgca7200a" class="outline-2">
<h2 id="orgca7200a"><span class="section-number-2">4.</span> How about with feature engineering?</h2>
<div class="outline-text-2" id="text-4">
<p>
Can we do even better with feature engineering here? Motivated by <a href="https://www.linkedin.com/feed/update/urn:li:activity:7342573774774386688?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7342573774774386688%2C7342949865590530052%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287342949865590530052%2Curn%3Ali%3Aactivity%3A7342573774774386688%29">this comment</a> by Cory Simon, we cast the problem as a linear regression in [1 / r<sup>6</sup>, 1 / r<sup>12</sup>] feature space. This is also a perfectly reasonable thing to do. Since our output is linear in these features, we simply use a linear kernel (aka the DotProduct kernel in sklearn).
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">r6</span> = 1 / _R**6
<span style="color: #BA36A5;">r12</span> = r6**2

<span style="color: #BA36A5;">kernel</span> = DotProduct() + WhiteKernel()

<span style="color: #BA36A5;">gpr</span> = GaussianProcessRegressor(kernel=kernel, normalize_y=<span style="color: #D0372D;">True</span>).fit(np.hstack([r6, r12]), _y)

plt.plot(_R, _y, <span style="color: #008000;">'b.'</span>)
plt.plot(r, y, <span style="color: #008000;">'b.'</span>, alpha=0.2)

<span style="color: #BA36A5;">fr6</span> = 1 / r[:, <span style="color: #D0372D;">None</span>]**6
<span style="color: #BA36A5;">fr12</span> = fr6**2

<span style="color: #BA36A5;">yp</span>, <span style="color: #BA36A5;">se</span> = gpr.predict(np.hstack([fr6, fr12]), return_std=<span style="color: #D0372D;">True</span>)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, <span style="color: #008000;">'k--'</span>, r, yp - 2 * se, <span style="color: #008000;">'k--'</span>);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);

gpr.kernel_
</pre>
</div>

<pre class="example">
DotProduct(sigma_0=0.74) + WhiteKernel(noise_level=0.00654)
</pre>

<p>
<figure><img src="/media/d8769fe652b9e902e3d349ce26cdbd7d8050b190.png"></figure> 
</p>

<p>
We can't easily plot these basis functions the same way, so we reduce them to a 1-d plot. You can see here that these basis functions practically the same as the one with the advanced kernel.
</p>

<div class="org-src-container">
<pre class="src src-jupyter-python"><span style="color: #BA36A5;">y_samples</span> = gpr.sample_y(np.hstack([fr6, fr12]),
                         n_samples=15, random_state=0)

plt.plot(_R, _y, <span style="color: #008000;">'.'</span>)

plt.plot(r, y_samples, <span style="color: #008000;">'k'</span>, alpha=0.2);

plt.xlabel(<span style="color: #008000;">'R'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>);
</pre>
</div>

<p>
<figure><img src="/media/f777586bb8e17bac5ca3dadbfba97119addeb46b.png"></figure> 
</p>



<p>
This also works quite well, and is another way to leverage knowledge about what we are building a model for.
</p>
</div>
</div>
<div id="outline-container-orgdd16d18" class="outline-2">
<h2 id="orgdd16d18"><span class="section-number-2">5.</span> Summary</h2>
<div class="outline-text-2" id="text-5">
<p>
Naive use of GPR can provide useful models when you have enough data, but these models likely do not accurately capture uncertainty outside that data, nor is it likely they are reliable in extrapolation. It is possible to do better than this, when you know what to do. Through feature and kernel engineering, you can sometimes create situations where the problem essentially becomes linear regression, where a simple linear kernel is what you want, or you develop a kernel that represents the underlying model. Kernel engineering is generally hard, with limited opportunities to be flexible. See <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">https://www.cs.toronto.edu/~duvenaud/cookbook/</a> for examples of kernels and combining them.
</p>

<p>
You can see it is not adequate to say "we used Gaussian process regression". That is about as informative as saying linear regression without identifying the features, or nonlinear regression and not saying what model. You have to be specific about the kernel, and thoughtful about how you know if a prediction is reliable or not. Just because you get an uncertainty prediction doesn't mean its right.
</p>
</div>
</div>
<p>Copyright (C) 2025 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2025/06/22/Lies,-damn-lies,-statistics-and-Bayesian-statistics.org">org-mode source</a></p>
<p>Org-mode version = 9.8-pre</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics">Discuss on Twitter</a>

  <hr class="interblog" />
 <a href="../2">Next Page »</a>

          </div>
          <div id="sidebar" class="grid_4">
            <aside>
<section>
<script>
  (function() {
    var cx = '002533177287215655227:l7uvu35ssbc';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</section>

<section>
    <h1 class="post_header_gradient theme_font">Twitter</h1>
    <a class="twitter-timeline" href="https://twitter.com/johnkitchin" data-widget-id="545217643582881792">Tweets by @johnkitchin</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</section>


  <section>
    <h1 class="post_header_gradient theme_font">Links</h1>
    <ul>
      <li><a href="https://www.continuum.io">Anaconda Python</a></li>
      <li><a href="/pycse">Pycse</a></li>
      <li><a href="/dft-book">DFT-book</a></li>
    </ul>
  </section>

  <section>
    <h1 class="post_header_gradient theme_font">Latest Posts</h1>
    <ul>
      <li><a href="/blog/2025/09/16/New-publication-Towards-Agentic-Science-for-Advancing-Scientific-Discovery/">New publication - Towards Agentic Science for Advancing Scientific Discovery</a></li>
      <li><a href="/blog/2025/07/31/New-publication-Mapping-uncertainty-using-differentiable-programming/">New publication - Mapping uncertainty using differentiable programming</a></li>
      <li><a href="/blog/2025/07/17/New-publication-Spin-informed-universal-graph-neural-networks-for-simulating-magnetic-ordering/">New publication - Spin-informed universal graph neural networks for simulating magnetic ordering</a></li>
      <li><a href="/blog/2025/07/09/New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design/">New publication - Hyperplane decision trees as piecewise linear surrogate models for chemical process design</a></li>
      <li><a href="/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics/">Lies, damn lies, statistics and Bayesian statistics</a></li>
    </ul>
  </section>

<section>
<h1 class="post_header_gradient theme_font">Latest GitHub Repos</h1>
  <a href="https://github.com/jkitchin">@jkitchin</a> on GitHub.
  <ul id="my-github-projects">
        <li class="loading">Status updating&#8230;</li>
  </ul>

</section>
</aside>

          </div>
          <div class="clear"></div>
        </div>
      </div>
      
<footer>
  <div id="footer" class="grid_12">
    <div class="grid_8">
      <p>
        <a href="/blog/feed/index.xml">RSS</a>
      </p>
    </div>
    <div class="grid_4" id="credits">
      <p>
        Copyright 2025
        John Kitchin
      </p>
      <p>
        Powered by <a href="http://www.blogofile.com">Blogofile</a>
      </p>
    </div>
  </div>
</footer>

    </div>
      <script src="//ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/js/libs/jquery-1.5.1.min.js"%3E%3C/script%3E'))</script>
  <script src="/js/plugins.js"></script>
  <script src="/js/script.js"></script>
  <script src="/js/jquery.tweet.js"></script>  
  <script src="/js/site.js"></script>
  <!--[if lt IE 7 ]>
  <script src="js/libs/dd_belatedpng.js"></script>
  <script> DD_belatedPNG.fix('img, .png_bg');</script>
  <![endif]-->
 
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PH8NF4F0RE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PH8NF4F0RE');
</script>


  </body>
</html>






<script src="http://ajax.microsoft.com/ajax/jquery/jquery-1.4.2.min.js" type="text/javascript"></script>
<script src="/js/git.js" type="text/javascript"></script>
<script type="text/javascript">
    $(function() {
     $("#my-github-projects").loadRepositories("jkitchin");
    });
</script>



