

<!doctype html>
<!--[if lt IE 7 ]> <html lang="en" class="no-js ie6"> <![endif]-->
<!--[if IE 7 ]>    <html lang="en" class="no-js ie7"> <![endif]-->
<!--[if IE 8 ]>    <html lang="en" class="no-js ie8"> <![endif]-->
<!--[if IE 9 ]>    <html lang="en" class="no-js ie9"> <![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html lang="en" class="no-js"> <!--<![endif]-->
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Kitchin Research Group</title>
  <meta name="google-site-verification" content="CGcacJdHc2YoZyI0Vey9XRA5qwhhFDzThKJezbRFcJ4" />
  <meta name="description" content="Chemical Engineering at Carnegie Mellon University">
  <meta name="author" content="John Kitchin">
  <link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
  <link rel="alternate" type="application/atom+xml" title="Atom 1.0" href="/blog/feed/atom" />
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">

  <link rel="stylesheet" href="/css/base.css?v=1">
  <link rel="stylesheet" href="/css/grid.css?v=1">
  <link rel="stylesheet" media="handheld" href="/css/handheld.css?v=1">
  <link rel="stylesheet" href="/css/pygments_murphy.css" type="text/css" />

  <script src="/js/libs/modernizr-1.7.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
  <link rel="stylesheet" href="/themes/theme1/style.css?v=1">
<link href='http://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>

</head>
  <body>
    <div id="container" class="container container_12">
      <div id="main" role="main">
        <div id="main_block">
          <header>
<div id="header" class="header_gradient theme_font">
<table><tr><td>
    <h1><a href="/">The Kitchin Research Group</a></h1>
    <h2>Chemical Engineering at Carnegie Mellon University</h2>
</td>
<td colspan=100%><div style="float:right;width:100%;text-align:right;"> <span id='badgeCont737515' style='width:126px'><script src='http://labs.researcherid.com/mashlets?el=badgeCont737515&mashlet=badge&showTitle=false&className=a&rid=A-2363-2010'></script></span></div>
</td></tr>
</table>
</div>
  <div id="navigation" class="grid_12">

    <ul class="theme_font">
      <li><a href="/blog"
             class="">Blog</a></li>

      <li><a href="/blog/archive"
             class="">Archives</a></li>

      <li><a href="/publications.html">Publications</a></li>

      <li><a href="/research.html"
             class="">Research</a></li>

      <li><a href="/categories.html"
             class="">Categories</a></li>

      <li><a href="/about.html"
             class="">About us</a></li>

      <li><a href="/subscribe.html">Subscribe</a></li>

    </ul>
  </div>
</header>

          <div id="prose_block" class="grid_8">
            
  





<article>
  <div class="blog_post">
    <header>
      <div id="Using-autograd-in-nonlinear-regression"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/11/17/Using-autograd-in-nonlinear-regression/" rel="bookmark" title="Permanent Link to Using autograd in nonlinear regression">Using autograd in nonlinear regression</a></h2>
      <p><small><span class="blog_post_date">Posted November 17, 2017 at 07:49 AM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/autograd/'>autograd</a>, <a href='/blog/category/regression/'>regression</a>, <a href='/blog/category/python/'>python</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
Table <a href="#raw-data">raw-data</a> contains the energy as a function of volume for some solid material from a set of density functional theory calculations. Our goal is to fit the Murnaghan equation of state to this data. The model is moderately nonlinear. I have previously done this with the standard nonlinear regression functions in scipy, so today we will use autograd along with a builtin optimizer to minimize an objective function to achieve the same thing. 
</p>

<p>
The basic idea is we define an objective function, in this case the summed squared errors between predicted values from the model and known values from our data. The objective function takes two arguments: the model parameters, and the "step". This function signature is a consequence of the built in optimizer we use; it expects that signature (it is useful for batch training, but we will not use that here).  We use autograd to create a gradient of the objective function which the adam optimizer will use to vary the parameters with the goal of minimizing the objective function.
</p>

<p>
The adam optimizer function takes as one argument a callback function, which we call <code>summary</code> to print out intermediate results during the convergence. We run the optimizer in a loop because the optimizer runs a fixed number of steps on each call. We check if the objective function is sufficiently small, and if it is we break out. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org7bbe046"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> grad
<span style="color: #0000FF;">from</span> autograd.misc.optimizers <span style="color: #0000FF;">import</span> adam

np.set_printoptions(precision=3, suppress=<span style="color: #D0372D;">True</span>)

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">input data</span>
<span style="color: #BA36A5;">Vinput</span> = np.array([row[0] <span style="color: #0000FF;">for</span> row <span style="color: #0000FF;">in</span> data]) 
<span style="color: #BA36A5;">Eknown</span> = np.array([row[1] <span style="color: #0000FF;">for</span> row <span style="color: #0000FF;">in</span> data])

<span style="color: #0000FF;">def</span> <span style="color: #006699;">Murnaghan</span>(pars, vol):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">'''</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span><span style="color: #036A07;">   given a vector of parameters and volumes, return a vector of energies.</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span><span style="color: #036A07;">   equation From PRB 28,5480 (1983)</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span><span style="color: #036A07;">   '''</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">E0</span>, <span style="color: #BA36A5;">B0</span>, <span style="color: #BA36A5;">BP</span>, <span style="color: #BA36A5;">V0</span> = pars
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">E</span> = E0 + B0 * vol / BP * (((V0 / vol)**BP) / (BP - 1.0) + 1.0) - V0 * B0 / (BP - 1.)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> E

<span style="color: #0000FF;">def</span> <span style="color: #006699;">objective</span>(pars, step):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"This is what we want to minimize by varying the pars."</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">predicted</span> = Murnaghan(pars, Vinput)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Note Eknown is not defined in this function scope</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">errors</span> = Eknown - predicted
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> np.<span style="color: #006FE0;">sum</span>(errors**2)

<span style="color: #BA36A5;">objective_grad</span> = grad(objective)

<span style="color: #0000FF;">def</span> <span style="color: #006699;">summary</span>(pars, step, gradient):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Note i, N are not defined in this function scope</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> step % N == 0: 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">'step {0:5d}: {1:1.3e}'</span>.<span style="color: #006FE0;">format</span>(i * N + step, 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>objective(pars, step)))

<span style="color: #BA36A5;">pars</span> = np.array([-400, 0.5, 2, 210]) <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">The initial guess</span>
<span style="color: #BA36A5;">N</span> = 200 <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">num of steps to take on each optimization</span>
<span style="color: #BA36A5;">learning_rate</span> = 0.001
<span style="color: #0000FF;">for</span> i <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(100):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">pars</span> = adam(objective_grad, pars, step_size=learning_rate, 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   num_iters=N, callback=summary)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">SSE</span> = objective(pars, <span style="color: #D0372D;">None</span>)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> SSE &lt; 0.00002:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">'Tolerance met.'</span>, SSE)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">break</span>
<span style="color: #0000FF;">print</span>(pars)
</pre>
</div>

<pre class="example">
step     0: 3.127e+02
step   200: 1.138e+02
step   400: 2.011e+01
step   600: 1.384e+00
step   800: 1.753e-01
step  1000: 2.044e-03
step  1200: 1.640e-03
step  1400: 1.311e-03
step  1600: 1.024e-03
step  1800: 7.765e-04
step  2000: 5.698e-04
step  2200: 4.025e-04
step  2400: 2.724e-04
step  2600: 1.762e-04
step  2800: 1.095e-04
step  3000: 6.656e-05
step  3200: 3.871e-05
step  3400: 2.359e-05
('Tolerance met.', 1.5768901008364176e-05)
[-400.029    0.004    4.032  211.847]

</pre>

<p>
There are some subtleties in the code above. One is the variables that are used kind of all over the place, which is noted in a few places. Those could get tricky to keep track of. Another is the variable I called learning_rate. I borrowed that terminology from the machine learning community. It is the <code>step_size</code> in this implementation of the optimizer. If you make it too large, the objective function doesn't converge, but if you set it too small, it will take a long time to converge. Note that it took at about 3400 steps of "training". This is a lot more than is typically required by something like <code>pycse.nlinfit</code>. This isn't the typical application for this approach to regression. More on that another day.
</p>

<p>
As with any fit, it is wise to check it out at least graphically. Here is the fit and data.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org0d237fb">%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib
matplotlib.rc(<span style="color: #008000;">'axes.formatter'</span>, useoffset=<span style="color: #D0372D;">False</span>)
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

plt.plot(Vinput, Eknown, <span style="color: #008000;">'ko'</span>, label=<span style="color: #008000;">'known'</span>)

<span style="color: #BA36A5;">vinterp</span> = np.linspace(Vinput.<span style="color: #006FE0;">min</span>(), Vinput.<span style="color: #006FE0;">max</span>(), 200)

plt.plot(vinterp, Murnaghan(pars, vinterp), <span style="color: #008000;">'r-'</span>, label=<span style="color: #008000;">'predicted'</span>)
plt.xlabel(<span style="color: #008000;">'Vol'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>)
</pre>
</div>

<p>
<img src="/media/ob-ipython-f106274e2be904c3f20c4c20ec425ebd.png"> 
</p>

<p>
The fit looks pretty good.
</p>


<table id="org71d3fa4" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Volume-Energy data for a solid state system.</caption>

<colgroup>
<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">volume</th>
<th scope="col" class="org-right">energy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">324.85990899</td>
<td class="org-right">-399.9731688470</td>
</tr>

<tr>
<td class="org-right">253.43999457</td>
<td class="org-right">-400.0172393178</td>
</tr>

<tr>
<td class="org-right">234.03826687</td>
<td class="org-right">-400.0256270548</td>
</tr>

<tr>
<td class="org-right">231.12159387</td>
<td class="org-right">-400.0265690700</td>
</tr>

<tr>
<td class="org-right">228.40609504</td>
<td class="org-right">-400.0273551120</td>
</tr>

<tr>
<td class="org-right">225.86490337</td>
<td class="org-right">-400.0280030862</td>
</tr>

<tr>
<td class="org-right">223.47556626</td>
<td class="org-right">-400.0285313450</td>
</tr>

<tr>
<td class="org-right">221.21992353</td>
<td class="org-right">-400.0289534593</td>
</tr>

<tr>
<td class="org-right">219.08319566</td>
<td class="org-right">-400.0292800709</td>
</tr>

<tr>
<td class="org-right">217.05369547</td>
<td class="org-right">-400.0295224970</td>
</tr>

<tr>
<td class="org-right">215.12089909</td>
<td class="org-right">-400.0296863867</td>
</tr>

<tr>
<td class="org-right">213.27525144</td>
<td class="org-right">-400.0297809256</td>
</tr>

<tr>
<td class="org-right">211.51060823</td>
<td class="org-right">-400.0298110000</td>
</tr>

<tr>
<td class="org-right">203.66743321</td>
<td class="org-right">-400.0291665573</td>
</tr>

<tr>
<td class="org-right">197.07888649</td>
<td class="org-right">-400.0275017142</td>
</tr>

<tr>
<td class="org-right">191.39717952</td>
<td class="org-right">-400.0250998136</td>
</tr>

<tr>
<td class="org-right">186.40163591</td>
<td class="org-right">-400.0221371852</td>
</tr>

<tr>
<td class="org-right">181.94435510</td>
<td class="org-right">-400.0187369863</td>
</tr>

<tr>
<td class="org-right">177.92077043</td>
<td class="org-right">-400.0149820198</td>
</tr>

<tr>
<td class="org-right">174.25380090</td>
<td class="org-right">-400.0109367042</td>
</tr>

<tr>
<td class="org-right">170.88582166</td>
<td class="org-right">-400.0066495100</td>
</tr>

<tr>
<td class="org-right">167.76711189</td>
<td class="org-right">-400.0021478258</td>
</tr>

<tr>
<td class="org-right">164.87096104</td>
<td class="org-right">-399.9974753449</td>
</tr>

<tr>
<td class="org-right">159.62553397</td>
<td class="org-right">-399.9876885136</td>
</tr>

<tr>
<td class="org-right">154.97005460</td>
<td class="org-right">-399.9774175487</td>
</tr>

<tr>
<td class="org-right">150.78475335</td>
<td class="org-right">-399.9667603369</td>
</tr>

<tr>
<td class="org-right">146.97722201</td>
<td class="org-right">-399.9557686286</td>
</tr>

<tr>
<td class="org-right">143.49380641</td>
<td class="org-right">-399.9445262604</td>
</tr>
</tbody>
</table>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/17/Using-autograd-in-nonlinear-regression.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/11/17/Using-autograd-in-nonlinear-regression">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="Sensitivity-analysis-using-automatic-differentiation-in-Python"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/11/15/Sensitivity-analysis-using-automatic-differentiation-in-Python/" rel="bookmark" title="Permanent Link to Sensitivity analysis using automatic differentiation in Python">Sensitivity analysis using automatic differentiation in Python</a></h2>
      <p><small><span class="blog_post_date">Posted November 15, 2017 at 08:34 AM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/sensitivity/'>sensitivity</a>, <a href='/blog/category/autograd/'>autograd</a>, <a href='/blog/category/python/'>python</a></span> | tags: 
      <p><small><span class="blog_post_date">Updated November 15, 2017 at 08:41 AM</span></small>
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
This <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.428.6699&amp;rep=rep1&amp;type=pdf">paper</a> describes how sensitivity analysis requires access to the derivatives of a function. Say, for example we have a function describing the time evolution of the concentration of species A:
</p>

<p>
\([A] = \frac{[A]_0}{k_1 + k_{-1}} (k_1 e^{(-(k_1 _ k_{-1})t)} + k_{-1})\)
</p>

<p>
The local sensitivity of the concentration of A to the parameters \(k1\) and \(k_1\) are defined as \(\frac{\partial A}{\partial k1}\) and \(\frac{\partial A}{\partial k_1}\). Our goal is to plot the sensitivity as a function of time. We could derive those derivatives, but we will use auto-differentiation instead through the autograd package. Here we import numpy from the autograd package and plot the function above.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org5d5b53b"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np

<span style="color: #BA36A5;">A0</span> = 1.0

<span style="color: #0000FF;">def</span> <span style="color: #006699;">A</span>(t, k1, k_1):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> A0 / (k1 + k_1) * (k1 * np.exp(-(k1 + k_1) * t) + k_1)

%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

<span style="color: #BA36A5;">t</span> = np.linspace(0, 0.5)

<span style="color: #BA36A5;">k1</span> = 3.0
<span style="color: #BA36A5;">k_1</span> = 3.0
plt.plot(t, A(t, k1, k_1))
plt.xlim([0, 0.5])
plt.ylim([0, 1])
plt.xlabel(<span style="color: #008000;">'t'</span>)
plt.ylabel(<span style="color: #008000;">'A'</span>)
</pre>
</div>

<p>
<img src="/media/ob-ipython-09dd39779fdcdb6e3f00397800ec05e6.png"> 
</p>

<p>
The figure above reproduces Fig. 1 from the paper referenced above.  Next, we use autograd to get the derivatives. This is subtly different than our previous <a href="http://kitchingroup.cheme.cmu.edu/blog/2017/11/14/Forces-by-automatic-differentiation-in-molecular-simulation/">post</a>. First, we need the derivative of the function with respect to the second and third arguments; the default is the first argument. Second, we want to evaluate this derivative at each time value. We use the jacobian function in autograd to get these. This is different than grad, which will sum up the derivatives at each time. That might be useful for regression, but not for sensitivity analysis. Finally, to reproduce Figure 2a, we plot the absolute value of the sensitivities.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org194abad"><span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> jacobian

<span style="color: #BA36A5;">dAdk1</span> = jacobian(A, 1)
<span style="color: #BA36A5;">dAdk_1</span> = jacobian(A, 2)

plt.plot(t, np.<span style="color: #006FE0;">abs</span>(dAdk1(t, k1, k_1)))
plt.plot(t, np.<span style="color: #006FE0;">abs</span>(dAdk_1(t, k1, k_1)))
plt.xlim([0, 0.5])
plt.ylim([0, 0.1])
plt.xlabel(<span style="color: #008000;">'t'</span>)
plt.legend([<span style="color: #008000;">'$S_{k1}$'</span>, <span style="color: #008000;">'$S_{k\_1}$'</span>])
</pre>
</div>

<p>
<img src="/media/ob-ipython-f3534f038e5e3a7c77041501838e9fdb.png"> 
</p>

<p>
That looks like the figure in the paper. To summarize the main takeaway, autograd enabled us to readily compute derivatives without having to derive them manually. There was a little subtlety in choosing jacobian over grad or elementwise_grad but once you know what these do, it seems reasonable. It is important to import the wrapped numpy first, to enable autograd to do its work. All the functions here are pretty standard, so everything worked out of the box. We should probably be using autograd, or something like it for more things in science!
</p>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/15/Sensitivity-analysis-using-automatic-differentiation-in-Python.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/11/15/Sensitivity-analysis-using-automatic-differentiation-in-Python">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="Forces-by-automatic-differentiation-in-molecular-simulation"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/11/14/Forces-by-automatic-differentiation-in-molecular-simulation/" rel="bookmark" title="Permanent Link to Forces by automatic differentiation in molecular simulation">Forces by automatic differentiation in molecular simulation</a></h2>
      <p><small><span class="blog_post_date">Posted November 14, 2017 at 09:06 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/autograd/'>autograd</a>, <a href='/blog/category/simulation/'>simulation</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
In molecular simulation we often use a potential to compute the total energy of a system. For example, we might use something simple like a <a href="https://en.wikipedia.org/wiki/Lennard-Jones_potential">Lennard-Jones</a> potential. If we have a potential function, e.g. \(E = V(R)\) where \(R\) are the positions of the atoms, then we know the forces on the atoms are defined by \(f = -\frac{dV}{dR}\). For simple functions, you can derive the derivative pretty easily, but these functions quickly get complicated. In this post, we consider <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a> as implemented by <a href="https://github.com/HIPS/autograd">autograd</a>. This is neither symbolic nor numerical differentiation. The gist is that the program uses the chain rule to evaluate derivatives. Here we do not delve into how it is done, we just see how it might help us in molecular simulation.
</p>

<p>
For reference, here is a result from the LennardJones calculator in ASE.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org610d524"><span style="color: #0000FF;">from</span> ase.calculators.lj <span style="color: #0000FF;">import</span> LennardJones
<span style="color: #0000FF;">from</span> ase.cluster.icosahedron <span style="color: #0000FF;">import</span> Icosahedron

<span style="color: #BA36A5;">atoms</span> = Icosahedron(<span style="color: #008000;">'Ar'</span>, noshells=2, latticeconstant=3)
atoms.set_calculator(LennardJones())

atoms.rattle(0.5)
<span style="color: #0000FF;">print</span>(<span style="color: #008000;">'LJ: '</span>, atoms.get_potential_energy())
</pre>
</div>

<pre class="example">
('LJ: ', -3.3553466825679812)

</pre>

<p>
First, we define a function for the Lennard-Jones potential. I adapted the code <a href="https://wiki.fysik.dtu.dk/ase/_modules/ase/calculators/lj.html#LennardJones">here</a> to implement a function that calculates the Lennard-Jones energy for a cluster of atoms with no periodic boundary conditions. <i>Instead</i> of using numpy directly, we import it from the autograd package which puts thin wrappers around the functions to enable the derivative calculations. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org022fb09"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np

<span style="color: #0000FF;">def</span> <span style="color: #006699;">energy</span>(positions):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"Compute the energy of a Lennard-Jones system."</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">natoms</span> = <span style="color: #006FE0;">len</span>(positions)

<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">sigma</span> = 1.0
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">epsilon</span> = 1.0
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">rc</span> = 3 * sigma

<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">e0</span> = 4 * epsilon * ((sigma / rc)**12 - (sigma / rc)**6)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">energy</span> = 0.0
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> a1 <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(natoms):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> j <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(a1 + 1, natoms):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">r2</span> = np.<span style="color: #006FE0;">sum</span>((positions[a1] - positions[j])**2)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> r2 &lt;= rc**2:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">c6</span> = (sigma**2 / r2)**3
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">energy</span> -= e0 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">c12</span> = c6**2
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">energy</span> += 4 * epsilon * (c12 - c6)

<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> energy
</pre>
</div>

<p>
Here is our function in action, and it produces the same result as the ASE calculator. So far there is nothing new.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org4bd1401"><span style="color: #0000FF;">print</span>(<span style="color: #008000;">'our func: '</span>, energy(atoms.positions))
</pre>
</div>

<pre class="example">
('our func: ', -3.3553466825679803)

</pre>

<p>
Now, we look at the forces from the ASE calculator. If you look at the ASE <a href="https://wiki.fysik.dtu.dk/ase/_modules/ase/calculators/lj.html#LennardJones">code</a> you will see that the formula for forces was analytically derived and accumulated in the loop.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org6dc5318">np.set_printoptions(precision=3, suppress=<span style="color: #D0372D;">True</span>)
<span style="color: #0000FF;">print</span>(atoms.get_forces())
</pre>
</div>

<pre class="example">
[[ 0.545  1.667  0.721]
 [-0.068  0.002  0.121]
 [-0.18   0.018 -0.121]
 [ 0.902 -0.874 -0.083]
 [ 0.901 -0.937 -1.815]
 [ 0.243 -0.19   0.063]
 [-0.952 -1.776 -0.404]
 [-0.562  1.822  1.178]
 [-0.235  0.231  0.081]
 [-0.023  0.204 -0.294]
 [ 0.221 -0.342 -0.425]
 [-5.385 -6.017  1.236]
 [ 4.593  6.193 -0.258]]

</pre>

<p>
Now we look at how to use autograd for this purpose. We want an element-wise gradient of the total energy with respect to the positions. autograd returns functions, so we wrap it in another function so we can take the negative of that function.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org2210bf9"><span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> elementwise_grad

<span style="color: #0000FF;">def</span> <span style="color: #006699;">forces</span>(pos):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">dEdR</span> = elementwise_grad(energy)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> -dEdR(pos)

<span style="color: #0000FF;">print</span>(forces(atoms.positions))
</pre>
</div>

<pre class="example">
[[ 0.545  1.667  0.721]
 [-0.068  0.002  0.121]
 [-0.18   0.018 -0.121]
 [ 0.902 -0.874 -0.083]
 [ 0.901 -0.937 -1.815]
 [ 0.243 -0.19   0.063]
 [-0.952 -1.776 -0.404]
 [-0.562  1.822  1.178]
 [-0.235  0.231  0.081]
 [-0.023  0.204 -0.294]
 [ 0.221 -0.342 -0.425]
 [-5.385 -6.017  1.236]
 [ 4.593  6.193 -0.258]]

</pre>

<p>
Here we show the results are the same from both approaches.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgc41aef4"><span style="color: #0000FF;">print</span>(np.allclose(atoms.get_forces(), forces(atoms.positions)))
</pre>
</div>

<pre class="example">
True

</pre>

<p>
Wow. We got forces without deriving a derivative, or using numerical finite differences, across loops, and conditionals. That is pretty awesome. You can easily modify the potential function now, without the need to rederive the force derivatives! This is an idea worth exploring further. In principle, it should be possible to include periodic boundary conditions and use autograd to compute stresses too. Maybe that will be a future post.
</p>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/14/Forces-by-automatic-differentiation-in-molecular-simulation.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/11/14/Forces-by-automatic-differentiation-in-molecular-simulation">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="New-publication-in-J-Phys-Chem-Lett"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/09/25/New-publication-in-J-Phys-Chem-Lett/" rel="bookmark" title="Permanent Link to New publication in J. Phys. Chem. Lett.">New publication in J. Phys. Chem. Lett.</a></h2>
      <p><small><span class="blog_post_date">Posted September 25, 2017 at 08:24 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/news/'>news</a>, <a href='/blog/category/publication/'>publication</a></span> | tags: 
      <p><small><span class="blog_post_date">Updated September 26, 2017 at 07:16 AM</span></small>
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
DFT calculations are extensively used to predict the chemical properties of metal alloy surfaces, but they are expensive which limits the number of calculations that can be practically be calculated. In this paper, we explore a perturbation approach known as alchemy to take previously calculated results and extend them to new compositions. We use oxygen reduction as a prototype reaction, and show that alchemy is often much faster than DFT, with an accuracy within 0.1 eV of the DFT. There are cases where the accuracy is not as good suggesting that further improvements to the perturbation model could be beneficial. Overall, alchemy appears to be a useful tool in high-throughput screening research.
</p>

<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #006699;">@article</span>{<span style="color: #D0372D;">saravanan-2017-alchem-predic</span>,
  <span style="color: #BA36A5;">title</span> =        {Alchemical Predictions for Computational Catalysis: Potential
                  and Limitations},
  <span style="color: #BA36A5;">year</span> =         2017,
  <span style="color: #BA36A5;">Author</span> =       {Saravanan, Karthikeyan and Kitchin, John R. and von
                  Lilienfeld, O. Anatole and Keith, John A.},
  <span style="color: #BA36A5;">Bdsk-Url-1</span> =   {https://doi.org/10.1021/acs.jpclett.7b01974},
  <span style="color: #BA36A5;">Doi</span> =          {<span style="color: #006DAF; text-decoration: underline;">10.1021/acs.jpclett.7b01974</span>},
  <span style="color: #BA36A5;">Eprint</span> =       {https://doi.org/10.1021/acs.jpclett.7b01974},
  <span style="color: #BA36A5;">Journal</span> =      {The Journal of Physical Chemistry Letters},
  <span style="color: #BA36A5;">Note</span> =         {PMID: 28938798},
  <span style="color: #BA36A5;">Number</span> =       {ja},
  <span style="color: #BA36A5;">Pages</span> =        {null},
  <span style="color: #BA36A5;">Url</span> =          {<span style="color: #006DAF; text-decoration: underline;">https://doi.org/10.1021/acs.jpclett.7b01974</span>},
  <span style="color: #BA36A5;">Volume</span> =       0,
}

</pre>
</div>

<p>
<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
<div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1021/acs.jpclett.7b01974'></div>
</p>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/09/25/New-publication-in-J.-Phys.-Chem.-Lett..org">org-mode source</a></p>
<p>Org-mode version = 9.0.7</p>


    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/09/25/New-publication-in-J-Phys-Chem-Lett">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="Finding-similar-bibtex-entries"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/09/16/Finding-similar-bibtex-entries/" rel="bookmark" title="Permanent Link to Finding similar bibtex entries">Finding similar bibtex entries</a></h2>
      <p><small><span class="blog_post_date">Posted September 16, 2017 at 10:00 AM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/bibtex/'>bibtex</a>, <a href='/blog/category/similarity/'>similarity</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
A common task while writing scientific papers is citing previous research. I use org-ref extensively for that, and it makes it pretty easy to find similar references, e.g. that have common authors, or common keywords. It also lets me find similar articles in Web of Science or Scopus. Suppose that I have cited a particular paper, e.g. e <a class='org-ref-reference' href="#boes-2016-neural-networ">boes-2016-neural-networ</a>, and I want to find similar references to it that are <i>already</i> in my bibtex file, and similar by <i>my definition</i>. With org-ref I can easily search by keyword or author to find similar entries, but these are limited by what I search for, and they are not sorted. Today, I will explore the first step in a recommender system that calculates similarity, and provides a sorted list of candidates with the most relevant ones first.
</p>

<p>
The idea is to calculate some measure of similarity between the title of that reference, and the titles of other references in my bibtex file, and then sort them by similarity. This is the reference I want to find similar entries for:
</p>

<p>
Boes, J. R., Groenenboom, M. C., Keith, J. A., &amp; Kitchin, J. R., Neural network and Reaxff comparison for Au properties, Int. J. Quantum Chem., 116(13), 979–987 (2016).  <a href="https://doi.org/10.1002/qua.25115">https://doi.org/10.1002/qua.25115</a>
</p>

<p>
The first thing we do is read in our bibtex file, and print a representative entry.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">import</span> bibtexparser
<span style="color: #0000FF;">from</span> bibtexparser.bparser <span style="color: #0000FF;">import</span> BibTexParser

<span style="color: #0000FF;">with</span> <span style="color: #006FE0;">open</span>(<span style="color: #008000;">'/Users/jkitchin/Dropbox/bibliography/references.bib'</span>) <span style="color: #0000FF;">as</span> bibtex_file:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">parser</span> = BibTexParser()
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">bib_database</span> = bibtexparser.load(bibtex_file, parser=parser)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">entries</span> = bib_database.entries

<span style="color: #0000FF;">print</span>(entries[10])
</pre>
</div>

<p>
{'author': 'Jaan Aarik and Aleks Aidla and V{\\"a}ino Sammelselg and Teet\nUustare', 'title': 'Effect of Growth Conditions on Formation of \\ce{TiO_2}-{II}\nThin Films in Atomic Layer Deposition Process', 'journal': 'Journal of Crystal Growth', 'volume': '181', 'number': '3', 'pages': '259 - 264', 'year': '1997', 'doi': '10.1016/S0022-0248(97)00279-0', 'link': 'http://www.sciencedirect.com/science/article/pii/S0022024897002790', 'issn': '0022-0248', 'ENTRYTYPE': 'article', 'ID': 'aarik-1997-effec-growt'}
</p>

<p>
Each entry is a dictionary containing the fields and their values. For this exploration, I will only consider similarities between titles. The next step is we find which entry corresponds to the reference we want to find similarities to.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">ids</span> = [e[<span style="color: #008000;">'ID'</span>] <span style="color: #0000FF;">for</span> e <span style="color: #0000FF;">in</span> entries]
<span style="color: #BA36A5;">i</span> = ids.index(<span style="color: #008000;">'boes-2016-neural-networ'</span>)
<span style="color: #0000FF;">print</span>(entries[i])
</pre>
</div>

<p>
{'author': 'Jacob R. Boes and Mitchell C. Groenenboom and John A. Keith\nand John R. Kitchin', 'title': 'Neural Network and {Reaxff} Comparison for {Au} Properties', 'journal': 'Int. J. Quantum Chem.', 'volume': '116', 'number': '13', 'pages': '979-987', 'year': '2016', 'doi': '10.1002/qua.25115', 'link': 'https://doi.org/10.1002/qua.25115', 'issn': '1097-461X', 'keyword': 'Kohn-Sham density functional theory, neural networks, reactive\nforce fields, potential energy surfaces, machine learning', 'ENTRYTYPE': 'article', 'ID': 'boes-2016-neural-networ'}
</p>

<p>
It is best if we make the entry we want to find similarities to the first one, so here we swap the first and i<sup>th</sup> entries.
</p>

<div class="org-src-container">
<pre class="src src-ipython">entries[0], <span style="color: #BA36A5;">entries</span>[i] = entries[i], entries[0]
</pre>
</div>

<p>
Now, we prepare the list of strings to get similarities for.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #BA36A5;">titles</span> = [e.get(<span style="color: #008000;">'title'</span>, <span style="color: #008000;">''</span>) <span style="color: #0000FF;">for</span> e <span style="color: #0000FF;">in</span> entries]
</pre>
</div>


<p>
We will use <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">term frequency–inverse document frequency</a> to get a vector that represents each title, and then use <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> as a measure of similarity. Here is the place to note that <i>I chose</i> these, and could choose other ones too. Also, it is worth noting that in this measure of similarity I did <i>not</i> choose which keywords to measure similarity on.
</p>

<p>
The functionality for this is provided by <a href="http://scikit-learn.org/stable/">sklearn</a>. It has implemented functions for the algorithms above, and in just a few lines of code you get an array of tf-idf features to analyze. The array we get from our vectorizer contains normalized vectors, so we can get the cosine similarity just from a dot product of the vectors. The first row corresponds to the similarity of the first string to all the others. I want them sorted in descending order. The argsort function returns ascending order, so we use a trick to sort the negative of the similarity score which achieves that. There are certainly more advanced treatments of the text we could use by <a href="http://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes">customizing the vectorizer</a>, e.g. word stemming, but for now we neglect that.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">from</span> sklearn.feature_extraction.text <span style="color: #0000FF;">import</span> TfidfVectorizer

<span style="color: #BA36A5;">vectorizer</span> = TfidfVectorizer(stop_words=<span style="color: #008000;">'english'</span>)
<span style="color: #BA36A5;">X</span> = vectorizer.fit_transform(titles)

<span style="color: #BA36A5;">cosine_similarities</span> = (X * X.T).A[0]

<span style="color: #BA36A5;">related_docs_indices</span> = (-cosine_similarities).argsort()

<span style="color: #0000FF;">print</span>(<span style="color: #008000;">'The top 10 recommendations for {} are:\n'</span>.<span style="color: #006FE0;">format</span>(S[0]))
<span style="color: #0000FF;">for</span> i, j <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">enumerate</span>(related_docs_indices[1:11]):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">'{i}. {ID}: {title}, {author}\n'</span>.<span style="color: #006FE0;">format</span>(i=i + 1, **entries[j]))
</pre>
</div>

<p>
The top 10 recommendations for Neural Network and {Reaxff} Comparison for {Au} Properties are:
</p>

<ol class="org-ol">
<li>behler-2010-neural: Neural network potential-energy surfaces for atomistic</li>
</ol>
<p>
simulations, J{\"o}rg Behler
</p>

<ol class="org-ol">
<li>boes-2017-neural-networ: Neural Network Predictions of Oxygen Interactions on a Dynamic</li>
</ol>
<p>
{Pd} Surface, Jacob R. Boes and John R. Kitchin
</p>

<ol class="org-ol">
<li>eshet-2010-ab: Ab Initio Quality Neural-Network Potential for Sodium, Hagai Eshet and Rustam Z. Khaliullin and Thomas D. K{\"u}hne</li>
</ol>
<p>
and J{\"o}rg Behler and Michele Parrinello
</p>

<ol class="org-ol">
<li>behler-2014-repres-poten: Representing Potential Energy Surfaces By High-Dimensional</li>
</ol>
<p>
Neural Network Potentials, J Behler
</p>

<ol class="org-ol">
<li>behler-2007-gener-neural: Generalized Neural-Network Representation of High-Dimensional</li>
</ol>
<p>
Potential-Energy Surfaces, J{\"o}rg Behler and Michele Parrinello
</p>

<ol class="org-ol">
<li>artrith-2012-high: High-Dimensional Neural Network Potentials for Metal Surfaces:</li>
</ol>
<p>
A Prototype Study for Copper, Nongnuch Artrith and J{\"o}rg Behler
</p>

<ol class="org-ol">
<li>behler-2015-const: Constructing High-Dimensional Neural Network Potentials: A</li>
</ol>
<p>
Tutorial Review, J{\"o}rg Behler
</p>

<ol class="org-ol">
<li>artrith-2011-high: High-Dimensional Neural-Network Potentials for Multicomponent</li>
</ol>
<p>
Systems: Applications To Zinc Oxide, Nongnuch Artrith and Tobias Morawietz and J{\"o}rg Behler
</p>

<ol class="org-ol">
<li>sosso-2012-neural-gete: Neural Network Interatomic Potential for the Phase Change</li>
</ol>
<p>
Material \ce{GeTe}, Gabriele C. Sosso and Giacomo Miceli and Sebastiano Caravati
and J{\"o}rg Behler and Marco Bernasconi
</p>

<ol class="org-ol">
<li>lorenz-2006-descr: Descriptions of Surface Chemical Reactions Using a Neural</li>
</ol>
<p>
Network Representation of the Potential-Energy Surface, S{\"o}nke Lorenz and Matthias Scheffler and Axel Gross
</p>

<p>
It is evident that this is showing other references containing the words "neural network"! I guess that is a little disappointing, since these would just as easily been narrowed down in org-ref. On the other hand, they are sorted and grouped, which would not happen in org-ref. This is a comparison of pretty short strings (just the titles), so maybe this would be much more interesting if abstracts were also included. Including authors would give a different set as well (I tried it, and got a bunch of my own references!).
</p>

<p>
I don't think it would be very difficult to get this into an Emacs selection tool, e.g. helm/ivy. Check this out:
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #0000FF;">import</span> pycse.lisp

related_docs_indices[1:6].lisp
</pre>
</div>

<p>

</p>

<p>
'(1592 1650 299 1751 103)'
</p>


<p>
That is a result that can be read directly by lisp, so we could simply write the code above as a shell script that takes an argument, and returns a list of indices to sort the candidates on. The alternative is to implement this in elisp, perhaps via a dynamic module if there is already a good C library for this. My sense is the Python libraries are more advanced in functionality.
</p>

<p>
This could have a number of other applications. Given some reference content, you could imagine finding emails that are similar to it, finding RSS entries that are similar to it, finding org headlines that are related, similar files, or similarity with any other set of strings that can be gathered, e.g. from Crossref or some other search, etc. I predict there will be more on these topics in the future!
</p>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/09/16/Finding-similar-bibtex-entries.org">org-mode source</a></p>
<p>Org-mode version = 9.0.7</p>


    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/09/16/Finding-similar-bibtex-entries">Discuss on Twitter</a>

  <hr class="interblog" />
 <a href="../11">« Previous Page</a>
  --  
 <a href="../13">Next Page »</a>

          </div>
          <div id="sidebar" class="grid_4">
            <aside>
<section>
<script>
  (function() {
    var cx = '002533177287215655227:l7uvu35ssbc';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</section>

<section>
    <h1 class="post_header_gradient theme_font">Twitter</h1>
    <a class="twitter-timeline" href="https://twitter.com/johnkitchin" data-widget-id="545217643582881792">Tweets by @johnkitchin</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</section>


  <section>
    <h1 class="post_header_gradient theme_font">Links</h1>
    <ul>
      <li><a href="https://www.continuum.io">Anaconda Python</a></li>
      <li><a href="/pycse">Pycse</a></li>
      <li><a href="/dft-book">DFT-book</a></li>
    </ul>
  </section>

  <section>
    <h1 class="post_header_gradient theme_font">Latest Posts</h1>
    <ul>
      <li><a href="/blog/2023/01/01/2022-in-a-nutshell/">2022 in a nutshell</a></li>
      <li><a href="/blog/2022/09/29/New-publication-Identifying-limitations-in-screening-high-throughput-photocatalytic-bimetallic-nanoparticles-with-machine-learned-hydrogen-adsorptions/">New publication - Identifying limitations in screening high-throughput photocatalytic bimetallic nanoparticles with machine-learned hydrogen adsorptions</a></li>
      <li><a href="/blog/2022/09/12/New-publication-Neural-network-embeddings-based-similarity-search-method-for-atomistic-systems/">New publication - Neural network embeddings based similarity search method for atomistic systems</a></li>
      <li><a href="/blog/2022/03/07/New-publication-Evaluation-of-the-Degree-of-Rate-Control-via-Automatic-Differentiation/">New publication - Evaluation of the Degree of Rate Control via Automatic Differentiation</a></li>
      <li><a href="/blog/2022/03/06/New-publication-Model-Specific-to-Model-General-Uncertainty-for-Physical-Properties/">New publication - Model-Specific to Model-General Uncertainty for Physical Properties</a></li>
    </ul>
  </section>

<section>
<h1 class="post_header_gradient theme_font">Latest GitHub Repos</h1>
  <a href="https://github.com/jkitchin">@jkitchin</a> on GitHub.
  <ul id="my-github-projects">
        <li class="loading">Status updating&#8230;</li>
  </ul>

</section>
</aside>

          </div>
          <div class="clear"></div>
        </div>
      </div>
      
<footer>
  <div id="footer" class="grid_12">
    <div class="grid_8">
      <p>
        <a href="/blog/feed/index.xml">RSS</a>
      </p>
    </div>
    <div class="grid_4" id="credits">
      <p>
        Copyright 2023
        John Kitchin
      </p>
      <p>
        Powered by <a href="http://www.blogofile.com">Blogofile</a>
      </p>
    </div>
  </div>
</footer>

    </div>
      <script src="//ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/js/libs/jquery-1.5.1.min.js"%3E%3C/script%3E'))</script>
  <script src="/js/plugins.js"></script>
  <script src="/js/script.js"></script>
  <script src="/js/jquery.tweet.js"></script>  
  <script src="/js/site.js"></script>
  <!--[if lt IE 7 ]>
  <script src="js/libs/dd_belatedpng.js"></script>
  <script> DD_belatedPNG.fix('img, .png_bg');</script>
  <![endif]-->
  <script>
      var _gaq=[['_setAccount','UA-35731398-1'],['_trackPageview']];
      (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.async=1;
      g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
      s.parentNode.insertBefore(g,s)}(document,'script'));
  </script>

  </body>
</html>






<script src="http://ajax.microsoft.com/ajax/jquery/jquery-1.4.2.min.js" type="text/javascript"></script>
<script src="/js/git.js" type="text/javascript"></script>
<script type="text/javascript">
    $(function() {
     $("#my-github-projects").loadRepositories("jkitchin");
    });
</script>



