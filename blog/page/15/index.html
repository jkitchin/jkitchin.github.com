

<!doctype html>
<!--[if lt IE 7 ]> <html lang="en" class="no-js ie6"> <![endif]-->
<!--[if IE 7 ]>    <html lang="en" class="no-js ie7"> <![endif]-->
<!--[if IE 8 ]>    <html lang="en" class="no-js ie8"> <![endif]-->
<!--[if IE 9 ]>    <html lang="en" class="no-js ie9"> <![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html lang="en" class="no-js"> <!--<![endif]-->
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Kitchin Research Group</title>
  <meta name="google-site-verification" content="CGcacJdHc2YoZyI0Vey9XRA5qwhhFDzThKJezbRFcJ4" />
  <meta name="description" content="Chemical Engineering at Carnegie Mellon University">
  <meta name="author" content="John Kitchin">
  <link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
  <link rel="alternate" type="application/atom+xml" title="Atom 1.0" href="/blog/feed/atom" />
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">

  <link rel="stylesheet" href="/css/base.css?v=1">
  <link rel="stylesheet" href="/css/grid.css?v=1">
  <link rel="stylesheet" media="handheld" href="/css/handheld.css?v=1">
  <link rel="stylesheet" href="/css/pygments_murphy.css" type="text/css" />

  <script src="/js/libs/modernizr-1.7.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
  <link rel="stylesheet" href="/themes/theme1/style.css?v=1">
<link href='http://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>

</head>
  <body>
    <div id="container" class="container container_12">
      <div id="main" role="main">
        <div id="main_block">
          <header>
<div id="header" class="header_gradient theme_font">
<table><tr><td>
    <h1><a href="/">The Kitchin Research Group</a></h1>
    <h2>Chemical Engineering at Carnegie Mellon University</h2>
</td>
<td colspan=100%><div style="float:right;width:100%;text-align:right;"> <span id='badgeCont737515' style='width:126px'><script src='http://labs.researcherid.com/mashlets?el=badgeCont737515&mashlet=badge&showTitle=false&className=a&rid=A-2363-2010'></script></span></div>
</td></tr>
</table>
</div>
  <div id="navigation" class="grid_12">

    <ul class="theme_font">
      <li><a href="/blog"
             class="">Blog</a></li>

      <li><a href="/blog/archive"
             class="">Archives</a></li>

      <li><a href="/publications.html">Publications</a></li>

      <li><a href="/research.html"
             class="">Research</a></li>

      <li><a href="/categories.html"
             class="">Categories</a></li>

      <li><a href="/about.html"
             class="">About us</a></li>

      <li><a href="/subscribe.html">Subscribe</a></li>

    </ul>
  </div>
</header>

          <div id="prose_block" class="grid_8">
            
  





<article>
  <div class="blog_post">
    <header>
      <div id="New-publication-in-Molecular-Simulation"></div>
      <h2 class="blog_post_title"><a href="/blog/2018/01/03/New-publication-in-Molecular-Simulation/" rel="bookmark" title="Permanent Link to New publication in Molecular Simulation">New publication in Molecular Simulation</a></h2>
      <p><small><span class="blog_post_date">Posted January 03, 2018 at 06:28 AM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/publication/'>publication</a>, <a href='/blog/category/news/'>news</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
This paper is our latest work using neural networks in molecular simulation. In this work, we build a Behler-Parinello neural network potential of bulk zirconia. The potential can describe several polymorphs of zirconia, as well as oxygen vacancy defect formation energies and diffusion barriers. We show that we can use the potential to model oxygen vacancy diffusion using molecular dynamics at different temperatures, and to use that data to estimate the effective diffusion activation energy. This is further evidence of the general utility of the neural network-based potential for molecular simulations with DFT accuracy.
</p>

<div class="org-src-container">
<pre class="src src-bibtex"><span style="color: #006699;">@article</span>{<span style="color: #D0372D;">wang-2018-densit-funct</span>,
  <span style="color: #BA36A5;">author</span> =       {Chen Wang and Akshay Tharval and John R. Kitchin},
  <span style="color: #BA36A5;">title</span> =        {A Density Functional Theory Parameterised Neural Network Model
                  of Zirconia},
  <span style="color: #BA36A5;">journal</span> =      {Molecular Simulation},
  <span style="color: #BA36A5;">volume</span> =       0,
  <span style="color: #BA36A5;">number</span> =       0,
  <span style="color: #BA36A5;">pages</span> =        {1-8},
  <span style="color: #BA36A5;">year</span> =         2018,
  <span style="color: #BA36A5;">doi</span> =          {<span style="color: #006DAF; text-decoration: underline;">10.1080/08927022.2017.1420185</span>},
  <span style="color: #BA36A5;">url</span> =          {<span style="color: #006DAF; text-decoration: underline;">https://doi.org/10.1080/08927022.2017.1420185</span>},
  <span style="color: #BA36A5;">eprint</span> =       { https://doi.org/10.1080/08927022.2017.1420185 },
  <span style="color: #BA36A5;">publisher</span> =    {Taylor \&amp; Francis},
}
</pre>
</div>

<p>
<script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
<div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1080/08927022.2017.1420185'></div>
</p>
<p>Copyright (C) 2018 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2018/01/03/New-publication-in-Molecular-Simulation.org">org-mode source</a></p>
<p>Org-mode version = 9.1.5</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2018/01/03/New-publication-in-Molecular-Simulation">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="2017-in-a-nutshell-for-the-Kitchin-Research-group"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/12/31/2017-in-a-nutshell-for-the-Kitchin-Research-group/" rel="bookmark" title="Permanent Link to 2017 in a nutshell for the Kitchin Research group">2017 in a nutshell for the Kitchin Research group</a></h2>
      <p><small><span class="blog_post_date">Posted December 31, 2017 at 01:21 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/news/'>news</a></span> | tags: 
      <p><small><span class="blog_post_date">Updated December 31, 2017 at 01:21 PM</span></small>
      </small></p>
    </header>
    <div class="post_prose">
      



<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orga22af1f">1. Student accomplishments</a></li>
<li><a href="#org1a1a81e">2. Publications</a>
<ul>
<li><a href="#org372deac">2.1. Collaborative papers</a></li>
<li><a href="#org59c38bf">2.2. Papers on neural networks in molecular simulation</a></li>
<li><a href="#org44aee74">2.3. Papers accepted in 2017 but not yet in press</a></li>
</ul>
</li>
<li><a href="#org79f134f">3. New courses</a></li>
<li><a href="#org6bb6401">4. Sabbatical at Google</a></li>
<li><a href="#org29808b7">5. Emacs and org-mode</a></li>
<li><a href="#org16e2c7b">6. Social media</a>
<ul>
<li><a href="#orgd9b416e">6.1. kitchingroup.cheme.cmu.edu</a></li>
<li><a href="#orga302874">6.2. Github</a></li>
<li><a href="#org8c173b0">6.3. Youtube</a></li>
</ul>
</li>
</ul>
</div>
</div>
<p>
Since the <a href="http://kitchingroup.cheme.cmu.edu/blog/2017/01/01/2016-in-a-nutshell-for-the-Kitchin-Research-group/">last update</a> a lot of new things have happened in the Kitchin Research group. Below are some summaries of the group accomplishments, publications and activities for the past year. 
</p>

<div id="outline-container-orga22af1f" class="outline-2">
<h2 id="orga22af1f"><span class="section-number-2">1</span> Student accomplishments</h2>
<div class="outline-text-2" id="text-1">
<p>
Jacob Boes completed his PhD and began postdoctoral work with Thomas Bligaard at SLAC/Suncat at Stanford. Congratulations Jake!
</p>

<p>
Four new PhD students joined the group:
</p>
<ol class="org-ol">
<li>Jenny Zhan will work on simulation of molten superalloys</li>
<li>Mingjie Liu will work on the design of single atom alloy catalysts</li>
<li>Yilin Yang will work on segregation in multicomponent alloys under reaction conditions</li>
<li>Zhitao Guo is also joining the group and will be co-advised by Prof. Gellman. He will work on multicomponent alloy catalysts.</li>
</ol>

<p>
Welcome to the group!
</p>
</div>
</div>

<div id="outline-container-org1a1a81e" class="outline-2">
<h2 id="org1a1a81e"><span class="section-number-2">2</span> Publications</h2>
<div class="outline-text-2" id="text-2">
<p>
Our publications and citation counts have continued to grow this year. Here is our current metrics according to <a href="http://www.researcherid.com/rid/A-2363-2010">Researcher ID</a>.
</p>

<p>
<img src="/media/date-30-12-2017-time-20-18-45.png"> 
</p>

<p>
We have eight new papers that are online, and two that are accepted, but not online yet. There are brief descriptions below.
</p>
</div>

<div id="outline-container-org372deac" class="outline-3">
<h3 id="org372deac"><span class="section-number-3">2.1</span> Collaborative papers</h3>
<div class="outline-text-3" id="text-2-1">
<dl class="org-dl">
<dt><a class='org-ref-reference' href="#larsen-2017-atomic-simul">larsen-2017-atomic-simul</a></dt><dd>This is a modern update on the Atomic Simulation Environment Python software. We have been using and contributing to this software for about 15 years now!</dd>

<dt><a class='org-ref-reference' href="#saravanan-2017-alchem-predic">saravanan-2017-alchem-predic</a></dt><dd>This collaborative effort with the Keith group at UPitt and Anatole von Lilienfeld explored a novel approach to estimating adsorption energies on alloy surfaces.</dd>

<dt><a class='org-ref-reference' href="#xu-2017-first-princ">xu-2017-first-princ</a></dt><dd>We used DFT calculations to understand epitaxial stabilization of titania films on strontium titanate surfaces.</dd>

<dt><a class='org-ref-reference' href="#wittkamper-2017-compet-growt">wittkamper-2017-compet-growt</a></dt><dd>We previously predicted that tin oxide should be able to form in the columbite phase as an epitaxial film. In this paper our collaborators show that it can be done!</dd>

<dt><a class='org-ref-reference' href="#kitchin-2017-autom-data">kitchin-2017-autom-data</a></dt><dd>This paper finally came out in print. It shows an automated approach to sharing data. Also, it may be the only paper with data hidden inside a picture of a library in the literature.</dd>
</dl>
</div>
</div>

<div id="outline-container-org59c38bf" class="outline-3">
<h3 id="org59c38bf"><span class="section-number-3">2.2</span> Papers on neural networks in molecular simulation</h3>
<div class="outline-text-3" id="text-2-2">
<dl class="org-dl">
<dt><a class='org-ref-reference' href="#boes-2017-neural-networ">boes-2017-neural-networ</a></dt><dd>We used neural networks in conjunction with molecular dynamics and Monte Carlo simulations to model the coverage dependent adsorption of oxygen and initial oxidation of a Pd(111) surface.</dd>

<dt><a class='org-ref-reference' href="#boes-2017-model-segreg">boes-2017-model-segreg</a></dt><dd>We used neural networks in conjunction with Monte Carlo simulations to model segregation across composition space for a Au-Pd alloy.</dd>

<dt><a class='org-ref-reference' href="#geng-2017-first-princ">geng-2017-first-princ</a></dt><dd>We used a cluster expansion with Monte Carlo simulations to resolve some inconsistencies in simulated Cu-Pd phase diagrams. There is an interesting transition from an fcc to bcc to fcc structure across the composition space that is subtle and difficult to compute.</dd>
</dl>
</div>
</div>

<div id="outline-container-org44aee74" class="outline-3">
<h3 id="org44aee74"><span class="section-number-3">2.3</span> Papers accepted in 2017 but not yet in press</h3>
<div class="outline-text-3" id="text-2-3">
<ol class="org-ol">
<li>Chen Wang, Akshay Tharval, John R. Kitchin, A density functional theory parameterized neural network model of zirconia, Accepted in Molecular Simulation, July 2017.</li>

<li>Hari Thirumalai, John R. Kitchin, Investigating the Reactivity of Single Atom Alloys using Density Functional Theory, Topics in Catalysis, Accepted November 2017.</li>
</ol>
</div>
</div>
</div>


<div id="outline-container-org79f134f" class="outline-2">
<h2 id="org79f134f"><span class="section-number-2">3</span> New courses</h2>
<div class="outline-text-2" id="text-3">
<p>
After a five year stint of teaching Master's and PhD courses, I taught the undergraduate chemical engineering course again. This was the first time I taught the course using Python. All the lectures and assignments were in Jupyter notebooks. You can find the course here: <a href="https://github.com/jkitchin/s17-06364">https://github.com/jkitchin/s17-06364</a>. The whole class basically ran from a browser using a Python Flask app to serve the syllabus, lectures and assignments. Assignments were submitted and returned by email through the Flask app. It was pretty interesting. I did not like it as much as using Emacs/org-mode like I have in the past, but it was easier to get 70 undergraduates up and running.
</p>

<p>
I did not teach in the Fall, because I was on Sabbatical!
</p>
</div>
</div>

<div id="outline-container-org6bb6401" class="outline-2">
<h2 id="org6bb6401"><span class="section-number-2">4</span> Sabbatical at Google</h2>
<div class="outline-text-2" id="text-4">
<p>
In August 2017 I started my first sabbatical! I am spending a year in the <a href="https://research.google.com/teams/gas/">Accelerated Science</a> group at Google in Mountain View, California. I am learning about machine learning applications in engineering and science. This is a pivotal year in my research program, so stay tuned for our new work!
</p>

<p>
It has been great for my family, who moved out here with me. We have been seeing a lot of California. I have been biking to work almost every day, usually 15-20 miles. I have logged over 1200 commuting miles already since August.
</p>
</div>
</div>

<div id="outline-container-org29808b7" class="outline-2">
<h2 id="org29808b7"><span class="section-number-2">5</span> Emacs and org-mode</h2>
<div class="outline-text-2" id="text-5">
<p>
org-ref remains in the top 15% of downloaded <a href="https://melpa.org/#/org-ref">MELPA</a> packages, with more than 24,000 downloads since it was released. It has been pretty stable lately. It remains a cornerstone of my technical writing toolbox.
</p>

<p>
I have spent some time improving org-mode/ipython interactions including inline images, asynchronous execution and export to jupyter notebooks. It is still a work in progress.
</p>

<p>
I spent a fair bit of time learning about dynamic modules for writing compiled extensions to Emacs to bring features like linear algebra, numerical methods and database access to it. I wish I had more time to work on this. I think it will be useful to make org-mode even better for  scientific research and documentation. 
</p>
</div>
</div>

<div id="outline-container-org16e2c7b" class="outline-2">
<h2 id="org16e2c7b"><span class="section-number-2">6</span> Social media</h2>
<div class="outline-text-2" id="text-6">
<p>
I have continued exploring the use of social media to share my work. It still seems like a worthwhile use of time, but we need continued efforts to make this really useful for science. 
</p>
</div>

<div id="outline-container-orgd9b416e" class="outline-3">
<h3 id="orgd9b416e"><span class="section-number-3">6.1</span> kitchingroup.cheme.cmu.edu</h3>
<div class="outline-text-3" id="text-6-1">
<p>
I use my blog to share technical knowledge and news about the group. We had 48 blog posts in 2017. A lot of them were on some use of org-mode and Emacs.  I also introduced a new exporter for org-mode to make jupyter notebooks. I spent November exploring automatic differentiation and applications of it to engineering problems. Visits to the site continue to grow. Here is the growth over the past two years. The big spike in Oct 2017 is from this <a href="https://news.ycombinator.com/item?id=15464340">article on Hacker News</a> about one of my posts!
</p>


<p>
<img src="/media/date-30-12-2017-time-20-15-28.png"> 
</p>

<p>
I continue to think that technical blogging is a valuable way to communicate technical knowledge. It provides an easy way to practice writing, and with comments enabled to get feedback on your ideas. It has taken several years to develop a style for doing this effectively that is useful to me, and to others. I have integrated my blog into Twitter so that new posts are automatically tweeted, which helps publicize the new posts.
</p>

<p>
It has some limitations, e.g. it is not obvious how to cite them in ways that are compatible with the current bibliometric driven assessment tools used in promotion and tenure. Overall, I find it very complementary to formal publications though, and I wish more people did it.
</p>
</div>
</div>

<div id="outline-container-orga302874" class="outline-3">
<h3 id="orga302874"><span class="section-number-3">6.2</span> Github</h3>
<div class="outline-text-3" id="text-6-2">
<p>
I was a little less active on <a href="https://github.com/jkitchin">Github</a> this year than last year, especially this fall as I started my sabbatical. Github remains my goto version control service though, and we continue using it for everything from code development and paper writing to course serving.
</p>


<p>
<img src="/media/date-30-12-2017-time-20-12-03.png"> 
</p>

<p>
scimax finally has more Github stars than jmax does!
</p>
</div>
</div>

<div id="outline-container-org8c173b0" class="outline-3">
<h3 id="org8c173b0"><span class="section-number-3">6.3</span> Youtube</h3>
<div class="outline-text-3" id="text-6-3">
<p>
Another year with over 100,000 minutes of <a href="https://www.youtube.com/analytics?o=U#dt=ty,fe=17165,fr=lw-001,fs=16801;fc=0,fcr=0,r=views,rpg=93">Youtube watch time</a> on our videos. <a href="https://www.youtube.com/watch?v=fgizHHd7nOo">org-mode is awesome</a> was most popular, with almost 50,000 views. We have six videos with over 2500 views for the past year!
</p>



<p>
<img src="/media/date-31-12-2017-time-13-08-54.png"> 
</p>

<p>
I have not made too many new videos this year. Hopefully there will be some new ones on the new features in scimax in the next year.
</p>
</div>
</div>
</div>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/12/31/2017-in-a-nutshell-for-the-Kitchin-Research-group.org">org-mode source</a></p>
<p>Org-mode version = 9.1.3</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/12/31/2017-in-a-nutshell-for-the-Kitchin-Research-group">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="Solving-an-eigenvalue-differential-equation-with-a-neural-network"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/11/29/Solving-an-eigenvalue-differential-equation-with-a-neural-network/" rel="bookmark" title="Permanent Link to Solving an eigenvalue differential equation with a neural network">Solving an eigenvalue differential equation with a neural network</a></h2>
      <p><small><span class="blog_post_date">Posted November 29, 2017 at 09:17 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/autograd/'>autograd</a>, <a href='/blog/category/eigenvalue/'>eigenvalue</a>, <a href='/blog/category/bvp/'>bvp</a></span> | tags: 
      <p><small><span class="blog_post_date">Updated November 29, 2017 at 09:20 PM</span></small>
      </small></p>
    </header>
    <div class="post_prose">
      



<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgdd6adc2">1. The neural network setup</a></li>
<li><a href="#org08ec74a">2. The objective function</a></li>
<li><a href="#org865c7c4">3. The minimization</a></li>
<li><a href="#org31faf55">4. The first excited state</a></li>
<li><a href="#org3e74a47">5. Summary</a></li>
</ul>
</div>
</div>
<p>
The 1D harmonic oscillator is described <a href="https://quantummechanics.ucsd.edu/ph130a/130_notes/node153.html">here</a>. It is a boundary value differential equation with eigenvalues. If we let let &omega;=1, m=1, and units where &hbar;=1. then, the governing differential equation becomes:
</p>

<p>
\(-0.5 \frac{d^2\psi(x)}{dx^2} + (0.5 x^2 - E) \psi(x) = 0\)
</p>

<p>
with boundary conditions: \(\psi(-\infty) = \psi(\infty) = 0\)
</p>

<p>
We can further stipulate that the probability of finding the particle over this domain is equal to one: \(\int_{-\infty}^{\infty} \psi^2(x) dx = 1\). In this set of equations, \(E\) is an eigenvalue, which means there are only non-trivial solutions for certain values of \(E\).
</p>

<p>
Our goal is to solve this equation using a neural network to represent the wave function. This is a different problem than the one <a href="http://kitchingroup.cheme.cmu.edu/blog/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd/">here</a> or <a href="http://kitchingroup.cheme.cmu.edu/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd/">here</a> because of the eigenvalue. This is an additional adjustable parameter we have to find. Also, we have the normalization constraint to consider, which we did not consider before.
</p>

<div id="outline-container-orgdd6adc2" class="outline-2">
<h2 id="orgdd6adc2"><span class="section-number-2">1</span> The neural network setup</h2>
<div class="outline-text-2" id="text-1">
<p>
Here we setup the neural network and its derivatives. This is the same as we did before.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orge0a59c3"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> grad, elementwise_grad
<span style="color: #0000FF;">import</span> autograd.numpy.random <span style="color: #0000FF;">as</span> npr
<span style="color: #0000FF;">from</span> autograd.misc.optimizers <span style="color: #0000FF;">import</span> adam

<span style="color: #0000FF;">def</span> <span style="color: #006699;">init_random_params</span>(scale, layer_sizes, rs=npr.RandomState(42)):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"""Build a list of (weights, biases) tuples, one for each layer."""</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> [(rs.randn(insize, outsize) * scale,   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">weight matrix</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>rs.randn(outsize) * scale)           <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">bias vector</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> insize, outsize <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">zip</span>(layer_sizes[:-1], layer_sizes[1:])]

<span style="color: #0000FF;">def</span> <span style="color: #006699;">swish</span>(x):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"see https://arxiv.org/pdf/1710.05941.pdf"</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> x / (1.0 + np.exp(-x))

<span style="color: #0000FF;">def</span> <span style="color: #006699;">psi</span>(nnparams, inputs):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"Neural network wavefunction"</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> W, b <span style="color: #0000FF;">in</span> nnparams:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">outputs</span> = np.dot(inputs, W) + b
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">inputs</span> = swish(outputs)    
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> outputs

<span style="color: #BA36A5;">psip</span> = elementwise_grad(psi, 1) <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">dpsi/dx </span>
<span style="color: #BA36A5;">psipp</span> = elementwise_grad(psip, 1) <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">d^2psi/dx^2</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org08ec74a" class="outline-2">
<h2 id="org08ec74a"><span class="section-number-2">2</span> The objective function</h2>
<div class="outline-text-2" id="text-2">
<p>
The important function we need is the objective function. This function codes the Schr√∂dinger equation, the boundary conditions, and the normalization as a cost function that we will later seek to minimize. Ideally, at the solution the objective function will be zero. We can't put infinity into our objective function, but it turns out that x = &plusmn; 6 is practically infinity in this case, so we approximate the boundary conditions there. 
</p>

<p>
Another note is the numerical integration by the trapezoid rule. I use a vectorized version of this because autograd doesn't have a trapz derivative and I didn't feel like figuring one out.
</p>

<p>
We define the params to vary here as a dictionary containing neural network weights and biases, and the value of the eigenvalue.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orge9e096c"><span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Here is our initial guess of params:</span>
<span style="color: #BA36A5;">nnparams</span> = init_random_params(0.1, layer_sizes=[1, 8, 1])

<span style="color: #BA36A5;">params</span> = {<span style="color: #008000;">'nn'</span>: nnparams, <span style="color: #008000;">'E'</span>: 0.4}

<span style="color: #BA36A5;">x</span> = np.linspace(-6, 6, 200)[:, <span style="color: #D0372D;">None</span>]

<span style="color: #0000FF;">def</span> <span style="color: #006699;">objective</span>(params, step):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">nnparams</span> = params[<span style="color: #008000;">'nn'</span>]
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">E</span> = params[<span style="color: #008000;">'E'</span>]        
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">This is Schrodinger's eqn</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">zeq</span> = -0.5 * psipp(nnparams, x) + (0.5 * x**2 - E) * psi(nnparams, x) 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">bc0</span> = psi(nnparams, -6.0) <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">This approximates -infinity</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">bc1</span> = psi(nnparams, 6.0)  <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">This approximates +infinity</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">y2</span> = psi(nnparams, x)**2
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">This is a numerical trapezoid integration</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">prob</span> = np.<span style="color: #006FE0;">sum</span>((y2[1:] + y2[0:-1]) / 2 * (x[1:] - x[0:-1]))
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> np.mean(zeq**2) + bc0**2 + bc1**2 + (1.0 - prob)**2

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">This gives us feedback from the optimizer</span>
<span style="color: #0000FF;">def</span> <span style="color: #006699;">callback</span>(params, step, g):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> step % 1000 == 0:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">"Iteration {0:3d} objective {1}"</span>.<span style="color: #006FE0;">format</span>(step,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> objective(params, step)))
</pre>
</div>
</div>
</div>

<div id="outline-container-org865c7c4" class="outline-2">
<h2 id="org865c7c4"><span class="section-number-2">3</span> The minimization</h2>
<div class="outline-text-2" id="text-3">
<p>
Now, we just let an optimizer minimize the objective function for us. Note, I ran this next block more than once, as the objective continued to decrease. I ran this one at least two times, and the loss was still decreasing slowly.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgf241c39"><span style="color: #BA36A5;">params</span> = adam(grad(objective), params,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> step_size=0.001, num_iters=5001, callback=callback) 

<span style="color: #0000FF;">print</span>(params[<span style="color: #008000;">'E'</span>])
</pre>
</div>

<pre class="example">
Iteration   0 objective [[ 0.00330204]]
Iteration 1000 objective [[ 0.00246459]]
Iteration 2000 objective [[ 0.00169862]]
Iteration 3000 objective [[ 0.00131453]]
Iteration 4000 objective [[ 0.00113132]]
Iteration 5000 objective [[ 0.00104405]]
0.5029457355415167

</pre>

<p>
Good news, the lowest energy eigenvalue is known to be 0.5 for our choice of parameters, and that is approximately what we got. Now let's see our solution and compare it to the known solution. Interestingly we got the negative of the solution, which is still a solution. The NN solution is not indistinguishable from the analytical solution, and has some spurious curvature in the tails, but it is approximately correct, and more training might get it closer. A different activation function might also work better.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgc343304">%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

<span style="color: #BA36A5;">x</span> = np.linspace(-6, 6)[:, <span style="color: #D0372D;">None</span>]
<span style="color: #BA36A5;">y</span> = psi(params[<span style="color: #008000;">'nn'</span>], x)

plt.plot(x, -y, label=<span style="color: #008000;">'NN'</span>)
plt.plot(x, (1/np.pi)**0.25 * np.exp(-x**2 / 2), <span style="color: #008000;">'r--'</span>, label=<span style="color: #008000;">'analytical'</span>)
plt.legend()
</pre>
</div>

<p>
<img src="/media/ob-ipython-a0315846d401b5468d391df4b1ee6e84.png"> 
</p>
</div>
</div>

<div id="outline-container-org31faf55" class="outline-2">
<h2 id="org31faf55"><span class="section-number-2">4</span> The first excited state</h2>
<div class="outline-text-2" id="text-4">
<p>
Now, what about the first excited state? This has an eigenvalue of 1.5, and the solution has odd parity. We can naively change the eigenvalue, and hope that the optimizer will find the right new solution. We do that here, and use the old NN params.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org78762bc"><span style="color: #BA36A5;">params</span>[<span style="color: #008000;">'E'</span>] = 1.6
</pre>
</div>

<p>
Now, we run a round of optimization:
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org77ad283"><span style="color: #BA36A5;">params</span> = adam(grad(objective), params,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> step_size=0.003, num_iters=5001, callback=callback) 

<span style="color: #0000FF;">print</span>(params[<span style="color: #008000;">'E'</span>])
</pre>
</div>

<pre class="example">
Iteration   0 objective [[ 0.09918192]]
Iteration 1000 objective [[ 0.00102333]]
Iteration 2000 objective [[ 0.00100269]]
Iteration 3000 objective [[ 0.00098684]]
Iteration 4000 objective [[ 0.00097425]]
Iteration 5000 objective [[ 0.00096347]]
0.502326347406645

</pre>


<p>
That doesn't work though. The optimizer just pushes the solution back to the known one. Next, we try starting from scratch with the eigenvalue guess.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org41c431c"><span style="color: #BA36A5;">nnparams</span> = init_random_params(0.1, layer_sizes=[1, 8, 1])

<span style="color: #BA36A5;">params</span> = {<span style="color: #008000;">'nn'</span>: nnparams, <span style="color: #008000;">'E'</span>: 1.6}

<span style="color: #BA36A5;">params</span> = adam(grad(objective), params,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> step_size=0.003, num_iters=5001, callback=callback) 

<span style="color: #0000FF;">print</span>(params[<span style="color: #008000;">'E'</span>])
</pre>
</div>

<pre class="example">
Iteration   0 objective [[ 2.08318762]]
Iteration 1000 objective [[ 0.02358685]]
Iteration 2000 objective [[ 0.00726497]]
Iteration 3000 objective [[ 0.00336433]]
Iteration 4000 objective [[ 0.00229851]]
Iteration 5000 objective [[ 0.00190942]]
0.5066213334684926

</pre>

<p>
That also doesn't work. We are going to have to steer this. The idea is pre-train the neural network to have the basic shape and symmetry we want, and then use that as the input for the objective function. The first excited state has odd parity, and here is a guess of that shape. This is a pretty ugly hacked up version that only roughly has the right shape. I am counting on the NN smoothing out the discontinuities.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org586b931"><span style="color: #BA36A5;">xm</span> = np.linspace(-6, 6)[:, <span style="color: #D0372D;">None</span>]
<span style="color: #BA36A5;">ym</span> = -0.5 * ((-1 * (xm + 1.5)**2) + 1.5) * (xm &lt; 0) * (xm &gt; -3)
<span style="color: #BA36A5;">yp</span> = -0.5 * ((1 * (xm - 1.5)**2 ) - 1.5) * (xm &gt; 0) * (xm &lt; 3)

plt.plot(xm, (ym + yp))
plt.plot(x, (1/np.pi)**0.25 * np.sqrt(2) * x * np.exp(-x**2 / 2), <span style="color: #008000;">'r--'</span>, label=<span style="color: #008000;">'analytical'</span>)
</pre>
</div>

<p>
<img src="/media/ob-ipython-7306bb4c2a75d356dd2246681bec193e.png"> 
</p>

<p>
Now we pretrain a bit.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgea1c301"><span style="color: #0000FF;">def</span> <span style="color: #006699;">pretrain</span>(params, step):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">nnparams</span> = params[<span style="color: #008000;">'nn'</span>]
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">errs</span> = psi(nnparams, xm) - (ym + yp)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> np.mean(errs**2)

<span style="color: #BA36A5;">params</span> = adam(grad(pretrain), params,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> step_size=0.003, num_iters=501, callback=callback) 
</pre>
</div>

<pre class="example">
Iteration   0 objective [[ 1.09283695]]

</pre>

<p>
Here is the new initial guess we are going to use. You can see that indeed a lot of smoothing has occurred.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org9df043e">plt.plot(xm, ym + yp, xm, psi(params[<span style="color: #008000;">'nn'</span>], xm))
</pre>
</div>

<p>
<img src="/media/ob-ipython-861dc15ae81c1a9d2bcab2aeca1c7b64.png"> 
</p>

<p>
That has the right shape now. So we go back to the original objective function. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org5298900"><span style="color: #BA36A5;">params</span> = adam(grad(objective), params,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> step_size=0.001, num_iters=5001, callback=callback) 

<span style="color: #0000FF;">print</span>(params[<span style="color: #008000;">'E'</span>])
</pre>
</div>

<pre class="example">
Iteration   0 objective [[ 0.00370029]]
Iteration 1000 objective [[ 0.00358193]]
Iteration 2000 objective [[ 0.00345137]]
Iteration 3000 objective [[ 0.00333]]
Iteration 4000 objective [[ 0.0032198]]
Iteration 5000 objective [[ 0.00311844]]
1.5065724128094344

</pre>

<p>
I ran that optimization block many times. The loss is still decreasing, but slowly. More importantly, the eigenvalue is converging to 1.5, which is the known analytical value, and the solution is converging to the known solution. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org0d766e2"><span style="color: #BA36A5;">x</span> = np.linspace(-6, 6)[:, <span style="color: #D0372D;">None</span>]
<span style="color: #BA36A5;">y</span> = psi(params[<span style="color: #008000;">'nn'</span>], x)

plt.plot(x, y, label=<span style="color: #008000;">'NN'</span>)
plt.plot(x, (1/np.pi)**0.25 * np.sqrt(2) * x * np.exp(-x**2 / 2), <span style="color: #008000;">'r--'</span>, label=<span style="color: #008000;">'analytical'</span>)
plt.legend()
</pre>
</div>

<p>
<img src="/media/ob-ipython-e63e275d2112849010d3e28381ccf41b.png"> 
</p>

<p>
We can confirm the normalization is reasonable:
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org6eef549"><span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">check the normalization</span>
<span style="color: #0000FF;">print</span>(np.trapz(y.T * y.T, x.T))
</pre>
</div>

<pre class="example">
[ 0.99781886]

</pre>
</div>
</div>

<div id="outline-container-org3e74a47" class="outline-2">
<h2 id="org3e74a47"><span class="section-number-2">5</span> Summary</h2>
<div class="outline-text-2" id="text-5">
<p>
This is another example of using autograd to solve an eigenvalue differential equation. Some of these solutions required tens of thousands of iterations of training. The groundstate wavefunction was very easy to get. The first excited state, on the other hand, took some active steering. This is very much like how an initial guess can change which solution a nonlinear optimization (which this is) finds.
</p>

<p>
There are other ways to solve this particular problem. What I think is interesting about this is the possibility to solve harder problems, e.g. not just a harmonic potential, but a more complex one. You could pretrain a network on the harmonic solution, and then use it as the initial guess for the harder problem (which has no analytical solution). 
</p>
</div>
</div>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/29/Solving-an-eigenvalue-differential-equation-with-a-neural-network.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/11/29/Solving-an-eigenvalue-differential-equation-with-a-neural-network">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="Solving-ODEs-with-a-neural-network-and-autograd"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd/" rel="bookmark" title="Permanent Link to Solving ODEs with a neural network and autograd">Solving ODEs with a neural network and autograd</a></h2>
      <p><small><span class="blog_post_date">Posted November 28, 2017 at 07:23 AM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/autograd/'>autograd</a>, <a href='/blog/category/ode/'>ode</a></span> | tags: 
      <p><small><span class="blog_post_date">Updated November 28, 2017 at 07:23 AM</span></small>
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
In the last <a href="http://kitchingroup.cheme.cmu.edu/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd/">post</a> I explored using a neural network to solve a BVP. Here, I expand the idea to solving an initial value ordinary differential equation. The idea is basically the same, we just have a slightly different objective function.
</p>

<p>
\(dCa/dt = -k Ca(t)\) where \(Ca(t=0) = 2.0\).
</p>

<p>
Here is the code that solves this equation, along with a comparison to the analytical solution: \(Ca(t) = Ca0 \exp -kt\).
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> grad, elementwise_grad
<span style="color: #0000FF;">import</span> autograd.numpy.random <span style="color: #0000FF;">as</span> npr
<span style="color: #0000FF;">from</span> autograd.misc.optimizers <span style="color: #0000FF;">import</span> adam

<span style="color: #0000FF;">def</span> <span style="color: #006699;">init_random_params</span>(scale, layer_sizes, rs=npr.RandomState(0)):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"""Build a list of (weights, biases) tuples, one for each layer."""</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> [(rs.randn(insize, outsize) * scale,   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">weight matrix</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>rs.randn(outsize) * scale)           <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">bias vector</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> insize, outsize <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">zip</span>(layer_sizes[:-1], layer_sizes[1:])]


<span style="color: #0000FF;">def</span> <span style="color: #006699;">swish</span>(x):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"see https://arxiv.org/pdf/1710.05941.pdf"</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> x / (1.0 + np.exp(-x))


<span style="color: #0000FF;">def</span> <span style="color: #006699;">Ca</span>(params, inputs):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"Neural network functions"</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> W, b <span style="color: #0000FF;">in</span> params:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">outputs</span> = np.dot(inputs, W) + b
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">inputs</span> = swish(outputs)    
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> outputs

<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   
<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Here is our initial guess of params:</span>
<span style="color: #BA36A5;">params</span> = init_random_params(0.1, layer_sizes=[1, 8, 1])

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Derivatives</span>
<span style="color: #BA36A5;">dCadt</span> = elementwise_grad(Ca, 1)

<span style="color: #BA36A5;">k</span> = 0.23
<span style="color: #BA36A5;">Ca0</span> = 2.0
<span style="color: #BA36A5;">t</span> = np.linspace(0, 10).reshape((-1, 1))

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">This is the function we seek to minimize</span>
<span style="color: #0000FF;">def</span> <span style="color: #006699;">objective</span>(params, step):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">These should all be zero at the solution</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">dCadt = -k * Ca(t)</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">zeq</span> = dCadt(params, t) - (-k * Ca(params, t))
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">ic</span> = Ca(params, 0) - Ca0
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> np.mean(zeq**2) + ic**2

<span style="color: #0000FF;">def</span> <span style="color: #006699;">callback</span>(params, step, g):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> step % 1000 == 0:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">"Iteration {0:3d} objective {1}"</span>.<span style="color: #006FE0;">format</span>(step,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> objective(params, step)))

<span style="color: #BA36A5;">params</span> = adam(grad(objective), params,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> step_size=0.001, num_iters=5001, callback=callback) 


<span style="color: #BA36A5;">tfit</span> = np.linspace(0, 20).reshape(-1, 1)
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt
plt.plot(tfit, Ca(params, tfit), label=<span style="color: #008000;">'soln'</span>)
plt.plot(tfit, Ca0 * np.exp(-k * tfit), <span style="color: #008000;">'r--'</span>, label=<span style="color: #008000;">'analytical soln'</span>)
plt.legend()
plt.xlabel(<span style="color: #008000;">'time'</span>)
plt.ylabel(<span style="color: #008000;">'$C_A$'</span>)
plt.xlim([0, 20])
plt.savefig(<span style="color: #008000;">'nn-ode.png'</span>)
</pre>
</div>

<pre class="example">
Iteration   0 objective [[ 3.20374053]]
Iteration 1000 objective [[  3.13906829e-05]]
Iteration 2000 objective [[  1.95894699e-05]]
Iteration 3000 objective [[  1.60381564e-05]]
Iteration 4000 objective [[  1.39930673e-05]]
Iteration 5000 objective [[  1.03554970e-05]]

</pre>


<p>
<img src="/media/nn-ode.png"> 
</p>

<p>
Huh. Those two solutions are nearly indistinguishable. Since we used a neural network, let's hype it up and say we learned the solution to a differential equation! But seriously, note that although we got an "analytical" solution, we should only rely on it in the region we trained the solution on. You can see the solution above is not that good past t=10, even perhaps going negative (which is not even physically correct). That is a reminder that the function we have for the solution <i>is not the same as the analytical solution</i>, it just approximates it really well over the region we solved over. Of course, you can expand that region to the region you care about, but the main point is don't rely on the solution outside where you know it is good.
</p>

<p>
This idea isn't new. There are several papers in the literature on using neural networks to solve differential equations, e.g. <a href="http://www.sciencedirect.com/science/article/pii/S0255270102002076">http://www.sciencedirect.com/science/article/pii/S0255270102002076</a> and <a href="https://arxiv.org/pdf/physics/9705023.pdf">https://arxiv.org/pdf/physics/9705023.pdf</a>, and other blog posts that are similar (<a href="https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c">https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c</a>, even using autograd). That means to me that there is some merit to continuing to investigate this approach to solving differential equations.
</p>

<p>
There are some interesting challenges for engineers to consider with this approach though. When is the solution accurate enough? How reliable are derivatives of the solution? What network architecture is appropriate or best? How do you know how good the solution is? Is it possible to build in solution features, e.g. asymptotes, or constraints on derivatives, or that the solution should be monotonic, etc. These would help us trust the solutions not to do weird things, and to extrapolate more reliably.
</p>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd">Discuss on Twitter</a>

  <hr class="interblog" />
  





<article>
  <div class="blog_post">
    <header>
      <div id="Solving-BVPs-with-a-neural-network-and-autograd"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd/" rel="bookmark" title="Permanent Link to Solving BVPs with a neural network and autograd">Solving BVPs with a neural network and autograd</a></h2>
      <p><small><span class="blog_post_date">Posted November 27, 2017 at 07:59 PM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/autograd/'>autograd</a>, <a href='/blog/category/bvp/'>bvp</a></span> | tags: 
      <p><small><span class="blog_post_date">Updated November 27, 2017 at 08:00 PM</span></small>
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
In this <a href="http://kitchingroup.cheme.cmu.edu/blog/2013/03/11/Solving-the-Blasius-equation/">post</a> we solved a boundary value problem by discretizing it, and approximating the derivatives by finite differences. Here I explore using a neural network to approximate the unknown function, autograd to get the required derivatives, and using autograd to train the neural network to satisfy the differential equations. We will look at the Blasius equation again.
</p>

\begin{eqnarray}
f''' + \frac{1}{2} f f'' &=& 0 \\
f(0) &=& 0 \\
f'(0) &=& 0 \\
f'(\infty) &=& 1
\end{eqnarray}

<p>
Here I setup a simple neural network
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> grad, elementwise_grad
<span style="color: #0000FF;">import</span> autograd.numpy.random <span style="color: #0000FF;">as</span> npr
<span style="color: #0000FF;">from</span> autograd.misc.optimizers <span style="color: #0000FF;">import</span> adam

<span style="color: #0000FF;">def</span> <span style="color: #006699;">init_random_params</span>(scale, layer_sizes, rs=npr.RandomState(0)):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"""Build a list of (weights, biases) tuples, one for each layer."""</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> [(rs.randn(insize, outsize) * scale,   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">weight matrix</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>rs.randn(outsize) * scale)           <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">bias vector</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> insize, outsize <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">zip</span>(layer_sizes[:-1], layer_sizes[1:])]


<span style="color: #0000FF;">def</span> <span style="color: #006699;">swish</span>(x):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"see https://arxiv.org/pdf/1710.05941.pdf"</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> x / (1.0 + np.exp(-x))


<span style="color: #0000FF;">def</span> <span style="color: #006699;">f</span>(params, inputs):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"Neural network functions"</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">for</span> W, b <span style="color: #0000FF;">in</span> params:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">outputs</span> = np.dot(inputs, W) + b
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">inputs</span> = swish(outputs)    
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> outputs

<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   
<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Here is our initial guess of params:</span>
<span style="color: #BA36A5;">params</span> = init_random_params(0.1, layer_sizes=[1, 8, 1])

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Derivatives</span>
<span style="color: #BA36A5;">fp</span> = elementwise_grad(f, 1)
<span style="color: #BA36A5;">fpp</span> = elementwise_grad(fp, 1)
<span style="color: #BA36A5;">fppp</span> = elementwise_grad(fpp, 1)

<span style="color: #BA36A5;">eta</span> = np.linspace(0, 6).reshape((-1, 1))

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">This is the function we seek to minimize</span>
<span style="color: #0000FF;">def</span> <span style="color: #006699;">objective</span>(params, step):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">These should all be zero at the solution</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">f''' + 0.5 f'' f = 0</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">zeq</span> = fppp(params, eta) + 0.5 * f(params, eta) * fpp(params, eta) 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">bc0</span> = f(params, 0.0)  <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">equal to zero at solution</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">bc1</span> = fp(params, 0.0)  <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">equal to zero at solution</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">bc2</span> = fp(params, 6.0) - 1.0 <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">this is the one at "infinity"</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> np.mean(zeq**2) + bc0**2 + bc1**2 + bc2**2

<span style="color: #0000FF;">def</span> <span style="color: #006699;">callback</span>(params, step, g):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> step % 1000 == 0:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">"Iteration {0:3d} objective {1}"</span>.<span style="color: #006FE0;">format</span>(step,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> objective(params, step)))

<span style="color: #BA36A5;">params</span> = adam(grad(objective), params,
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span> step_size=0.001, num_iters=10000, callback=callback) 

<span style="color: #0000FF;">print</span>(<span style="color: #008000;">'f(0) = {}'</span>.<span style="color: #006FE0;">format</span>(f(params, 0.0)))
<span style="color: #0000FF;">print</span>(<span style="color: #008000;">'fp(0) = {}'</span>.<span style="color: #006FE0;">format</span>(fp(params, 0.0)))
<span style="color: #0000FF;">print</span>(<span style="color: #008000;">'fp(6) = {}'</span>.<span style="color: #006FE0;">format</span>(fp(params, 6.0)))
<span style="color: #0000FF;">print</span>(<span style="color: #008000;">'fpp(0) = {}'</span>.<span style="color: #006FE0;">format</span>(fpp(params, 0.0)))

<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt
plt.plot(eta, f(params, eta))
plt.xlabel(<span style="color: #008000;">'$\eta$'</span>)
plt.ylabel(<span style="color: #008000;">'$f(\eta)$'</span>)
plt.xlim([0, 6])
plt.ylim([0, 4.5])
plt.savefig(<span style="color: #008000;">'nn-blasius.png'</span>)
</pre>
</div>

<p>
Iteration   0 objective 1.11472535
Iteration 1000 objective 0.00049768
Iteration 2000 objective 0.0004579
Iteration 3000 objective 0.00041697
Iteration 4000 objective 0.00037408
Iteration 5000 objective 0.00033705
Iteration 6000 objective 0.00031016
Iteration 7000 objective 0.00029197
Iteration 8000 objective 0.00027585
Iteration 9000 objective 0.00024616
f(0) = -0.00014613
fp(0) = 0.0003518041251639459
fp(6) = 0.999518061473252
fpp(0) = 0.3263370503702663
</p>

<p>
<img src="/media/nn-blasius.png"> 
I think it is worth discussing what we accomplished here. You can see we have arrived at an approximate solution to our differential equation and the boundary conditions. The boundary conditions seem pretty closely met, and the figure is approximately the same as the previous post. Even better, our solution is <i>an actual function</i> and not a numeric solution that has to be interpolated. We can evaluate it any where we want, including its derivatives!
</p>

<p>
We did not, however, have to convert the ODE into a system of first-order differential equations, and we did <i>not</i> have to approximate the derivatives with finite differences.
</p>

<p>
Note, however, that with finite differences we got <code>f''(0)=0.3325</code>. This <a href="https://www.calpoly.edu/~kshollen/ME347/Handouts/Blasius.pdf">site</a> reports <code>f''(0)=0.3321</code>. We get close to that here with <code>f''(0) = 0.3263</code>. We could probably get closer to this with more training to reduce the objective function further, or with a finer grid. It is evident that even after 9000 steps, it is still decreasing. Getting accurate derivatives is a stringent test for this, as they are measures of the function curvature.
</p>

<p>
It is hard to tell how broadly useful this is; I have not tried it beyond this example. In the past, I have found solving BVPs to be pretty sensitive to the initial guesses of the solution. Here we made almost no guess at all, and still got a solution. I find that pretty remarkable.
</p>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd">Discuss on Twitter</a>

  <hr class="interblog" />
 <a href="../14">¬´ Previous Page</a>
  --  
 <a href="../16">Next Page ¬ª</a>

          </div>
          <div id="sidebar" class="grid_4">
            <aside>
<section>
<script>
  (function() {
    var cx = '002533177287215655227:l7uvu35ssbc';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</section>

<section>
    <h1 class="post_header_gradient theme_font">Twitter</h1>
    <a class="twitter-timeline" href="https://twitter.com/johnkitchin" data-widget-id="545217643582881792">Tweets by @johnkitchin</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</section>


  <section>
    <h1 class="post_header_gradient theme_font">Links</h1>
    <ul>
      <li><a href="https://www.continuum.io">Anaconda Python</a></li>
      <li><a href="/pycse">Pycse</a></li>
      <li><a href="/dft-book">DFT-book</a></li>
    </ul>
  </section>

  <section>
    <h1 class="post_header_gradient theme_font">Latest Posts</h1>
    <ul>
      <li><a href="/blog/2025/01/28/New-publication-Investigating-the-Error-Imbalance-of-Large-Scale-Machine-Learning-Potentials-in-Catalysis/">New publication - Investigating the Error Imbalance of Large-Scale Machine Learning Potentials in Catalysis</a></li>
      <li><a href="/blog/2025/01/27/New-publication-Structure-Sensitive-Reaction-Kinetics-of-Chiral-Molecules-on-Intrinsically-Chiral-Surfaces/">New publication - Structure Sensitive Reaction Kinetics of Chiral Molecules on Intrinsically Chiral Surfaces</a></li>
      <li><a href="/blog/2024/07/04/10-years-ago-org-mode-is-awesome-video/">10 years ago - org mode is awesome video</a></li>
      <li><a href="/blog/2024/06/08/New-publication-Pourbaix-Machine-Learning-Framework-Identifies-Acidic-Water-Oxidation-Catalysts-Exhibiting-Suppressed-Ruthenium-Dissolution/">New publication - Pourbaix Machine Learning Framework Identifies Acidic Water Oxidation Catalysts Exhibiting Suppressed Ruthenium Dissolution</a></li>
      <li><a href="/blog/2024/06/06/New-publication-Surface-Segregation-Studies-in-Ternary-Noble-Metal-Alloys-Comparing-DFT-and-Machine-Learning-with-Experimental-Data/">New publication - Surface Segregation Studies in Ternary Noble Metal Alloys Comparing DFT and Machine Learning with Experimental Data</a></li>
    </ul>
  </section>

<section>
<h1 class="post_header_gradient theme_font">Latest GitHub Repos</h1>
  <a href="https://github.com/jkitchin">@jkitchin</a> on GitHub.
  <ul id="my-github-projects">
        <li class="loading">Status updating&#8230;</li>
  </ul>

</section>
</aside>

          </div>
          <div class="clear"></div>
        </div>
      </div>
      
<footer>
  <div id="footer" class="grid_12">
    <div class="grid_8">
      <p>
        <a href="/blog/feed/index.xml">RSS</a>
      </p>
    </div>
    <div class="grid_4" id="credits">
      <p>
        Copyright 2025
        John Kitchin
      </p>
      <p>
        Powered by <a href="http://www.blogofile.com">Blogofile</a>
      </p>
    </div>
  </div>
</footer>

    </div>
      <script src="//ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/js/libs/jquery-1.5.1.min.js"%3E%3C/script%3E'))</script>
  <script src="/js/plugins.js"></script>
  <script src="/js/script.js"></script>
  <script src="/js/jquery.tweet.js"></script>  
  <script src="/js/site.js"></script>
  <!--[if lt IE 7 ]>
  <script src="js/libs/dd_belatedpng.js"></script>
  <script> DD_belatedPNG.fix('img, .png_bg');</script>
  <![endif]-->
 
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PH8NF4F0RE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PH8NF4F0RE');
</script>


  </body>
</html>






<script src="http://ajax.microsoft.com/ajax/jquery/jquery-1.4.2.min.js" type="text/javascript"></script>
<script src="/js/git.js" type="text/javascript"></script>
<script type="text/javascript">
    $(function() {
     $("#my-github-projects").loadRepositories("jkitchin");
    });
</script>



