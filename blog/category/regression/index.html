

<!doctype html>
<!--[if lt IE 7 ]> <html lang="en" class="no-js ie6"> <![endif]-->
<!--[if IE 7 ]>    <html lang="en" class="no-js ie7"> <![endif]-->
<!--[if IE 8 ]>    <html lang="en" class="no-js ie8"> <![endif]-->
<!--[if IE 9 ]>    <html lang="en" class="no-js ie9"> <![endif]-->
<!--[if (gt IE 9)|!(IE)]><!--> <html lang="en" class="no-js"> <!--<![endif]-->
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Kitchin Research Group: regression</title>
  <meta name="google-site-verification" content="CGcacJdHc2YoZyI0Vey9XRA5qwhhFDzThKJezbRFcJ4" />
  <meta name="description" content="Chemical Engineering at Carnegie Mellon University">
  <meta name="author" content="John Kitchin">
  <link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
  <link rel="alternate" type="application/atom+xml" title="Atom 1.0" href="/blog/feed/atom" />
  <link rel="shortcut icon" href="/favicon.ico">
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png">

  <link rel="stylesheet" href="/css/base.css?v=1">
  <link rel="stylesheet" href="/css/grid.css?v=1">
  <link rel="stylesheet" media="handheld" href="/css/handheld.css?v=1">
  <link rel="stylesheet" href="/css/pygments_murphy.css" type="text/css" />

  <script src="/js/libs/modernizr-1.7.min.js"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
  <link rel="stylesheet" href="/themes/theme1/style.css?v=1">
<link href='http://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>

</head>
  <body>
    <div id="container" class="container container_12">
      <div id="main" role="main">
        <div id="main_block">
          <header>
<div id="header" class="header_gradient theme_font">
<table><tr><td>
    <h1><a href="/">The Kitchin Research Group</a></h1>
    <h2>Chemical Engineering at Carnegie Mellon University</h2>
</td>
<td colspan=100%><div style="float:right;width:100%;text-align:right;"> <span id='badgeCont737515' style='width:126px'><script src='http://labs.researcherid.com/mashlets?el=badgeCont737515&mashlet=badge&showTitle=false&className=a&rid=A-2363-2010'></script></span></div>
</td></tr>
</table>
</div>
  <div id="navigation" class="grid_12">

    <ul class="theme_font">
      <li><a href="/blog"
             class="">Blog</a></li>

      <li><a href="/blog/archive"
             class="">Archives</a></li>

      <li><a href="/publications.html">Publications</a></li>

      <li><a href="/research.html"
             class="">Research</a></li>

      <li><a href="/categories.html"
             class="">Categories</a></li>

      <li><a href="/about.html"
             class="">About us</a></li>

      <li><a href="/subscribe.html">Subscribe</a></li>

    </ul>
  </div>
</header>

          <div id="prose_block" class="grid_8">
            
  





<article>
  <div class="blog_post">
    <header>
      <div id="Using-autograd-in-nonlinear-regression"></div>
      <h2 class="blog_post_title"><a href="/blog/2017/11/17/Using-autograd-in-nonlinear-regression/" rel="bookmark" title="Permanent Link to Using autograd in nonlinear regression">Using autograd in nonlinear regression</a></h2>
      <p><small><span class="blog_post_date">Posted November 17, 2017 at 07:49 AM</span> | categories:
        <span class="blog_post_categories"><a href='/blog/category/regression/'>regression</a>, <a href='/blog/category/autograd/'>autograd</a>, <a href='/blog/category/python/'>python</a></span> | tags: 
      </small></p>
    </header>
    <div class="post_prose">
      



<p>
Table <a href="#raw-data">raw-data</a> contains the energy as a function of volume for some solid material from a set of density functional theory calculations. Our goal is to fit the Murnaghan equation of state to this data. The model is moderately nonlinear. I have previously done this with the standard nonlinear regression functions in scipy, so today we will use autograd along with a builtin optimizer to minimize an objective function to achieve the same thing. 
</p>

<p>
The basic idea is we define an objective function, in this case the summed squared errors between predicted values from the model and known values from our data. The objective function takes two arguments: the model parameters, and the "step". This function signature is a consequence of the built in optimizer we use; it expects that signature (it is useful for batch training, but we will not use that here).  We use autograd to create a gradient of the objective function which the adam optimizer will use to vary the parameters with the goal of minimizing the objective function.
</p>

<p>
The adam optimizer function takes as one argument a callback function, which we call <code>summary</code> to print out intermediate results during the convergence. We run the optimizer in a loop because the optimizer runs a fixed number of steps on each call. We check if the objective function is sufficiently small, and if it is we break out. 
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org7bbe046"><span style="color: #0000FF;">import</span> autograd.numpy <span style="color: #0000FF;">as</span> np
<span style="color: #0000FF;">from</span> autograd <span style="color: #0000FF;">import</span> grad
<span style="color: #0000FF;">from</span> autograd.misc.optimizers <span style="color: #0000FF;">import</span> adam

np.set_printoptions(precision=3, suppress=<span style="color: #D0372D;">True</span>)

<span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">input data</span>
<span style="color: #BA36A5;">Vinput</span> = np.array([row[0] <span style="color: #0000FF;">for</span> row <span style="color: #0000FF;">in</span> data]) 
<span style="color: #BA36A5;">Eknown</span> = np.array([row[1] <span style="color: #0000FF;">for</span> row <span style="color: #0000FF;">in</span> data])

<span style="color: #0000FF;">def</span> <span style="color: #006699;">Murnaghan</span>(pars, vol):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">'''</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span><span style="color: #036A07;">   given a vector of parameters and volumes, return a vector of energies.</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span><span style="color: #036A07;">   equation From PRB 28,5480 (1983)</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span><span style="color: #036A07;">   '''</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">E0</span>, <span style="color: #BA36A5;">B0</span>, <span style="color: #BA36A5;">BP</span>, <span style="color: #BA36A5;">V0</span> = pars
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">E</span> = E0 + B0 * vol / BP * (((V0 / vol)**BP) / (BP - 1.0) + 1.0) - V0 * B0 / (BP - 1.)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> E

<span style="color: #0000FF;">def</span> <span style="color: #006699;">objective</span>(pars, step):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #036A07;">"This is what we want to minimize by varying the pars."</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">predicted</span> = Murnaghan(pars, Vinput)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Note Eknown is not defined in this function scope</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">errors</span> = Eknown - predicted
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">return</span> np.<span style="color: #006FE0;">sum</span>(errors**2)

<span style="color: #BA36A5;">objective_grad</span> = grad(objective)

<span style="color: #0000FF;">def</span> <span style="color: #006699;">summary</span>(pars, step, gradient):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">Note i, N are not defined in this function scope</span>
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> step % N == 0: 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">'step {0:5d}: {1:1.3e}'</span>.<span style="color: #006FE0;">format</span>(i * N + step, 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>objective(pars, step)))

<span style="color: #BA36A5;">pars</span> = np.array([-400, 0.5, 2, 210]) <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">The initial guess</span>
<span style="color: #BA36A5;">N</span> = 200 <span style="color: #8D8D84;"># </span><span style="color: #8D8D84; font-style: italic;">num of steps to take on each optimization</span>
<span style="color: #BA36A5;">learning_rate</span> = 0.001
<span style="color: #0000FF;">for</span> i <span style="color: #0000FF;">in</span> <span style="color: #006FE0;">range</span>(100):
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">pars</span> = adam(objective_grad, pars, step_size=learning_rate, 
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   num_iters=N, callback=summary)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #BA36A5;">SSE</span> = objective(pars, <span style="color: #D0372D;">None</span>)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">if</span> SSE &lt; 0.00002:
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">print</span>(<span style="color: #008000;">'Tolerance met.'</span>, SSE)
<span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #9B9B9B; background-color: #EDEDED;"> </span>   <span style="color: #0000FF;">break</span>
<span style="color: #0000FF;">print</span>(pars)
</pre>
</div>

<pre class="example">
step     0: 3.127e+02
step   200: 1.138e+02
step   400: 2.011e+01
step   600: 1.384e+00
step   800: 1.753e-01
step  1000: 2.044e-03
step  1200: 1.640e-03
step  1400: 1.311e-03
step  1600: 1.024e-03
step  1800: 7.765e-04
step  2000: 5.698e-04
step  2200: 4.025e-04
step  2400: 2.724e-04
step  2600: 1.762e-04
step  2800: 1.095e-04
step  3000: 6.656e-05
step  3200: 3.871e-05
step  3400: 2.359e-05
('Tolerance met.', 1.5768901008364176e-05)
[-400.029    0.004    4.032  211.847]

</pre>

<p>
There are some subtleties in the code above. One is the variables that are used kind of all over the place, which is noted in a few places. Those could get tricky to keep track of. Another is the variable I called learning_rate. I borrowed that terminology from the machine learning community. It is the <code>step_size</code> in this implementation of the optimizer. If you make it too large, the objective function doesn't converge, but if you set it too small, it will take a long time to converge. Note that it took at about 3400 steps of "training". This is a lot more than is typically required by something like <code>pycse.nlinfit</code>. This isn't the typical application for this approach to regression. More on that another day.
</p>

<p>
As with any fit, it is wise to check it out at least graphically. Here is the fit and data.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org0d237fb">%matplotlib inline
<span style="color: #0000FF;">import</span> matplotlib
matplotlib.rc(<span style="color: #008000;">'axes.formatter'</span>, useoffset=<span style="color: #D0372D;">False</span>)
<span style="color: #0000FF;">import</span> matplotlib.pyplot <span style="color: #0000FF;">as</span> plt

plt.plot(Vinput, Eknown, <span style="color: #008000;">'ko'</span>, label=<span style="color: #008000;">'known'</span>)

<span style="color: #BA36A5;">vinterp</span> = np.linspace(Vinput.<span style="color: #006FE0;">min</span>(), Vinput.<span style="color: #006FE0;">max</span>(), 200)

plt.plot(vinterp, Murnaghan(pars, vinterp), <span style="color: #008000;">'r-'</span>, label=<span style="color: #008000;">'predicted'</span>)
plt.xlabel(<span style="color: #008000;">'Vol'</span>)
plt.ylabel(<span style="color: #008000;">'E'</span>)
</pre>
</div>

<p>
<img src="/media/ob-ipython-f106274e2be904c3f20c4c20ec425ebd.png"> 
</p>

<p>
The fit looks pretty good.
</p>


<table id="org71d3fa4" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Volume-Energy data for a solid state system.</caption>

<colgroup>
<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">volume</th>
<th scope="col" class="org-right">energy</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">324.85990899</td>
<td class="org-right">-399.9731688470</td>
</tr>

<tr>
<td class="org-right">253.43999457</td>
<td class="org-right">-400.0172393178</td>
</tr>

<tr>
<td class="org-right">234.03826687</td>
<td class="org-right">-400.0256270548</td>
</tr>

<tr>
<td class="org-right">231.12159387</td>
<td class="org-right">-400.0265690700</td>
</tr>

<tr>
<td class="org-right">228.40609504</td>
<td class="org-right">-400.0273551120</td>
</tr>

<tr>
<td class="org-right">225.86490337</td>
<td class="org-right">-400.0280030862</td>
</tr>

<tr>
<td class="org-right">223.47556626</td>
<td class="org-right">-400.0285313450</td>
</tr>

<tr>
<td class="org-right">221.21992353</td>
<td class="org-right">-400.0289534593</td>
</tr>

<tr>
<td class="org-right">219.08319566</td>
<td class="org-right">-400.0292800709</td>
</tr>

<tr>
<td class="org-right">217.05369547</td>
<td class="org-right">-400.0295224970</td>
</tr>

<tr>
<td class="org-right">215.12089909</td>
<td class="org-right">-400.0296863867</td>
</tr>

<tr>
<td class="org-right">213.27525144</td>
<td class="org-right">-400.0297809256</td>
</tr>

<tr>
<td class="org-right">211.51060823</td>
<td class="org-right">-400.0298110000</td>
</tr>

<tr>
<td class="org-right">203.66743321</td>
<td class="org-right">-400.0291665573</td>
</tr>

<tr>
<td class="org-right">197.07888649</td>
<td class="org-right">-400.0275017142</td>
</tr>

<tr>
<td class="org-right">191.39717952</td>
<td class="org-right">-400.0250998136</td>
</tr>

<tr>
<td class="org-right">186.40163591</td>
<td class="org-right">-400.0221371852</td>
</tr>

<tr>
<td class="org-right">181.94435510</td>
<td class="org-right">-400.0187369863</td>
</tr>

<tr>
<td class="org-right">177.92077043</td>
<td class="org-right">-400.0149820198</td>
</tr>

<tr>
<td class="org-right">174.25380090</td>
<td class="org-right">-400.0109367042</td>
</tr>

<tr>
<td class="org-right">170.88582166</td>
<td class="org-right">-400.0066495100</td>
</tr>

<tr>
<td class="org-right">167.76711189</td>
<td class="org-right">-400.0021478258</td>
</tr>

<tr>
<td class="org-right">164.87096104</td>
<td class="org-right">-399.9974753449</td>
</tr>

<tr>
<td class="org-right">159.62553397</td>
<td class="org-right">-399.9876885136</td>
</tr>

<tr>
<td class="org-right">154.97005460</td>
<td class="org-right">-399.9774175487</td>
</tr>

<tr>
<td class="org-right">150.78475335</td>
<td class="org-right">-399.9667603369</td>
</tr>

<tr>
<td class="org-right">146.97722201</td>
<td class="org-right">-399.9557686286</td>
</tr>

<tr>
<td class="org-right">143.49380641</td>
<td class="org-right">-399.9445262604</td>
</tr>
</tbody>
</table>
<p>Copyright (C) 2017 by John Kitchin. See the <a href="/copying.html">License</a> for information about copying.<p>
<p><a href="/org/2017/11/17/Using-autograd-in-nonlinear-regression.org">org-mode source</a></p>
<p>Org-mode version = 9.1.2</p>

    </div>
  </div>
</article>



<a href="https://twitter.com/share" class="twitter-share-button" data-via="johnkitchin">Share on Twitter</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>


<a href="https://twitter.com/search?q=https://kitchingroup.cheme.cmu.edu/blog/2017/11/17/Using-autograd-in-nonlinear-regression">Discuss on Twitter</a>

  <hr class="interblog" />

          </div>
          <div id="sidebar" class="grid_4">
            <aside>
<section>
<script>
  (function() {
    var cx = '002533177287215655227:l7uvu35ssbc';
    var gcse = document.createElement('script');
    gcse.type = 'text/javascript';
    gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(gcse, s);
  })();
</script>
<gcse:search></gcse:search>
</section>

<section>
    <h1 class="post_header_gradient theme_font">Twitter</h1>
    <a class="twitter-timeline" href="https://twitter.com/johnkitchin" data-widget-id="545217643582881792">Tweets by @johnkitchin</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</section>


  <section>
    <h1 class="post_header_gradient theme_font">Links</h1>
    <ul>
      <li><a href="https://www.continuum.io">Anaconda Python</a></li>
      <li><a href="/pycse">Pycse</a></li>
      <li><a href="/dft-book">DFT-book</a></li>
    </ul>
  </section>

  <section>
    <h1 class="post_header_gradient theme_font">Latest Posts</h1>
    <ul>
      <li><a href="/blog/2025/02/01/New-publication-Improved-Uncertainty-Estimation-of-Graph-Neural-Network-Potentials-Using-Engineered-Latent-Space-Distance/">New publication - Improved Uncertainty Estimation of Graph Neural Network Potentials Using Engineered Latent Space Distance</a></li>
      <li><a href="/blog/2025/01/31/New-publication-AdsorbDiff-:-Adsorbate-Placement-via-Conditional-Denoising-Diffusion/">New publication - AdsorbDiff &#58; Adsorbate Placement via Conditional Denoising Diffusion</a></li>
      <li><a href="/blog/2025/01/30/New-publication-Unifying-theory-of-electronic-descriptors-of-metal-surfaces-upon-perturbation/">New publication - Unifying theory of electronic descriptors of metal surfaces upon perturbation</a></li>
      <li><a href="/blog/2025/01/29/New-publication-The-Potential-of-Zero-Total-Charge-Predicts-Cation-Effects-for-the-Oxygen-Reduction-Reaction/">New publication - The Potential of Zero Total Charge Predicts Cation Effects for the Oxygen Reduction Reaction</a></li>
      <li><a href="/blog/2025/01/28/New-publication-Investigating-the-Error-Imbalance-of-Large-Scale-Machine-Learning-Potentials-in-Catalysis/">New publication - Investigating the Error Imbalance of Large-Scale Machine Learning Potentials in Catalysis</a></li>
    </ul>
  </section>

<section>
<h1 class="post_header_gradient theme_font">Latest GitHub Repos</h1>
  <a href="https://github.com/jkitchin">@jkitchin</a> on GitHub.
  <ul id="my-github-projects">
        <li class="loading">Status updating&#8230;</li>
  </ul>

</section>
</aside>

          </div>
          <div class="clear"></div>
        </div>
      </div>
      
<footer>
  <div id="footer" class="grid_12">
    <div class="grid_8">
      <p>
        <a href="/blog/feed/index.xml">RSS</a>
      </p>
    </div>
    <div class="grid_4" id="credits">
      <p>
        Copyright 2025
        John Kitchin
      </p>
      <p>
        Powered by <a href="http://www.blogofile.com">Blogofile</a>
      </p>
    </div>
  </div>
</footer>

    </div>
      <script src="//ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/js/libs/jquery-1.5.1.min.js"%3E%3C/script%3E'))</script>
  <script src="/js/plugins.js"></script>
  <script src="/js/script.js"></script>
  <script src="/js/jquery.tweet.js"></script>  
  <script src="/js/site.js"></script>
  <!--[if lt IE 7 ]>
  <script src="js/libs/dd_belatedpng.js"></script>
  <script> DD_belatedPNG.fix('img, .png_bg');</script>
  <![endif]-->
 
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PH8NF4F0RE"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PH8NF4F0RE');
</script>


  </body>
</html>






<script src="http://ajax.microsoft.com/ajax/jquery/jquery-1.4.2.min.js" type="text/javascript"></script>
<script src="/js/git.js" type="text/javascript"></script>
<script type="text/javascript">
    $(function() {
     $("#my-github-projects").loadRepositories("jkitchin");
    });
</script>



