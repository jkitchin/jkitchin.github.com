<?xml version="1.0" encoding="UTF-8"?>

<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <atom:link href="http://kitchingroup.cheme.cmu.edu/blog/feed/index.xml" rel="self" type="application/rss+xml" />
    <title>The Kitchin Research Group</title>
    <link>https://kitchingroup.cheme.cmu.edu/blog</link>
    <description>Chemical Engineering at Carnegie Mellon University</description>
    <pubDate>Sat, 27 Sep 2025 15:51:00 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    
    <item>
      <title>New publication - Uncertainty Quantification in Graph Neural Networks With    Shallow Ensembles</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/09/27/New-publication-Uncertainty-Quantification-in-Graph-Neural-Networks-With-Shallow-Ensembles</link>
      <pubDate>Sat, 27 Sep 2025 11:50:48 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">s_fSx3dppxsWwS3QKPjeYvxDS7A=</guid>
      <description>New publication - Uncertainty Quantification in Graph Neural Networks With    Shallow Ensembles</description>
      <content:encoded><![CDATA[


&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/schnet_uncertainty_adventure.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
In our latest work, we tackled a critical challenge in materials modeling: ensuring that AI models don't just make predictions, but also tell us &lt;i&gt;how confident&lt;/i&gt; they are in those predictions. When Graph Neural Networks (GNNs) encounter new, "out-of-domain" materials they haven't seen during training, their predictions can become unreliable, and it's tough to know when that's happening. So, we integrated a clever, lightweight technique called Direct Propagation of Shallow Ensembles (DPOSE) into a GNN model called SchNet. Essentially, DPOSE allows the model to estimate its own uncertainty efficiently. Our findings showed that this approach is really effective at flagging when the model is venturing into unfamiliar territory, giving us higher uncertainty for novel molecules or material structures across various datasets. While it performed well, we also learned about its limitations in distinguishing very subtle structural differences. Ultimately, this work is a step towards building more trustworthy AI for materials discovery, paving the way for smarter active learning strategies where the AI itself helps decide what new data to explore.
&lt;/p&gt;


&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;vinchurkar-2025-uncer-quant&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Tirtha Vinchurkar and Kareem Abdelmaqsoud and John R Kitchin},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Uncertainty Quantification in Graph Neural Networks With
                  Shallow Ensembles},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Machine Learning: Science and Technology},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {nil},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1088/2632-2153/ae0bf0&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1088/2632-2153/ae0bf0&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Sat Sep 27 11:42:36 2025},
}

&lt;/pre&gt;
&lt;/div&gt;


&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1088/2632-2153/ae0bf0'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;
Curious about using an LLM to interact with this paper? Check out
&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/kHsbUjgCGBY?si=bejDCZJqdXTExL0J" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/09/27/New-publication---Uncertainty-Quantification-in-Graph-Neural-Networks-With-Shallow-Ensembles.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>New publication - Towards Agentic Science for Advancing Scientific Discovery</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/09/16/New-publication-Towards-Agentic-Science-for-Advancing-Scientific-Discovery</link>
      <pubDate>Tue, 16 Sep 2025 13:38:54 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">L2nYbbOG0rMQ9_4L2K7M3nJNrlI=</guid>
      <description>New publication - Towards Agentic Science for Advancing Scientific Discovery</description>
      <content:encoded><![CDATA[


&lt;p&gt;
In our new paper published in Nature Machine Intelligence, my colleagues Hongliang Xin, Heather Kulik, and I explore what we call "agentic science" – a new paradigm where AI agents can (semi-)autonomously conduct scientific research.
&lt;/p&gt;

&lt;p&gt;
Scientific discovery has evolved through distinct eras: from early empirical observations and theoretical frameworks like Newtonian mechanics, through the computational modeling revolution, to today's data science approaches. We argue that we're now entering the age of agentic science, where AI systems don't just analyze data but can independently reason, plan experiments, and interact with both digital tools and physical laboratory equipment.
&lt;/p&gt;

&lt;p&gt;
What makes these AI agents special is their capacity for independent agency. Built around large language models that can process text, images, and structured data, they can actively learn, integrate with external tools, and think strategically about long-term research goals. Systems like Coscientist can already interpret natural language requests and autonomously operate lab equipment, while A-Lab represents a fully autonomous materials synthesis laboratory.
&lt;/p&gt;

&lt;p&gt;
However, we're honest about the challenges. These systems can "hallucinate" – producing convincing but incorrect information – and they're sensitive to how questions are phrased. We also lack standardized ways to evaluate their performance, and they consume significant computational resources.
&lt;/p&gt;

&lt;p&gt;
The key to success lies in maintaining human oversight while leveraging AI for high-throughput tasks. With proper safeguards, transparent documentation, and ethical considerations, agentic AI could dramatically accelerate scientific discovery while actually improving reproducibility by systematically analyzing literature and identifying research gaps.
&lt;/p&gt;

&lt;p&gt;
We believe this represents a fundamental shift in how science gets done – not replacing human scientists, but creating powerful human-AI partnerships that could unlock new pathways to discovery.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;xin-2025-towar-agent&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Hongliang Xin and John R. Kitchin and Heather J. Kulik},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Towards Agentic Science for Advancing Scientific Discovery},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Nature Machine Intelligence},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {nil},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1038/s42256-025-01110-x&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1038/s42256-025-01110-x&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Tue Sep 16 13:36:03 2025},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1038/s42256-025-01110-x'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/t_A_TAa3t1A?si=mYM2UTghkiL0nUJO" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/09/16/New-publication---Towards-Agentic-Science-for-Advancing-Scientific-Discovery.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>New publication - Mapping uncertainty using differentiable programming</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/07/31/New-publication-Mapping-uncertainty-using-differentiable-programming</link>
      <pubDate>Thu, 31 Jul 2025 13:42:54 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">O3DUeMu59LLLkgGGyXgKFlWQhhc=</guid>
      <description>New publication - Mapping uncertainty using differentiable programming</description>
      <content:encoded><![CDATA[


&lt;p&gt;
In our latest work, we set out to tackle a common challenge in engineering and science: understanding how small uncertainties in inputs, like temperature changes or slight variations in pressure, can ripple through complex systems and affect the final outcome. Instead of relying on slow, trial-and-error simulations, we leveraged an emerging computing technique that treats uncertainty like a path we can follow mathematically. By "teaching" our software to calculate these paths directly, we can predict how errors build up in real processes, whether in a chemical reactor, a filtration system, or a bioreactor, 100 times faster than traditional methods. This speed and precision mean we can design safer, more reliable systems and respond more quickly when things don't go exactly as planned.
&lt;/p&gt;

&lt;p&gt;
We introduce a differentiable-programming-based framework for uncertainty quantification that leverages the implicit function theorem and path integration to compute both forward and inverse uncertainty maps directly from high-fidelity or surrogate models . Our approach requires only C&lt;sup&gt;1&lt;/sup&gt; differentiability and injectivity of the implicit system and avoids expensive Monte Carlo sampling by tracing uncertainty "contours" through the model. We validate it on three chemical-engineering case studies: a CSTR (showing exact agreement with analytical solutions in unimodal and multimodal scenarios), a membrane reactor for natural-gas aromatization (recovering 95% of exhaustive-search samples in 7 min vs. ~10 h), and a fed-batch bioreactor with 3D ellipsoidal uncertainty regions. All code and reproducible Jupyter notebooks are available at github.com/KitchinHUB/Mapping-Uncertainty-Using-Differentiable-Programming.
&lt;/p&gt;


&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-31-07-2025-time-13-25-21.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;


&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;alves-2025-mappin-uncer&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Victor Alves and Carl D. Laird and Fernando V. Lima and John
                  R. Kitchin},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Mapping Uncertainty Using Differentiable Programming},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {AIChE Journal},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {e18940},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1002/aic.18940&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1002/aic.18940&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Thu Jul 31 09:53:53 2025},
}
&lt;/pre&gt;
&lt;/div&gt;


&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1002/aic.18940'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/07/31/New-publication---Mapping-uncertainty-using-differentiable-programming.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>New publication - Spin-informed universal graph neural networks for simulating magnetic ordering</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/07/17/New-publication-Spin-informed-universal-graph-neural-networks-for-simulating-magnetic-ordering</link>
      <pubDate>Thu, 17 Jul 2025 20:07:27 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">BJXMMMg332FqKCCPa5eZs0sHspg=</guid>
      <description>New publication - Spin-informed universal graph neural networks for simulating magnetic ordering</description>
      <content:encoded><![CDATA[


&lt;p&gt;
In this work, we developed a data-efficient, spin-informed graph neural network framework that augments universal machine-learning interatomic potentials with explicit spin coordinates and initial magnetic-moment guesses, while rigorously preserving the physical symmetries of collinear magnetism. This allows us to predict both the magnitude and direction of atomic spins in bulk and surface materials. By integrating a closed-loop anomaly detection pipeline based on Gaussian mixture models and z-score outlier filtering, we uncovered and corrected mislabeled DFT data, substantially improving dataset quality and model robustness. The resulting SI-GemNet-OC model achieves state-of-the-art accuracy, dramatically speeds up DFT convergence (e.g., reducing SCF cycles for GdDyAl₄ from 211 to 28), and successfully ranks magnetic orderings across hundreds of compounds with a Spearman’s ρ of 0.896. Importantly, we also show that this approach generalizes to complex surface and adsorbate-induced spin configurations, offering a powerful new tool for high-throughput discovery of magnetic materials.
&lt;/p&gt;



&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-01-07-2025-time-16-58-18.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;



&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;xu-2025-spin-infor&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Wenbin Xu and Rohan Yuri Sanspeur and Adeesh Kolluru and Bowen
                  Deng and Peter Harrington and Steven Farrell and Karsten
                  Reuter and John R. Kitchin },
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Spin-Informed Universal Graph Neural Networks for Simulating
                  Magnetic Ordering},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Proceedings of the National Academy of Sciences},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       122,
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       27,
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {e2422973122},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1073/pnas.2422973122&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;URL&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://www.pnas.org/doi/abs/10.1073/pnas.2422973122&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;eprint&lt;/span&gt; =       {https://www.pnas.org/doi/pdf/10.1073/pnas.2422973122},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1073/pnas.2422973122'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/07/17/New-publication---Spin-informed-universal-graph-neural-networks-for-simulating-magnetic-ordering.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>New publication - Hyperplane decision trees as piecewise linear surrogate models for chemical process design</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/07/09/New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design</link>
      <pubDate>Wed, 09 Jul 2025 14:52:57 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">04VPFP6piWTCrkuI-RCZm74USQY=</guid>
      <description>New publication - Hyperplane decision trees as piecewise linear surrogate models for chemical process design</description>
      <content:encoded><![CDATA[


&lt;p&gt;
We’ve developed a new kind of decision-tree model that’s both smart and practical for tackling tough engineering problems. First, we take raw data and "lift" it into a richer feature space so we can slice it more cleverly including angular shapes. Next, we grow a friendly “hyperplane” tree that splits data along these angled cuts, fitting simple linear models in each branch. The result is a piecewise-linear surrogate that behaves a lot like the real system but runs orders of magnitude faster. Finally, because each piece is just a linear model, we can plug the whole thing straight into an optimizer that finds the very best solution under complex rules. That means we can design chemical processes, heat exchangers, or any engineering system more reliably and sustainably - saving time, energy, and cost.
&lt;/p&gt;


&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-09-07-2025-time-14-19-14.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;



&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;sunshine-2025-hyper-decis&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Ethan M. Sunshine and Carolina Colombo Tedesco and Sneha A.
                  Akhade and Matthew J. McNenly and John R. Kitchin and Carl D.
                  Laird},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Hyperplane Decision Trees As Piecewise Linear Surrogate Models
                  for Chemical Process Design},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Computers \&amp;amp; Chemical Engineering},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        109204,
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1016/j.compchemeng.2025.109204&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1016/j.compchemeng.2025.109204&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Wed Jul 9 14:14:17 2025},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1016/j.compchemeng.2025.109204'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/07/09/New-publication---Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>Lies, damn lies, statistics and Bayesian statistics</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics</link>
      <pubDate>Sun, 22 Jun 2025 11:14:23 EDT</pubDate>
      <category><![CDATA[machine-learning]]></category>
      <guid isPermaLink="false">14mSE18aHhiNdMq3g69gXgG5w2o=</guid>
      <description>Lies, damn lies, statistics and Bayesian statistics</description>
      <content:encoded><![CDATA[


&lt;div id="table-of-contents" role="doc-toc"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents" role="doc-toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#org035da70"&gt;1. The data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org28ad175"&gt;2. GPR with a RBF kernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orga7fb619"&gt;3. a better kernel solves these issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orgca7200a"&gt;4. How about with feature engineering?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orgdd16d18"&gt;5. Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
This post on LinkedIn (&lt;a href="https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI"&gt;https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI&lt;/a&gt;) reminded me of a quip I often make of "Lies, damn lies, statistics, and Bayesian statistics". I am frequently skeptical of claims about "Bayesian something something", especially when the claim is about uncertainty quantification. That skepticism comes from practical experience of mine that "Bayesian something something" is rarely as well behaved and informative as advertised (in my hands of course).
&lt;/p&gt;

&lt;p&gt;
To illustrate, I will use some noisy, 1d data from a Lennard-Jones function and Gaussian process regression to fit the data.
&lt;/p&gt;
&lt;div id="outline-container-org035da70" class="outline-2"&gt;
&lt;h2 id="org035da70"&gt;&lt;span class="section-number-2"&gt;1.&lt;/span&gt; The data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
We get our data by sampling a Lennard-Jones function, adding some noise, and removing a gap in the data. The gap in the middle might be classically considered an interpolation region.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; numpy &lt;span style="color: #0000FF; font-weight: bold;"&gt;as&lt;/span&gt; np
&lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style="color: #0000FF; font-weight: bold;"&gt;as&lt;/span&gt; plt

&lt;span style="color: #BA36A5;"&gt;r&lt;/span&gt; = np.linspace(0.95, 3, 200)

&lt;span style="color: #BA36A5;"&gt;eps&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;sig&lt;/span&gt; = 1, 1
&lt;span style="color: #BA36A5;"&gt;y&lt;/span&gt; = 4 * eps * ((1 / r)**12 - (1 / r)**6) + np.random.normal(0, 0.03, size=r.shape)


&lt;span style="color: #BA36A5;"&gt;ind&lt;/span&gt; = ((r &amp;gt; 1) &amp;amp; (r &amp;lt; 1.25)) | ((r &amp;gt; 2) &amp;amp; (r &amp;lt; 2.5))
&lt;span style="color: #BA36A5;"&gt;_R&lt;/span&gt; = r[ind][:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]
&lt;span style="color: #BA36A5;"&gt;_y&lt;/span&gt; = y[ind]
plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)
plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/653165863df7654b10ddaca2f7645560768bd870.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org28ad175" class="outline-2"&gt;
&lt;h2 id="org28ad175"&gt;&lt;span class="section-number-2"&gt;2.&lt;/span&gt; GPR with a RBF kernel&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
The RBF kernel is the most standard kernel. It does an ok job fitting here, although I see evidence of overfitting (the wiggles are caused by the noise). You can reduce the overfitting by using a larger &lt;code&gt;alpha&lt;/code&gt; value in the gpr, but that requires you to know in advance how smooth it should be.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF; font-weight: bold;"&gt;from&lt;/span&gt; sklearn.gaussian_process &lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; GaussianProcessRegressor
&lt;span style="color: #0000FF; font-weight: bold;"&gt;from&lt;/span&gt; sklearn.gaussian_process.kernels &lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; RBF, WhiteKernel
&lt;span style="color: #BA36A5;"&gt;kernel&lt;/span&gt; = RBF() + WhiteKernel()
&lt;span style="color: #BA36A5;"&gt;gpr&lt;/span&gt; = GaussianProcessRegressor(kernel=kernel,
                               random_state=0, normalize_y=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;).fit(_R, _y)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;)
plt.plot(r, y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;, alpha=0.2)

&lt;span style="color: #BA36A5;"&gt;yp&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;se&lt;/span&gt; = gpr.predict(r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;], return_std=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;, r, yp - 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;);
plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)
plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);

gpr.kernel_
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
RBF(length_scale=0.0948) + WhiteKernel(noise_level=0.00635)
&lt;/pre&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/6acc7dccc37ee773aeb4c97c62929401733a02f6.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
The uncertainty here is primarily related to the model, i.e. it is constrained to be correct where there is data, but with no data, the model is not the right one.
&lt;/p&gt;

&lt;p&gt;
The model does well in the region where there is data, but is qualitatively wrong in the gap (even though classically this would be considered interpolation), and overestimates the uncertainty in this region. The problem is the covariance kernel decays to 0 about two length scales away from the last point, which means there is no data to inform what the weights in that region should look like.  That causes the model to revert to the mean of the data.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;gpr.predict([[1.8]]), gpr.predict([[3.0]]), np.mean(_y)
&lt;/pre&gt;
&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col  class="org-left" /&gt;

&lt;col  class="org-left" /&gt;

&lt;col  class="org-left" /&gt;

&lt;col  class="org-left" /&gt;

&lt;col  class="org-right" /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;array&lt;/td&gt;
&lt;td class="org-left"&gt;((-0.2452041))&lt;/td&gt;
&lt;td class="org-left"&gt;array&lt;/td&gt;
&lt;td class="org-left"&gt;((-0.29363654))&lt;/td&gt;
&lt;td class="org-right"&gt;-0.2936364964541409&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Why is this happening? It is not that tricky. You can think of the GP as an expansion of the data in basis functions. The kernel trick effectively makes this expansion in the infinite limit. What are those basis functions? We can draw samples of them, which we show here. You can see that where there is no data, the basis functions are "wiggly". That means they are simply not good at making predictions here.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;y_samples&lt;/span&gt; = gpr.sample_y(r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;], n_samples=15, random_state=0)

plt.plot(r, yp)
plt.plot(r, yp + 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;, r, yp - 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;);
plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)

plt.plot(r, y_samples, &lt;span style="color: #008000;"&gt;'k'&lt;/span&gt;, alpha=0.2);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/cff4cfa5cacedcef6ecde8ec2b63dcee659949fb.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;


&lt;p&gt;
This kernel simply cannot be used for extrapolation, or any predictions more than about two length scales away from the nearest point. Calling it Bayesian doesn't make it better. For similar reasons, this model will not work well outside the data range.
&lt;/p&gt;

&lt;p&gt;
A practical person would still consider using this model, and might even rely on the uncertainty being too large to identify regions of low reliability.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga7fb619" class="outline-2"&gt;
&lt;h2 id="orga7fb619"&gt;&lt;span class="section-number-2"&gt;3.&lt;/span&gt; a better kernel solves these issues&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
Not all is lost, if we know more. In this case we can construct a kernel that reflects our understanding that the data came from a Lennard Jones like interaction model. You can construct kernels by adding and multiplying kernels. Here we consider a linear kernel, the &lt;code&gt;DotProduct&lt;/code&gt; kernel, and construct a new kernel that is a sum of the linear kernel to the 12&lt;sup&gt;th&lt;/sup&gt; power, a linear kernel to the 6&lt;sup&gt;th&lt;/sup&gt; power, and a &lt;code&gt;WhiteKernel&lt;/code&gt; for the noise. It is a little subtle that this kernel should work in \(1 / r\) space, so in addition to kernel engineering, we also do feature engineering.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF; font-weight: bold;"&gt;from&lt;/span&gt; sklearn.gaussian_process.kernels &lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; DotProduct

&lt;span style="color: #BA36A5;"&gt;kernel&lt;/span&gt; = DotProduct()**12 + DotProduct()**6 +  WhiteKernel()
&lt;span style="color: #BA36A5;"&gt;gpr&lt;/span&gt; = GaussianProcessRegressor(kernel=kernel, normalize_y=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;).fit(1 / _R, _y)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;)
plt.plot(r, y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;, alpha=0.2)


&lt;span style="color: #BA36A5;"&gt;yp&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;se&lt;/span&gt; = gpr.predict(1 / r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;], return_std=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;, r, yp - 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);

gpr.kernel_
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
DotProduct(sigma_0=0.0281) ** 12 + DotProduct(sigma_0=0.936) ** 6 + WhiteKernel(noise_level=0.0077)
&lt;/pre&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/70e91f8419a473ed578a14442694e67a3409bd1e.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
Note that this GPR does fine in the gap, including the right level of uncertainty there. This model is better because we used the kernel to constrain what forms the model can have. This model actually extrapolates correctly outside the data. It is worth noting that although this model has great predictive and UQ properties, it does not tell us anything about the values of &amp;epsilon; and &amp;sigma; in the Lennard Jones model. Although we might say the kernel is physics-based, i.e. it is based on the relevant features and equation, it does not have physical parameters in it.
&lt;/p&gt;

&lt;p&gt;
How about those basis functions here? You can see that all of them basically look like the LJ potential. That means they are good basis functions to expand this data set in.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;y_samples&lt;/span&gt; = gpr.sample_y(1 / r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;], n_samples=15, random_state=0)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)

plt.plot(r, y_samples, &lt;span style="color: #008000;"&gt;'k'&lt;/span&gt;, alpha=0.2);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/e7fe34a01c52cb228cbbcde85e5f334e7f8237a1.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgca7200a" class="outline-2"&gt;
&lt;h2 id="orgca7200a"&gt;&lt;span class="section-number-2"&gt;4.&lt;/span&gt; How about with feature engineering?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
Can we do even better with feature engineering here? Motivated by &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7342573774774386688?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7342573774774386688%2C7342949865590530052%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287342949865590530052%2Curn%3Ali%3Aactivity%3A7342573774774386688%29"&gt;this comment&lt;/a&gt; by Cory Simon, we cast the problem as a linear regression in [1 / r&lt;sup&gt;6&lt;/sup&gt;, 1 / r&lt;sup&gt;12&lt;/sup&gt;] feature space. This is also a perfectly reasonable thing to do. Since our output is linear in these features, we simply use a linear kernel (aka the DotProduct kernel in sklearn).
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;r6&lt;/span&gt; = 1 / _R**6
&lt;span style="color: #BA36A5;"&gt;r12&lt;/span&gt; = r6**2

&lt;span style="color: #BA36A5;"&gt;kernel&lt;/span&gt; = DotProduct() + WhiteKernel()

&lt;span style="color: #BA36A5;"&gt;gpr&lt;/span&gt; = GaussianProcessRegressor(kernel=kernel, normalize_y=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;).fit(np.hstack([r6, r12]), _y)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;)
plt.plot(r, y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;, alpha=0.2)

&lt;span style="color: #BA36A5;"&gt;fr6&lt;/span&gt; = 1 / r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]**6
&lt;span style="color: #BA36A5;"&gt;fr12&lt;/span&gt; = fr6**2

&lt;span style="color: #BA36A5;"&gt;yp&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;se&lt;/span&gt; = gpr.predict(np.hstack([fr6, fr12]), return_std=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;, r, yp - 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);

gpr.kernel_
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
DotProduct(sigma_0=0.74) + WhiteKernel(noise_level=0.00654)
&lt;/pre&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/d8769fe652b9e902e3d349ce26cdbd7d8050b190.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
We can't easily plot these basis functions the same way, so we reduce them to a 1-d plot. You can see here that these basis functions practically the same as the one with the advanced kernel.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;y_samples&lt;/span&gt; = gpr.sample_y(np.hstack([fr6, fr12]),
                         n_samples=15, random_state=0)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)

plt.plot(r, y_samples, &lt;span style="color: #008000;"&gt;'k'&lt;/span&gt;, alpha=0.2);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/f777586bb8e17bac5ca3dadbfba97119addeb46b.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;



&lt;p&gt;
This also works quite well, and is another way to leverage knowledge about what we are building a model for.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdd16d18" class="outline-2"&gt;
&lt;h2 id="orgdd16d18"&gt;&lt;span class="section-number-2"&gt;5.&lt;/span&gt; Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
Naive use of GPR can provide useful models when you have enough data, but these models likely do not accurately capture uncertainty outside that data, nor is it likely they are reliable in extrapolation. It is possible to do better than this, when you know what to do. Through feature and kernel engineering, you can sometimes create situations where the problem essentially becomes linear regression, where a simple linear kernel is what you want, or you develop a kernel that represents the underlying model. Kernel engineering is generally hard, with limited opportunities to be flexible. See &lt;a href="https://www.cs.toronto.edu/~duvenaud/cookbook/"&gt;https://www.cs.toronto.edu/~duvenaud/cookbook/&lt;/a&gt; for examples of kernels and combining them.
&lt;/p&gt;

&lt;p&gt;
You can see it is not adequate to say "we used Gaussian process regression". That is about as informative as saying linear regression without identifying the features, or nonlinear regression and not saying what model. You have to be specific about the kernel, and thoughtful about how you know if a prediction is reliable or not. Just because you get an uncertainty prediction doesn't mean its right.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/06/22/Lies,-damn-lies,-statistics-and-Bayesian-statistics.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>New Publication - Solving an inverse problem with generative models</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/06/17/New-Publication-Solving-an-inverse-problem-with-generative-models</link>
      <pubDate>Tue, 17 Jun 2025 13:24:43 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">z6MMLUeScTzI-71IEjgrFHiJj-0=</guid>
      <description>New Publication - Solving an inverse problem with generative models</description>
      <content:encoded><![CDATA[


&lt;p&gt;
Inverse problems—where we aim to find inputs that produce a desired output—are notoriously challenging in science and engineering. In this study, I explore how generative AI models can tackle these problems by comparing four approaches: a forward model combined with nonlinear optimization, a backward model using partial least squares regression, and two generative methods based on Gaussian mixture models and diffusion-based flow transformations. Using data from a simple RGB-controlled light sensor, the paper demonstrates that generative models can accurately and flexibly infer input settings for target outputs, with advantages such as uncertainty quantification and the ability to condition on partial outputs. This work showcases the promise of generative modeling in reshaping how we approach inverse problems across disciplines.
&lt;/p&gt;


&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/zx6FHzx8V-Y?si=1OQBQ25Ze8e5mzZl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;kitchin-2025-solvin-inver&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       "Kitchin, John R.",
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Solving an Inverse Problem With Generative Models},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      "Digital Discovery",
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        "-",
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          "&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1039/D5DD00137D&lt;/span&gt;",
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          "&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1039/D5DD00137D&lt;/span&gt;",
  &lt;span style="color: #BA36A5;"&gt;abstract&lt;/span&gt; =     "Inverse problems{,} where we seek the values of inputs to a
                  model that lead to a desired set of outputs{,} are a
                  challenges subset of problems in science and engineering. In
                  this work we demonstrate the use of two generative AI methods
                  to solve inverse problems. We compare this approach to two
                  more conventional approaches that use a forward model with
                  nonlinear programming{,} and the use of a backward model. We
                  illustrate each method on a dataset obtained from a simple
                  remote instrument that has three inputs: the setting of the
                  red{,} green and blue channels of an RGB LED. We focus on
                  several outputs from a light sensor that measures intensity at
                  445 nm{,} 515 nm{,} 590 nm{,} and 630 nm. The speci&amp;#64257;c problem
                  we solve is identifying inputs that lead to a speci&amp;#64257;c
                  intensity in three of those channels. We show that generative
                  models can be used to solve this kind of inverse problem{,}
                  and they have some advantages over the conventional
                  approaches.",
  &lt;span style="color: #BA36A5;"&gt;publisher&lt;/span&gt; =    "RSC",
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1039/D5DD00137D'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/06/17/New-Publication---Solving-an-inverse-problem-with-generative-models.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>New publication - The Evolving Role of Programming and LLMs in the Development of Self-Driving Laboratories</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/05/07/New-publication-The-Evolving-Role-of-Programming-and-LLMs-in-the-Development-of-Self-Driving-Laboratories</link>
      <pubDate>Wed, 07 May 2025 06:58:27 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">h80Asf6Ovi4JvGXCEK4n26vrlic=</guid>
      <description>New publication - The Evolving Role of Programming and LLMs in the Development of Self-Driving Laboratories</description>
      <content:encoded><![CDATA[


&lt;p&gt;
In this paper, I introduce Claude-Light, a lightweight self-driving lab prototype built on a Raspberry Pi with an RGB LED and ten-channel photometer, all accessible via a simple REST API and Python library. By demonstrating structured automation—from basic scripting and statistical design of experiments through Gaussian process active learning—and exploring large language models for instrument selection, structured data extraction, function calling, and code generation, I showcase both the opportunities and challenges LLMs bring to lab automation (reproducibility, security, and reliability). Claude-Light lowers the barrier for students and researchers to prototype and test automation and AI-driven experimentation before scaling to full self-driving laboratories.
&lt;/p&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-05-05-2025-time-12-10-14.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;pre class="example" id="org5521f3c"&gt;
@article{kitchin-2025-evolv-role,
  author =	 {John R. Kitchin},
  title =	 {The Evolving Role of Programming and LLMs in the Development
                  of Self-Driving Laboratories},
  journal =	 {APL Machine Learning},
  volume =	 3,
  number =	 2,
  pages =	 {026111},
  year =	 2025,
  doi =		 {10.1063/5.0266757},
  url =		 {http://dx.doi.org/10.1063/5.0266757},
  DATE_ADDED =	 {Thu May 1 09:22:44 2025},
}
&lt;/pre&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1063/5.0266757'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/05/07/New-publication---The-Evolving-Role-of-Programming-and-LLMs-in-the-Development-of-Self-Driving-Laboratories.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>New publication - A Classification-based Methodology for the Estimation of Binary Surfactant Critical Micelle Concentrations</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/04/11/New-publication-A-Classification-based-Methodology-for-the-Estimation-of-Binary-Surfactant-Critical-Micelle-Concentrations</link>
      <pubDate>Fri, 11 Apr 2025 09:27:37 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">g9LSDtjz1IxxW-0ZvachZPdCTUs=</guid>
      <description>New publication - A Classification-based Methodology for the Estimation of Binary Surfactant Critical Micelle Concentrations</description>
      <content:encoded><![CDATA[


&lt;p&gt;
In our latest paper, we developed a high-throughput method to efficiently determine the critical micelle concentration (CMC) of binary surfactant mixtures using a 96-well plate setup. Instead of relying on traditional regression techniques, we used a physics-informed classification approach based on regular solution theory to identify the micellization boundary. By combining model-driven experimental design with a dye solubilization assay, we mapped out the CMC across mixture compositions and accurately extracted the binary interaction parameter, β. We validated the method using the SDS-C8E4 system, and extended it to an electrolyte-rich environment, showing less than 15–18% deviation from literature values. This approach not only accelerates formulation screening but also lays the groundwork for analyzing more complex surfactant systems in the future.
&lt;/p&gt;



&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-11-04-2025-time-09-10-54.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;



&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;D5DD00058K&lt;/span&gt;,
        &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; = {Chilkunda, Chetan R and Kitchin, John R. and Tilton, Robert D.},
        &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; = {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1039/D5DD00058K&lt;/span&gt;},
        &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; = {Digital Discovery},
        &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; = {-},
        &lt;span style="color: #BA36A5;"&gt;publisher&lt;/span&gt; = {RSC},
        &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; = {A Classification-based Methodology for the Estimation of Binary Surfactant Critical Micelle Concentrations},
        &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; = {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1039/D5DD00058K&lt;/span&gt;},
        &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; = {2025}}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1039/D5DD00058K'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/04/11/New-publication---A-Classification-based-Methodology-for-the-Estimation-of-Binary-Surfactant-Critical-Micelle-Concentrations.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
    <item>
      <title>New publication - CatTsunami Accelerating Transition State Energy Calculations With Pretrained Graph Neural Networks</title>
      <link>https://kitchingroup.cheme.cmu.edu/blog/2025/03/17/New-publication-CatTsunami-Accelerating-Transition-State-Energy-Calculations-With-Pretrained-Graph-Neural-Networks</link>
      <pubDate>Mon, 17 Mar 2025 20:58:15 EDT</pubDate>
      <category><![CDATA[publication]]></category>
      <category><![CDATA[news]]></category>
      <guid isPermaLink="false">57lUmlBuVN7iW9__ddNOONLkQqI=</guid>
      <description>New publication - CatTsunami Accelerating Transition State Energy Calculations With Pretrained Graph Neural Networks</description>
      <content:encoded><![CDATA[


&lt;p&gt;
In this work, we tackled the challenge of accelerating catalyst discovery by focusing on transition state energy calculations. We show that a graph neural network potential, despite being trained on a different task, could accurately predict transition states—a crucial step in catalyst discovery—with a remarkable 28x speedup over traditional methods. To provide a benchmark for machine learning model performance in this area, we also curated the Open Catalyst 2020 Nudged Elastic Band (OC20NEB) dataset, which includes 932 DFT nudged elastic band calculations. To showcase the effectiveness of our approach, we applied it to two case studies: reaction mechanism search and ammonia synthesis. These demonstrations highlighted the significant potential of machine learning to enhance and speed up catalyst research.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;wander-2025-catts&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Brook Wander and Muhammed Shuaibi and John R. Kitchin and
                  Zachary W. Ulissi and C. Lawrence Zitnick},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {{CatTsunami}: Accelerating Transition State Energy Calculations
                  With Pretrained Graph Neural Networks},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {ACS Catalysis},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {5283-5294},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1021/acscatal.4c04272&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1021/acscatal.4c04272&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Mon Mar 17 20:50:26 2025},
}
&lt;/pre&gt;
&lt;/div&gt;



&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1021/acscatal.4c04272'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/03/17/New-publication---CatTsunami-Accelerating-Transition-State-Energy-Calculations-With-Pretrained-Graph-Neural-Networks.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content:encoded>
    </item>
  </channel>
</rss>
