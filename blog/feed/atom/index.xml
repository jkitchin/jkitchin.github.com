<?xml version="1.0" encoding="UTF-8"?>

<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
  >
  <title type="text">The Kitchin Research Group</title>
  <subtitle type="text">Chemical Engineering at Carnegie Mellon University</subtitle>

  <updated>2018-04-16T17:55:42Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog" />
  <id>http://jkitchin.github.io/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="http://jkitchin.github.io/blog/feed/atom/" />
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication in Nature Catalysis]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2018/04/16/New-publication-in-Nature-Catalysis" />
    <id>http://jkitchin.github.io/blog/2018/04/16/New-publication-in-Nature-Catalysis</id>
    <updated>2018-04-16T12:55:36Z</updated>
    <published>2018-04-16T12:53:52Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="publication" />
    <category scheme="http://jkitchin.github.io/blog" term="news" />
    <summary type="html"><![CDATA[New publication in Nature Catalysis]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2018/04/16/New-publication-in-Nature-Catalysis"><![CDATA[


&lt;p&gt;
Machine learning (ML) is impacting many fields, including catalysis. In this comment, I briefly discuss the major directions that ML is influencing the field of catalysis, along with some outlook on future directions. There were strict word and reference limits, so apologies in advance if I left out your work!
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;kitchin-2018-machin-learn-catal&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {John R. Kitchin},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Machine Learning in Catalysis},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Nature Catalysis},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       1,
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       4,
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {230-232},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2018,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1038/s41929-018-0056-y&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1038/s41929-018-0056-y&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Mon Apr 16 12:50:43 2018},
}
&lt;/pre&gt;
&lt;/div&gt;


&lt;p&gt;
You can see a read-only version of the paper here: &lt;a href="https://rdcu.be/LGrM"&gt;https://rdcu.be/LGrM&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1038/s41929-018-0056-y'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2018 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2018/04/16/New-publication-in-Nature-Catalysis.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.6&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[Caching searches using biblio and only seeing new results]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2018/04/11/Caching-searches-using-biblio-and-only-seeing-new-results" />
    <id>http://jkitchin.github.io/blog/2018/04/11/Caching-searches-using-biblio-and-only-seeing-new-results</id>
    <updated>2018-04-11T20:46:56Z</updated>
    <published>2018-04-11T20:46:56Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="elisp" />
    <category scheme="http://jkitchin.github.io/blog" term="arxiv" />
    <category scheme="http://jkitchin.github.io/blog" term="biblio" />
    <summary type="html"><![CDATA[Caching searches using biblio and only seeing new results]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2018/04/11/Caching-searches-using-biblio-and-only-seeing-new-results"><![CDATA[


&lt;p&gt;
In this &lt;a href="https://github.com/jkitchin/scimax/issues/196"&gt;issue&lt;/a&gt; in scimax, Robert asked if it was possible to save searches, and then to repeat them every so often and only see the new results. This needs some persistent caching of the records, and a comparison of the current search results with the previous search results.
&lt;/p&gt;

&lt;p&gt;
&lt;a href="https://github.com/cpitclaudel/biblio.el"&gt;biblio&lt;/a&gt; provides a nice interface to searching a range of resources for bibliographic references. In this post, I will focus on arxiv. Out of the box, biblio does not seem to support this use case, but as you will see, it has many of the pieces required to achieve it. Let's start picking those pieces apart.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-emacs-lisp"&gt;(&lt;span style="color: #0000FF;"&gt;require&lt;/span&gt; '&lt;span style="color: #D0372D;"&gt;biblio&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
biblio

&lt;/pre&gt;

&lt;p&gt;
Here is the first piece we need: a way to run a query, and get results back as a data structure. Here we just look at the first result.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-emacs-lisp"&gt;(&lt;span style="color: #0000FF;"&gt;let*&lt;/span&gt; ((query &lt;span style="color: #008000;"&gt;"alloy segregration"&lt;/span&gt;)
       (backend 'biblio-arxiv-backend)
       (cb (url-retrieve-synchronously (funcall backend 'url query)))
       (results (&lt;span style="color: #0000FF;"&gt;with-current-buffer&lt;/span&gt; cb
                  (funcall backend 'parse-buffer))))
  (car results))
&lt;/pre&gt;
&lt;/div&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-emacs-lisp"&gt;((doi . &lt;span style="color: #008000;"&gt;"10.1103/PhysRevB.76.014112"&lt;/span&gt;)
 (identifier . &lt;span style="color: #008000;"&gt;"0704.2752v2"&lt;/span&gt;)
 (year . &lt;span style="color: #008000;"&gt;"2007"&lt;/span&gt;)
 (title . &lt;span style="color: #008000;"&gt;"Modelling Thickness-Dependence of Ferroelectric Thin Film Properties"&lt;/span&gt;)
 (authors nil nil nil nil nil nil nil nil nil nil nil nil nil &lt;span style="color: #008000;"&gt;"L. Palova"&lt;/span&gt; nil &lt;span style="color: #008000;"&gt;"P. Chandra"&lt;/span&gt; nil &lt;span style="color: #008000;"&gt;"K. M. Rabe"&lt;/span&gt; nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil)
 (container . &lt;span style="color: #008000;"&gt;"PRB 76, 014112 (2007)"&lt;/span&gt;)
 (category . &lt;span style="color: #008000;"&gt;"cond-mat.mtrl-sci"&lt;/span&gt;)
 (references &lt;span style="color: #008000;"&gt;"10.1103/PhysRevB.76.014112"&lt;/span&gt; &lt;span style="color: #008000;"&gt;"0704.2752v2"&lt;/span&gt;)
 (type . &lt;span style="color: #008000;"&gt;"eprint"&lt;/span&gt;)
 (url . &lt;span style="color: #008000;"&gt;"http://dx.doi.org/10.1103/PhysRevB.76.014112"&lt;/span&gt;)
 (direct-url . &lt;span style="color: #008000;"&gt;"http://arxiv.org/pdf/0704.2752v2"&lt;/span&gt;))
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Next, we need a database to store the results in. I will just use a flat file database with a file for each record. The filename will be the md5 hash of the doi or the record itself. Why is that a good idea? Well, the doi is a constant, so if it exists the md5 will also be a constant. The doi itself is not a good filename in general, but the md5 is. The md5 of the record itself will be fragile to any changes, so if it has a doi, we should use it. If it doesn't and later gets one, we should see it again since that could mean it has been published. Also, if it changes because of some new version we might want to see it again. In any case, the existence of that file will be evidence we have seen that record before, and will indicate we need to remove it from the current view.
&lt;/p&gt;

&lt;p&gt;
The flat file database is not super inspired. It is modeled a little after elfeed, but other solutions might work better for large sets of records, but this approach will work fine for this post.
&lt;/p&gt;

&lt;p&gt;
Here is a function that returns nil if the record has been seen, and if not, saves the record and returns it.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-emacs-lisp"&gt;(&lt;span style="color: #0000FF;"&gt;defvar&lt;/span&gt; &lt;span style="color: #BA36A5;"&gt;db-dir&lt;/span&gt; &lt;span style="color: #008000;"&gt;"~/.arxiv-db/"&lt;/span&gt;)

(&lt;span style="color: #0000FF;"&gt;unless&lt;/span&gt; (f-dir? db-dir) (make-directory db-dir t))

(&lt;span style="color: #0000FF;"&gt;defun&lt;/span&gt; &lt;span style="color: #006699;"&gt;unseen-record-p&lt;/span&gt; (record)
  &lt;span style="color: #036A07;"&gt;"Given a RECORD return it if it is unseen.&lt;/span&gt;
&lt;span style="color: #036A07;"&gt;Also, save the record so next time it will be marked seen. A&lt;/span&gt;
&lt;span style="color: #036A07;"&gt;record is seen if we have seen the DOI or the record as a string&lt;/span&gt;
&lt;span style="color: #036A07;"&gt;before."&lt;/span&gt;
  (&lt;span style="color: #0000FF;"&gt;let*&lt;/span&gt; ((doi (cdr (assoc 'doi record)))
         (contents (&lt;span style="color: #0000FF;"&gt;with-temp-buffer&lt;/span&gt;
                     (prin1 record (current-buffer))
                     (buffer-string)))
         (hash (md5 (&lt;span style="color: #0000FF;"&gt;or&lt;/span&gt; doi contents)))
         (fname (expand-file-name hash db-dir)))

    (&lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; (f-exists? fname)
        nil
      (&lt;span style="color: #0000FF;"&gt;with-temp-file&lt;/span&gt; fname
        (insert contents))
      record)))
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
unseen-record-p

&lt;/pre&gt;

&lt;p&gt;
Now we can use that as a filter that saves records by side effect.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-emacs-lisp"&gt;(&lt;span style="color: #0000FF;"&gt;defun&lt;/span&gt; &lt;span style="color: #006699;"&gt;scimax-arxiv&lt;/span&gt; (query)
  (&lt;span style="color: #0000FF;"&gt;interactive&lt;/span&gt; &lt;span style="color: #008000;"&gt;"Query: "&lt;/span&gt;)

  (&lt;span style="color: #0000FF;"&gt;let*&lt;/span&gt; ((backend 'biblio-arxiv-backend)
         (cb (url-retrieve-synchronously (funcall backend 'url query)))
         (results (-filter 'unseen-record-p (&lt;span style="color: #0000FF;"&gt;with-current-buffer&lt;/span&gt; cb
                                              (funcall backend 'parse-buffer))))
         (results-buffer (biblio--make-results-buffer (current-buffer) query backend)))
    (&lt;span style="color: #0000FF;"&gt;with-current-buffer&lt;/span&gt; results-buffer
      (biblio-insert-results results &lt;span style="color: #008000;"&gt;""&lt;/span&gt;))
    (pop-to-buffer results-buffer)))

(scimax-arxiv &lt;span style="color: #008000;"&gt;"alloy segregation"&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
#&amp;lt;buffer *arXiv search*&amp;gt;

&lt;/pre&gt;

&lt;p&gt;
Now, when I run that once I see something like this:
&lt;/p&gt;

&lt;p&gt;
&lt;img src="/media/date-11-04-2018-time-20-19-52.png"&gt; 
&lt;/p&gt;


&lt;p&gt;
and if I run it again:
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-emacs-lisp"&gt;(scimax-arxiv &lt;span style="color: #008000;"&gt;"alloy segregation"&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
#&amp;lt;buffer *arXiv search*&amp;gt;

&lt;/pre&gt;

&lt;p&gt;
Then the buffer is empty, since we have seen all the entries before.
&lt;/p&gt;


&lt;p&gt;
&lt;img src="/media/date-11-04-2018-time-20-20-37.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
Here are the files in our database:
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-sh"&gt;ls ~/.arxiv-db/
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Here are the contents of one of those files:
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-emacs-lisp"&gt;(&lt;span style="color: #0000FF;"&gt;with-temp-buffer&lt;/span&gt;
 (insert-file-contents &lt;span style="color: #008000;"&gt;"~/.arxiv-db/18085fe2512e15d66addc7dfb71f7cd2"&lt;/span&gt;)
 (read (buffer-string)))
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
((doi) (identifier . 1101.3464v3) (year . 2011) (title . Characterizing Solute Segregation and Grain Boundary Energy in a Binary
  Alloy Phase Field Crystal Model) (authors nil nil nil nil nil nil nil nil nil nil nil nil nil Jonathan Stolle nil Nikolas Provatas nil nil nil nil nil nil nil nil nil nil nil) (container) (category . cond-mat.mtrl-sci) (references nil 1101.3464v3) (type . eprint) (url . http://arxiv.org/abs/1101.3464v3) (direct-url . http://arxiv.org/pdf/1101.3464v3))

&lt;/pre&gt;

&lt;p&gt;
So, if you need to read this in again later, no problem.
&lt;/p&gt;

&lt;p&gt;
Now, what could go wrong? I don't know much about how the search results from arxiv are returned. For example, this query returns 10 hits.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-emacs-lisp"&gt;(&lt;span style="color: #0000FF;"&gt;let*&lt;/span&gt; ((query &lt;span style="color: #008000;"&gt;"alloy segregration"&lt;/span&gt;)
       (backend 'biblio-arxiv-backend)
       (cb (url-retrieve-synchronously (funcall backend 'url query)))
       (results (&lt;span style="color: #0000FF;"&gt;with-current-buffer&lt;/span&gt; cb
                  (funcall backend 'parse-buffer))))
  (length results))
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
10

&lt;/pre&gt;

&lt;p&gt;
There is just no way there are only 10 hits for this query. So, there must be a bunch more that you get by either changing the requested number in some argument, or by using subsequent queries to get the rest of them. I don't know if there are more advanced query options with biblio, e.g. to find entries newer than the last time it was run. On the advanced search &lt;a href="https://arxiv.org/find"&gt;page&lt;/a&gt; for arxiv, it looks like there is only a by year option.
&lt;/p&gt;

&lt;p&gt;
This is still a good idea, and a lot of the pieces are here,
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2018 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2018/04/11/Caching-searches-using-biblio-and-only-seeing-new-results.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.6&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[Zhitao Guo receives the 2017-2018 James C. Meade Fellowship in Chemical Engineering]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2018/04/09/Zhitao-Guo-receives-the-2017-2018-James-C-Meade-Fellowship-in-Chemical-Engineering" />
    <id>http://jkitchin.github.io/blog/2018/04/09/Zhitao-Guo-receives-the-2017-2018-James-C-Meade-Fellowship-in-Chemical-Engineering</id>
    <updated>2018-04-09T18:18:40Z</updated>
    <published>2018-04-09T18:18:40Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="news" />
    <summary type="html"><![CDATA[Zhitao Guo receives the 2017-2018 James C. Meade Fellowship in Chemical Engineering]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2018/04/09/Zhitao-Guo-receives-the-2017-2018-James-C-Meade-Fellowship-in-Chemical-Engineering"><![CDATA[


&lt;p&gt;
The James C. Meade Fellowship was made possible by a generous donation by James Meade. This will help support Zhitao during his research this year. Zhitao is a first year PhD student who is co-advised by Andy Gellman and myself (John Kitchin), and is working on segregation in ternary alloy thin films.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="/media/date-09-04-2018-time-18-17-59.png"&gt; 
&lt;/p&gt;



&lt;p&gt;
Zhitao joined us from Tsinghua University in Beijing, China, where he studied chemical engineering and double majored in economics.
&lt;/p&gt;

&lt;p&gt;
Congratulations Zhitao!
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2018 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2018/04/09/Zhitao-Guo-receives-the-2017-2018-James-C.-Meade-Fellowship-in-Chemical-Engineering.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.6&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication in Catalysis Today]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2018/04/01/New-publication-in-Catalysis-Today" />
    <id>http://jkitchin.github.io/blog/2018/04/01/New-publication-in-Catalysis-Today</id>
    <updated>2018-04-01T19:07:10Z</updated>
    <published>2018-04-01T19:07:10Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="publication" />
    <category scheme="http://jkitchin.github.io/blog" term="news" />
    <summary type="html"><![CDATA[New publication in Catalysis Today]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2018/04/01/New-publication-in-Catalysis-Today"><![CDATA[


&lt;p&gt;
In this paper we continue our exploration of using high-dimensional neural networks (NN) to model metal surface properties. Our first work started with modeling Au in a variety of structures using ReaxFF and a NN &lt;a class='org-ref-reference' href="#boes-2016-neural-networ"&gt;boes-2016-neural-networ&lt;/a&gt;. We then modeled atomic oxygen adsorbates on a Pd (111) surface &lt;a class='org-ref-reference' href="#boes-2017-neural-networ"&gt;boes-2017-neural-networ&lt;/a&gt;, and segregation of an Au-Pd alloy surface &lt;a class='org-ref-reference' href="#boes-2017-model-segreg"&gt;boes-2017-model-segreg&lt;/a&gt;. Our goal throughout this work has been to systematically build up complexity in the systems we are modeling, and to explore the limitations of these potentials for modeling surfaces. This current work happened in parallel with those works, and focused on modeling Pd adatom diffusion on Pd(111) surfaces. We show another example of how to train a neural network, and then to use it model the temperature dependent diffusion of adatoms on a metal surface using molecular dynamics and Arrhenius analysis.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;gao-2018-model-pallad&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Tianyu Gao and John R. Kitchin},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Modeling Palladium Surfaces With Density Functional Theory,
                  Neural Networks and Molecular Dynamics},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Catalysis Today},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2018,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1016/j.cattod.2018.03.045&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1016/j.cattod.2018.03.045&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Sun Apr 1 18:47:55 2018},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1016/j.cattod.2018.03.045'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;h1 class='org-ref-bib-h1'&gt;Bibliography&lt;/h1&gt;
&lt;ul class='org-ref-bib'&gt;&lt;li&gt;&lt;a id="boes-2016-neural-networ"&gt;[boes-2016-neural-networ]&lt;/a&gt; &lt;a name="boes-2016-neural-networ"&gt;Jacob Boes, Mitchell Groenenboom, John Keith, &amp; John Kitchin, Neural Network and Reaxff Comparison for Au Properties, &lt;i&gt;Int. J. Quantum Chem.&lt;/i&gt;, &lt;b&gt;116(13)&lt;/b&gt;, 979-987 (2016). &lt;a href="http://dx.doi.org/10.1002/qua.25115"&gt;link&lt;/a&gt;. &lt;a href="http://dx.doi.org/10.1002/qua.25115"&gt;doi&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a id="boes-2017-neural-networ"&gt;[boes-2017-neural-networ]&lt;/a&gt; &lt;a name="boes-2017-neural-networ"&gt;Jacob Boes &amp; John Kitchin, Neural Network Predictions of Oxygen Interactions on a Dynamic  Pd Surface, &lt;i&gt;Molecular Simulation&lt;/i&gt;, &lt;b&gt;&lt;/b&gt;, 1-9 (2017). &lt;a href="https://doi.org/10.1080/08927022.2016.1274984"&gt;link&lt;/a&gt;. &lt;a href="http://dx.doi.org/10.1080/08927022.2016.1274984"&gt;doi&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a id="boes-2017-model-segreg"&gt;[boes-2017-model-segreg]&lt;/a&gt; &lt;a name="boes-2017-model-segreg"&gt;Boes &amp; Kitchin, Modeling Segregation on AuPd(111) Surfaces With Density  Functional Theory and Monte Carlo Simulations, &lt;i&gt;The Journal of Physical Chemistry C&lt;/i&gt;, &lt;b&gt;121(6)&lt;/b&gt;, 3479-3487 (2017). &lt;a href="https://doi.org/10.1021/acs.jpcc.6b12752"&gt;link&lt;/a&gt;. &lt;a href="http://dx.doi.org/10.1021/acs.jpcc.6b12752"&gt;doi&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
&lt;p&gt;Copyright (C) 2018 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2018/04/01/New-publication-in-Catalysis-Today.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.6&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication in Topics in Catalysis]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2018/01/27/New-publication-in-Topics-in-Catalysis" />
    <id>http://jkitchin.github.io/blog/2018/01/27/New-publication-in-Topics-in-Catalysis</id>
    <updated>2018-04-01T18:49:28Z</updated>
    <published>2018-01-27T16:39:35Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="publication" />
    <category scheme="http://jkitchin.github.io/blog" term="news" />
    <summary type="html"><![CDATA[New publication in Topics in Catalysis]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2018/01/27/New-publication-in-Topics-in-Catalysis"><![CDATA[


&lt;p&gt;
Single atom alloys are alloys in the extreme dilute limit, where single atoms of a reactive metal are surrounded by comparatively unreactive metals. This makes the single reactive atoms like single atom sites where reactions can occur. These sites are interesting because they are metallic, but their electronic structure is different than the atoms in more concentrated alloys. This means there is the opportunity for different, perhaps better catalytic performance for the single atom alloys. In this paper, we studied the electronic structure and some representative reaction pathways on a series of single atom alloy surfaces.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;Thirumalai2018&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       "Thirumalai, Hari and Kitchin, John R.",
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        "Investigating the Reactivity of Single Atom Alloys Using
                  Density Functional Theory",
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      "Topics in Catalysis",
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         "2018",
  &lt;span style="color: #BA36A5;"&gt;month&lt;/span&gt; =        "Jan",
  &lt;span style="color: #BA36A5;"&gt;day&lt;/span&gt; =          "25",
  &lt;span style="color: #BA36A5;"&gt;abstract&lt;/span&gt; =     "Single atom alloys are gaining importance as atom-efficient
                  catalysts which can be extremely selective and active towards
                  the formation of desired products. They possess such desirable
                  characteristics because of the presence of a highly reactive
                  single atom in a less reactive host surface. In this work, we
                  calculated the electronic structure of several representative
                  single atom alloys. We examined single atom alloys of gold,
                  silver and copper doped with single atoms of platinum,
                  palladium, iridium, rhodium and nickel in the context of the
                  d-band model of Hammer and N{\o}rskov. The reactivity of these
                  alloys was probed through the dissociation of water and nitric
                  oxide and the hydrogenation of acetylene to ethylene. We
                  observed that these alloys exhibit a sharp peak in their atom
                  projected d-band density of states, which we hypothesize could
                  be the cause of high surface reactivity. We found that the
                  d-band centers and d-band widths of these systems correlated
                  linearly as with other alloys, but that the energy of
                  adsorption of a hydrogen atom on these surfaces could not be
                  correlated with the d-band center, or the average reactivity
                  of the surface. Finally, the single atom alloys, with the
                  exception of copper--palladium showed good catalytic behavior
                  by activating the reactant molecules more strongly than the
                  bulk atom behavior and showing favorable reaction pathways on
                  the free energy diagrams for the reactions investigated.",
  &lt;span style="color: #BA36A5;"&gt;issn&lt;/span&gt; =         "1572-9028",
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          "&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1007/s11244-018-0899-0&lt;/span&gt;",
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          "&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1007/s11244-018-0899-0&lt;/span&gt;"
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1007/s11244-018-0899-0'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2018 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2018/01/27/New-publication-in-Topics-in-Catalysis.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.6&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication in Molecular Simulation]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2018/01/03/New-publication-in-Molecular-Simulation" />
    <id>http://jkitchin.github.io/blog/2018/01/03/New-publication-in-Molecular-Simulation</id>
    <updated>2018-01-03T06:28:21Z</updated>
    <published>2018-01-03T06:28:21Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="publication" />
    <category scheme="http://jkitchin.github.io/blog" term="news" />
    <summary type="html"><![CDATA[New publication in Molecular Simulation]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2018/01/03/New-publication-in-Molecular-Simulation"><![CDATA[


&lt;p&gt;
This paper is our latest work using neural networks in molecular simulation. In this work, we build a Behler-Parinello neural network potential of bulk zirconia. The potential can describe several polymorphs of zirconia, as well as oxygen vacancy defect formation energies and diffusion barriers. We show that we can use the potential to model oxygen vacancy diffusion using molecular dynamics at different temperatures, and to use that data to estimate the effective diffusion activation energy. This is further evidence of the general utility of the neural network-based potential for molecular simulations with DFT accuracy.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;wang-2018-densit-funct&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Chen Wang and Akshay Tharval and John R. Kitchin},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {A Density Functional Theory Parameterised Neural Network Model
                  of Zirconia},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Molecular Simulation},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       0,
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       0,
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {1-8},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2018,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1080/08927022.2017.1420185&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1080/08927022.2017.1420185&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;eprint&lt;/span&gt; =       { https://doi.org/10.1080/08927022.2017.1420185 },
  &lt;span style="color: #BA36A5;"&gt;publisher&lt;/span&gt; =    {Taylor \&amp;amp; Francis},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1080/08927022.2017.1420185'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2018 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2018/01/03/New-publication-in-Molecular-Simulation.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.5&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[2017 in a nutshell for the Kitchin Research group]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2017/12/31/2017-in-a-nutshell-for-the-Kitchin-Research-group" />
    <id>http://jkitchin.github.io/blog/2017/12/31/2017-in-a-nutshell-for-the-Kitchin-Research-group</id>
    <updated>2017-12-31T13:21:59Z</updated>
    <published>2017-12-31T13:21:58Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="news" />
    <summary type="html"><![CDATA[2017 in a nutshell for the Kitchin Research group]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2017/12/31/2017-in-a-nutshell-for-the-Kitchin-Research-group"><![CDATA[


&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#orga22af1f"&gt;1. Student accomplishments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org1a1a81e"&gt;2. Publications&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#org372deac"&gt;2.1. Collaborative papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org59c38bf"&gt;2.2. Papers on neural networks in molecular simulation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org44aee74"&gt;2.3. Papers accepted in 2017 but not yet in press&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#org79f134f"&gt;3. New courses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org6bb6401"&gt;4. Sabbatical at Google&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org29808b7"&gt;5. Emacs and org-mode&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org16e2c7b"&gt;6. Social media&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#orgd9b416e"&gt;6.1. kitchingroup.cheme.cmu.edu&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orga302874"&gt;6.2. Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org8c173b0"&gt;6.3. Youtube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
Since the &lt;a href="http://kitchingroup.cheme.cmu.edu/blog/2017/01/01/2016-in-a-nutshell-for-the-Kitchin-Research-group/"&gt;last update&lt;/a&gt; a lot of new things have happened in the Kitchin Research group. Below are some summaries of the group accomplishments, publications and activities for the past year. 
&lt;/p&gt;

&lt;div id="outline-container-orga22af1f" class="outline-2"&gt;
&lt;h2 id="orga22af1f"&gt;&lt;span class="section-number-2"&gt;1&lt;/span&gt; Student accomplishments&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
Jacob Boes completed his PhD and began postdoctoral work with Thomas Bligaard at SLAC/Suncat at Stanford. Congratulations Jake!
&lt;/p&gt;

&lt;p&gt;
Four new PhD students joined the group:
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Jenny Zhan will work on simulation of molten superalloys&lt;/li&gt;
&lt;li&gt;Mingjie Liu will work on the design of single atom alloy catalysts&lt;/li&gt;
&lt;li&gt;Yilin Yang will work on segregation in multicomponent alloys under reaction conditions&lt;/li&gt;
&lt;li&gt;Zhitao Guo is also joining the group and will be co-advised by Prof. Gellman. He will work on multicomponent alloy catalysts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
Welcome to the group!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org1a1a81e" class="outline-2"&gt;
&lt;h2 id="org1a1a81e"&gt;&lt;span class="section-number-2"&gt;2&lt;/span&gt; Publications&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
Our publications and citation counts have continued to grow this year. Here is our current metrics according to &lt;a href="http://www.researcherid.com/rid/A-2363-2010"&gt;Researcher ID&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
&lt;img src="/media/date-30-12-2017-time-20-18-45.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
We have eight new papers that are online, and two that are accepted, but not online yet. There are brief descriptions below.
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-org372deac" class="outline-3"&gt;
&lt;h3 id="org372deac"&gt;&lt;span class="section-number-3"&gt;2.1&lt;/span&gt; Collaborative papers&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-2-1"&gt;
&lt;dl class="org-dl"&gt;
&lt;dt&gt;&lt;a class='org-ref-reference' href="#larsen-2017-atomic-simul"&gt;larsen-2017-atomic-simul&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;This is a modern update on the Atomic Simulation Environment Python software. We have been using and contributing to this software for about 15 years now!&lt;/dd&gt;

&lt;dt&gt;&lt;a class='org-ref-reference' href="#saravanan-2017-alchem-predic"&gt;saravanan-2017-alchem-predic&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;This collaborative effort with the Keith group at UPitt and Anatole von Lilienfeld explored a novel approach to estimating adsorption energies on alloy surfaces.&lt;/dd&gt;

&lt;dt&gt;&lt;a class='org-ref-reference' href="#xu-2017-first-princ"&gt;xu-2017-first-princ&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;We used DFT calculations to understand epitaxial stabilization of titania films on strontium titanate surfaces.&lt;/dd&gt;

&lt;dt&gt;&lt;a class='org-ref-reference' href="#wittkamper-2017-compet-growt"&gt;wittkamper-2017-compet-growt&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;We previously predicted that tin oxide should be able to form in the columbite phase as an epitaxial film. In this paper our collaborators show that it can be done!&lt;/dd&gt;

&lt;dt&gt;&lt;a class='org-ref-reference' href="#kitchin-2017-autom-data"&gt;kitchin-2017-autom-data&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;This paper finally came out in print. It shows an automated approach to sharing data. Also, it may be the only paper with data hidden inside a picture of a library in the literature.&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org59c38bf" class="outline-3"&gt;
&lt;h3 id="org59c38bf"&gt;&lt;span class="section-number-3"&gt;2.2&lt;/span&gt; Papers on neural networks in molecular simulation&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-2-2"&gt;
&lt;dl class="org-dl"&gt;
&lt;dt&gt;&lt;a class='org-ref-reference' href="#boes-2017-neural-networ"&gt;boes-2017-neural-networ&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;We used neural networks in conjunction with molecular dynamics and Monte Carlo simulations to model the coverage dependent adsorption of oxygen and initial oxidation of a Pd(111) surface.&lt;/dd&gt;

&lt;dt&gt;&lt;a class='org-ref-reference' href="#boes-2017-model-segreg"&gt;boes-2017-model-segreg&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;We used neural networks in conjunction with Monte Carlo simulations to model segregation across composition space for a Au-Pd alloy.&lt;/dd&gt;

&lt;dt&gt;&lt;a class='org-ref-reference' href="#geng-2017-first-princ"&gt;geng-2017-first-princ&lt;/a&gt;&lt;/dt&gt;&lt;dd&gt;We used a cluster expansion with Monte Carlo simulations to resolve some inconsistencies in simulated Cu-Pd phase diagrams. There is an interesting transition from an fcc to bcc to fcc structure across the composition space that is subtle and difficult to compute.&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org44aee74" class="outline-3"&gt;
&lt;h3 id="org44aee74"&gt;&lt;span class="section-number-3"&gt;2.3&lt;/span&gt; Papers accepted in 2017 but not yet in press&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-2-3"&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;Chen Wang, Akshay Tharval, John R. Kitchin, A density functional theory parameterized neural network model of zirconia, Accepted in Molecular Simulation, July 2017.&lt;/li&gt;

&lt;li&gt;Hari Thirumalai, John R. Kitchin, Investigating the Reactivity of Single Atom Alloys using Density Functional Theory, Topics in Catalysis, Accepted November 2017.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-org79f134f" class="outline-2"&gt;
&lt;h2 id="org79f134f"&gt;&lt;span class="section-number-2"&gt;3&lt;/span&gt; New courses&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
After a five year stint of teaching Master's and PhD courses, I taught the undergraduate chemical engineering course again. This was the first time I taught the course using Python. All the lectures and assignments were in Jupyter notebooks. You can find the course here: &lt;a href="https://github.com/jkitchin/s17-06364"&gt;https://github.com/jkitchin/s17-06364&lt;/a&gt;. The whole class basically ran from a browser using a Python Flask app to serve the syllabus, lectures and assignments. Assignments were submitted and returned by email through the Flask app. It was pretty interesting. I did not like it as much as using Emacs/org-mode like I have in the past, but it was easier to get 70 undergraduates up and running.
&lt;/p&gt;

&lt;p&gt;
I did not teach in the Fall, because I was on Sabbatical!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org6bb6401" class="outline-2"&gt;
&lt;h2 id="org6bb6401"&gt;&lt;span class="section-number-2"&gt;4&lt;/span&gt; Sabbatical at Google&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
In August 2017 I started my first sabbatical! I am spending a year in the &lt;a href="https://research.google.com/teams/gas/"&gt;Accelerated Science&lt;/a&gt; group at Google in Mountain View, California. I am learning about machine learning applications in engineering and science. This is a pivotal year in my research program, so stay tuned for our new work!
&lt;/p&gt;

&lt;p&gt;
It has been great for my family, who moved out here with me. We have been seeing a lot of California. I have been biking to work almost every day, usually 15-20 miles. I have logged over 1200 commuting miles already since August.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org29808b7" class="outline-2"&gt;
&lt;h2 id="org29808b7"&gt;&lt;span class="section-number-2"&gt;5&lt;/span&gt; Emacs and org-mode&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
org-ref remains in the top 15% of downloaded &lt;a href="https://melpa.org/#/org-ref"&gt;MELPA&lt;/a&gt; packages, with more than 24,000 downloads since it was released. It has been pretty stable lately. It remains a cornerstone of my technical writing toolbox.
&lt;/p&gt;

&lt;p&gt;
I have spent some time improving org-mode/ipython interactions including inline images, asynchronous execution and export to jupyter notebooks. It is still a work in progress.
&lt;/p&gt;

&lt;p&gt;
I spent a fair bit of time learning about dynamic modules for writing compiled extensions to Emacs to bring features like linear algebra, numerical methods and database access to it. I wish I had more time to work on this. I think it will be useful to make org-mode even better for  scientific research and documentation. 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org16e2c7b" class="outline-2"&gt;
&lt;h2 id="org16e2c7b"&gt;&lt;span class="section-number-2"&gt;6&lt;/span&gt; Social media&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-6"&gt;
&lt;p&gt;
I have continued exploring the use of social media to share my work. It still seems like a worthwhile use of time, but we need continued efforts to make this really useful for science. 
&lt;/p&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgd9b416e" class="outline-3"&gt;
&lt;h3 id="orgd9b416e"&gt;&lt;span class="section-number-3"&gt;6.1&lt;/span&gt; kitchingroup.cheme.cmu.edu&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-6-1"&gt;
&lt;p&gt;
I use my blog to share technical knowledge and news about the group. We had 48 blog posts in 2017. A lot of them were on some use of org-mode and Emacs.  I also introduced a new exporter for org-mode to make jupyter notebooks. I spent November exploring automatic differentiation and applications of it to engineering problems. Visits to the site continue to grow. Here is the growth over the past two years. The big spike in Oct 2017 is from this &lt;a href="https://news.ycombinator.com/item?id=15464340"&gt;article on Hacker News&lt;/a&gt; about one of my posts!
&lt;/p&gt;


&lt;p&gt;
&lt;img src="/media/date-30-12-2017-time-20-15-28.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
I continue to think that technical blogging is a valuable way to communicate technical knowledge. It provides an easy way to practice writing, and with comments enabled to get feedback on your ideas. It has taken several years to develop a style for doing this effectively that is useful to me, and to others. I have integrated my blog into Twitter so that new posts are automatically tweeted, which helps publicize the new posts.
&lt;/p&gt;

&lt;p&gt;
It has some limitations, e.g. it is not obvious how to cite them in ways that are compatible with the current bibliometric driven assessment tools used in promotion and tenure. Overall, I find it very complementary to formal publications though, and I wish more people did it.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orga302874" class="outline-3"&gt;
&lt;h3 id="orga302874"&gt;&lt;span class="section-number-3"&gt;6.2&lt;/span&gt; Github&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-6-2"&gt;
&lt;p&gt;
I was a little less active on &lt;a href="https://github.com/jkitchin"&gt;Github&lt;/a&gt; this year than last year, especially this fall as I started my sabbatical. Github remains my goto version control service though, and we continue using it for everything from code development and paper writing to course serving.
&lt;/p&gt;


&lt;p&gt;
&lt;img src="/media/date-30-12-2017-time-20-12-03.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
scimax finally has more Github stars than jmax does!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org8c173b0" class="outline-3"&gt;
&lt;h3 id="org8c173b0"&gt;&lt;span class="section-number-3"&gt;6.3&lt;/span&gt; Youtube&lt;/h3&gt;
&lt;div class="outline-text-3" id="text-6-3"&gt;
&lt;p&gt;
Another year with over 100,000 minutes of &lt;a href="https://www.youtube.com/analytics?o=U#dt=ty,fe=17165,fr=lw-001,fs=16801;fc=0,fcr=0,r=views,rpg=93"&gt;Youtube watch time&lt;/a&gt; on our videos. &lt;a href="https://www.youtube.com/watch?v=fgizHHd7nOo"&gt;org-mode is awesome&lt;/a&gt; was most popular, with almost 50,000 views. We have six videos with over 2500 views for the past year!
&lt;/p&gt;



&lt;p&gt;
&lt;img src="/media/date-31-12-2017-time-13-08-54.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
I have not made too many new videos this year. Hopefully there will be some new ones on the new features in scimax in the next year.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copyright (C) 2017 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2017/12/31/2017-in-a-nutshell-for-the-Kitchin-Research-group.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.3&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[Solving an eigenvalue differential equation with a neural network]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2017/11/29/Solving-an-eigenvalue-differential-equation-with-a-neural-network" />
    <id>http://jkitchin.github.io/blog/2017/11/29/Solving-an-eigenvalue-differential-equation-with-a-neural-network</id>
    <updated>2017-11-29T21:20:07Z</updated>
    <published>2017-11-29T21:17:03Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="autograd" />
    <category scheme="http://jkitchin.github.io/blog" term="eigenvalue" />
    <category scheme="http://jkitchin.github.io/blog" term="bvp" />
    <summary type="html"><![CDATA[Solving an eigenvalue differential equation with a neural network]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2017/11/29/Solving-an-eigenvalue-differential-equation-with-a-neural-network"><![CDATA[


&lt;div id="table-of-contents"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#orgdd6adc2"&gt;1. The neural network setup&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org08ec74a"&gt;2. The objective function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org865c7c4"&gt;3. The minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org31faf55"&gt;4. The first excited state&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org3e74a47"&gt;5. Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
The 1D harmonic oscillator is described &lt;a href="https://quantummechanics.ucsd.edu/ph130a/130_notes/node153.html"&gt;here&lt;/a&gt;. It is a boundary value differential equation with eigenvalues. If we let let &amp;omega;=1, m=1, and units where &amp;hbar;=1. then, the governing differential equation becomes:
&lt;/p&gt;

&lt;p&gt;
\(-0.5 \frac{d^2\psi(x)}{dx^2} + (0.5 x^2 - E) \psi(x) = 0\)
&lt;/p&gt;

&lt;p&gt;
with boundary conditions: \(\psi(-\infty) = \psi(\infty) = 0\)
&lt;/p&gt;

&lt;p&gt;
We can further stipulate that the probability of finding the particle over this domain is equal to one: \(\int_{-\infty}^{\infty} \psi^2(x) dx = 1\). In this set of equations, \(E\) is an eigenvalue, which means there are only non-trivial solutions for certain values of \(E\).
&lt;/p&gt;

&lt;p&gt;
Our goal is to solve this equation using a neural network to represent the wave function. This is a different problem than the one &lt;a href="http://kitchingroup.cheme.cmu.edu/blog/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd/"&gt;here&lt;/a&gt; or &lt;a href="http://kitchingroup.cheme.cmu.edu/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd/"&gt;here&lt;/a&gt; because of the eigenvalue. This is an additional adjustable parameter we have to find. Also, we have the normalization constraint to consider, which we did not consider before.
&lt;/p&gt;

&lt;div id="outline-container-orgdd6adc2" class="outline-2"&gt;
&lt;h2 id="orgdd6adc2"&gt;&lt;span class="section-number-2"&gt;1&lt;/span&gt; The neural network setup&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
Here we setup the neural network and its derivatives. This is the same as we did before.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="orge0a59c3"&gt;&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; autograd.numpy &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; np
&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; autograd &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; grad, elementwise_grad
&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; autograd.numpy.random &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; npr
&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; autograd.misc.optimizers &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; adam

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;init_random_params&lt;/span&gt;(scale, layer_sizes, rs=npr.RandomState(42)):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"""Build a list of (weights, biases) tuples, one for each layer."""&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; [(rs.randn(insize, outsize) * scale,   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;weight matrix&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;rs.randn(outsize) * scale)           &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;bias vector&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;for&lt;/span&gt; insize, outsize &lt;span style="color: #0000FF;"&gt;in&lt;/span&gt; &lt;span style="color: #006FE0;"&gt;zip&lt;/span&gt;(layer_sizes[:-1], layer_sizes[1:])]

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;swish&lt;/span&gt;(x):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"see https://arxiv.org/pdf/1710.05941.pdf"&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x / (1.0 + np.exp(-x))

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;psi&lt;/span&gt;(nnparams, inputs):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"Neural network wavefunction"&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;for&lt;/span&gt; W, b &lt;span style="color: #0000FF;"&gt;in&lt;/span&gt; nnparams:
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;outputs&lt;/span&gt; = np.dot(inputs, W) + b
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;inputs&lt;/span&gt; = swish(outputs)    
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; outputs

&lt;span style="color: #BA36A5;"&gt;psip&lt;/span&gt; = elementwise_grad(psi, 1) &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;dpsi/dx &lt;/span&gt;
&lt;span style="color: #BA36A5;"&gt;psipp&lt;/span&gt; = elementwise_grad(psip, 1) &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;d^2psi/dx^2&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org08ec74a" class="outline-2"&gt;
&lt;h2 id="org08ec74a"&gt;&lt;span class="section-number-2"&gt;2&lt;/span&gt; The objective function&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
The important function we need is the objective function. This function codes the Schrödinger equation, the boundary conditions, and the normalization as a cost function that we will later seek to minimize. Ideally, at the solution the objective function will be zero. We can't put infinity into our objective function, but it turns out that x = &amp;plusmn; 6 is practically infinity in this case, so we approximate the boundary conditions there. 
&lt;/p&gt;

&lt;p&gt;
Another note is the numerical integration by the trapezoid rule. I use a vectorized version of this because autograd doesn't have a trapz derivative and I didn't feel like figuring one out.
&lt;/p&gt;

&lt;p&gt;
We define the params to vary here as a dictionary containing neural network weights and biases, and the value of the eigenvalue.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="orge9e096c"&gt;&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;Here is our initial guess of params:&lt;/span&gt;
&lt;span style="color: #BA36A5;"&gt;nnparams&lt;/span&gt; = init_random_params(0.1, layer_sizes=[1, 8, 1])

&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = {&lt;span style="color: #008000;"&gt;'nn'&lt;/span&gt;: nnparams, &lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;: 0.4}

&lt;span style="color: #BA36A5;"&gt;x&lt;/span&gt; = np.linspace(-6, 6, 200)[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;objective&lt;/span&gt;(params, step):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;nnparams&lt;/span&gt; = params[&lt;span style="color: #008000;"&gt;'nn'&lt;/span&gt;]
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;E&lt;/span&gt; = params[&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;]        
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;This is Schrodinger's eqn&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;zeq&lt;/span&gt; = -0.5 * psipp(nnparams, x) + (0.5 * x**2 - E) * psi(nnparams, x) 
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;bc0&lt;/span&gt; = psi(nnparams, -6.0) &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;This approximates -infinity&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;bc1&lt;/span&gt; = psi(nnparams, 6.0)  &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;This approximates +infinity&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;y2&lt;/span&gt; = psi(nnparams, x)**2
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;This is a numerical trapezoid integration&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;prob&lt;/span&gt; = np.&lt;span style="color: #006FE0;"&gt;sum&lt;/span&gt;((y2[1:] + y2[0:-1]) / 2 * (x[1:] - x[0:-1]))
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; np.mean(zeq**2) + bc0**2 + bc1**2 + (1.0 - prob)**2

&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;This gives us feedback from the optimizer&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;callback&lt;/span&gt;(params, step, g):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; step % 1000 == 0:
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;"Iteration {0:3d} objective {1}"&lt;/span&gt;.&lt;span style="color: #006FE0;"&gt;format&lt;/span&gt;(step,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; objective(params, step)))
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org865c7c4" class="outline-2"&gt;
&lt;h2 id="org865c7c4"&gt;&lt;span class="section-number-2"&gt;3&lt;/span&gt; The minimization&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
Now, we just let an optimizer minimize the objective function for us. Note, I ran this next block more than once, as the objective continued to decrease. I ran this one at least two times, and the loss was still decreasing slowly.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="orgf241c39"&gt;&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = adam(grad(objective), params,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; step_size=0.001, num_iters=5001, callback=callback) 

&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(params[&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;])
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
Iteration   0 objective [[ 0.00330204]]
Iteration 1000 objective [[ 0.00246459]]
Iteration 2000 objective [[ 0.00169862]]
Iteration 3000 objective [[ 0.00131453]]
Iteration 4000 objective [[ 0.00113132]]
Iteration 5000 objective [[ 0.00104405]]
0.5029457355415167

&lt;/pre&gt;

&lt;p&gt;
Good news, the lowest energy eigenvalue is known to be 0.5 for our choice of parameters, and that is approximately what we got. Now let's see our solution and compare it to the known solution. Interestingly we got the negative of the solution, which is still a solution. The NN solution is not indistinguishable from the analytical solution, and has some spurious curvature in the tails, but it is approximately correct, and more training might get it closer. A different activation function might also work better.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="orgc343304"&gt;%matplotlib inline
&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; plt

&lt;span style="color: #BA36A5;"&gt;x&lt;/span&gt; = np.linspace(-6, 6)[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]
&lt;span style="color: #BA36A5;"&gt;y&lt;/span&gt; = psi(params[&lt;span style="color: #008000;"&gt;'nn'&lt;/span&gt;], x)

plt.plot(x, -y, label=&lt;span style="color: #008000;"&gt;'NN'&lt;/span&gt;)
plt.plot(x, (1/np.pi)**0.25 * np.exp(-x**2 / 2), &lt;span style="color: #008000;"&gt;'r--'&lt;/span&gt;, label=&lt;span style="color: #008000;"&gt;'analytical'&lt;/span&gt;)
plt.legend()
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;img src="/media/ob-ipython-a0315846d401b5468d391df4b1ee6e84.png"&gt; 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org31faf55" class="outline-2"&gt;
&lt;h2 id="org31faf55"&gt;&lt;span class="section-number-2"&gt;4&lt;/span&gt; The first excited state&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
Now, what about the first excited state? This has an eigenvalue of 1.5, and the solution has odd parity. We can naively change the eigenvalue, and hope that the optimizer will find the right new solution. We do that here, and use the old NN params.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="org78762bc"&gt;&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt;[&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;] = 1.6
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Now, we run a round of optimization:
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="org77ad283"&gt;&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = adam(grad(objective), params,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; step_size=0.003, num_iters=5001, callback=callback) 

&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(params[&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;])
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
Iteration   0 objective [[ 0.09918192]]
Iteration 1000 objective [[ 0.00102333]]
Iteration 2000 objective [[ 0.00100269]]
Iteration 3000 objective [[ 0.00098684]]
Iteration 4000 objective [[ 0.00097425]]
Iteration 5000 objective [[ 0.00096347]]
0.502326347406645

&lt;/pre&gt;


&lt;p&gt;
That doesn't work though. The optimizer just pushes the solution back to the known one. Next, we try starting from scratch with the eigenvalue guess.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="org41c431c"&gt;&lt;span style="color: #BA36A5;"&gt;nnparams&lt;/span&gt; = init_random_params(0.1, layer_sizes=[1, 8, 1])

&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = {&lt;span style="color: #008000;"&gt;'nn'&lt;/span&gt;: nnparams, &lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;: 1.6}

&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = adam(grad(objective), params,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; step_size=0.003, num_iters=5001, callback=callback) 

&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(params[&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;])
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
Iteration   0 objective [[ 2.08318762]]
Iteration 1000 objective [[ 0.02358685]]
Iteration 2000 objective [[ 0.00726497]]
Iteration 3000 objective [[ 0.00336433]]
Iteration 4000 objective [[ 0.00229851]]
Iteration 5000 objective [[ 0.00190942]]
0.5066213334684926

&lt;/pre&gt;

&lt;p&gt;
That also doesn't work. We are going to have to steer this. The idea is pre-train the neural network to have the basic shape and symmetry we want, and then use that as the input for the objective function. The first excited state has odd parity, and here is a guess of that shape. This is a pretty ugly hacked up version that only roughly has the right shape. I am counting on the NN smoothing out the discontinuities.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="org586b931"&gt;&lt;span style="color: #BA36A5;"&gt;xm&lt;/span&gt; = np.linspace(-6, 6)[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]
&lt;span style="color: #BA36A5;"&gt;ym&lt;/span&gt; = -0.5 * ((-1 * (xm + 1.5)**2) + 1.5) * (xm &amp;lt; 0) * (xm &amp;gt; -3)
&lt;span style="color: #BA36A5;"&gt;yp&lt;/span&gt; = -0.5 * ((1 * (xm - 1.5)**2 ) - 1.5) * (xm &amp;gt; 0) * (xm &amp;lt; 3)

plt.plot(xm, (ym + yp))
plt.plot(x, (1/np.pi)**0.25 * np.sqrt(2) * x * np.exp(-x**2 / 2), &lt;span style="color: #008000;"&gt;'r--'&lt;/span&gt;, label=&lt;span style="color: #008000;"&gt;'analytical'&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;img src="/media/ob-ipython-7306bb4c2a75d356dd2246681bec193e.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
Now we pretrain a bit.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="orgea1c301"&gt;&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;pretrain&lt;/span&gt;(params, step):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;nnparams&lt;/span&gt; = params[&lt;span style="color: #008000;"&gt;'nn'&lt;/span&gt;]
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;errs&lt;/span&gt; = psi(nnparams, xm) - (ym + yp)
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; np.mean(errs**2)

&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = adam(grad(pretrain), params,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; step_size=0.003, num_iters=501, callback=callback) 
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
Iteration   0 objective [[ 1.09283695]]

&lt;/pre&gt;

&lt;p&gt;
Here is the new initial guess we are going to use. You can see that indeed a lot of smoothing has occurred.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="org9df043e"&gt;plt.plot(xm, ym + yp, xm, psi(params[&lt;span style="color: #008000;"&gt;'nn'&lt;/span&gt;], xm))
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;img src="/media/ob-ipython-861dc15ae81c1a9d2bcab2aeca1c7b64.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
That has the right shape now. So we go back to the original objective function. 
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="org5298900"&gt;&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = adam(grad(objective), params,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; step_size=0.001, num_iters=5001, callback=callback) 

&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(params[&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;])
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
Iteration   0 objective [[ 0.00370029]]
Iteration 1000 objective [[ 0.00358193]]
Iteration 2000 objective [[ 0.00345137]]
Iteration 3000 objective [[ 0.00333]]
Iteration 4000 objective [[ 0.0032198]]
Iteration 5000 objective [[ 0.00311844]]
1.5065724128094344

&lt;/pre&gt;

&lt;p&gt;
I ran that optimization block many times. The loss is still decreasing, but slowly. More importantly, the eigenvalue is converging to 1.5, which is the known analytical value, and the solution is converging to the known solution. 
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="org0d766e2"&gt;&lt;span style="color: #BA36A5;"&gt;x&lt;/span&gt; = np.linspace(-6, 6)[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]
&lt;span style="color: #BA36A5;"&gt;y&lt;/span&gt; = psi(params[&lt;span style="color: #008000;"&gt;'nn'&lt;/span&gt;], x)

plt.plot(x, y, label=&lt;span style="color: #008000;"&gt;'NN'&lt;/span&gt;)
plt.plot(x, (1/np.pi)**0.25 * np.sqrt(2) * x * np.exp(-x**2 / 2), &lt;span style="color: #008000;"&gt;'r--'&lt;/span&gt;, label=&lt;span style="color: #008000;"&gt;'analytical'&lt;/span&gt;)
plt.legend()
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;img src="/media/ob-ipython-e63e275d2112849010d3e28381ccf41b.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
We can confirm the normalization is reasonable:
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-ipython" id="org6eef549"&gt;&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;check the normalization&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(np.trapz(y.T * y.T, x.T))
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
[ 0.99781886]

&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3e74a47" class="outline-2"&gt;
&lt;h2 id="org3e74a47"&gt;&lt;span class="section-number-2"&gt;5&lt;/span&gt; Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
This is another example of using autograd to solve an eigenvalue differential equation. Some of these solutions required tens of thousands of iterations of training. The groundstate wavefunction was very easy to get. The first excited state, on the other hand, took some active steering. This is very much like how an initial guess can change which solution a nonlinear optimization (which this is) finds.
&lt;/p&gt;

&lt;p&gt;
There are other ways to solve this particular problem. What I think is interesting about this is the possibility to solve harder problems, e.g. not just a harmonic potential, but a more complex one. You could pretrain a network on the harmonic solution, and then use it as the initial guess for the harder problem (which has no analytical solution). 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copyright (C) 2017 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2017/11/29/Solving-an-eigenvalue-differential-equation-with-a-neural-network.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.2&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[Solving ODEs with a neural network and autograd]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd" />
    <id>http://jkitchin.github.io/blog/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd</id>
    <updated>2017-11-28T07:23:59Z</updated>
    <published>2017-11-28T07:23:03Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="autograd" />
    <category scheme="http://jkitchin.github.io/blog" term="ode" />
    <summary type="html"><![CDATA[Solving ODEs with a neural network and autograd]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd"><![CDATA[


&lt;p&gt;
In the last &lt;a href="http://kitchingroup.cheme.cmu.edu/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd/"&gt;post&lt;/a&gt; I explored using a neural network to solve a BVP. Here, I expand the idea to solving an initial value ordinary differential equation. The idea is basically the same, we just have a slightly different objective function.
&lt;/p&gt;

&lt;p&gt;
\(dCa/dt = -k Ca(t)\) where \(Ca(t=0) = 2.0\).
&lt;/p&gt;

&lt;p&gt;
Here is the code that solves this equation, along with a comparison to the analytical solution: \(Ca(t) = Ca0 \exp -kt\).
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-python"&gt;&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; autograd.numpy &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; np
&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; autograd &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; grad, elementwise_grad
&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; autograd.numpy.random &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; npr
&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; autograd.misc.optimizers &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; adam

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;init_random_params&lt;/span&gt;(scale, layer_sizes, rs=npr.RandomState(0)):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"""Build a list of (weights, biases) tuples, one for each layer."""&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; [(rs.randn(insize, outsize) * scale,   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;weight matrix&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;rs.randn(outsize) * scale)           &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;bias vector&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;for&lt;/span&gt; insize, outsize &lt;span style="color: #0000FF;"&gt;in&lt;/span&gt; &lt;span style="color: #006FE0;"&gt;zip&lt;/span&gt;(layer_sizes[:-1], layer_sizes[1:])]


&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;swish&lt;/span&gt;(x):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"see https://arxiv.org/pdf/1710.05941.pdf"&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x / (1.0 + np.exp(-x))


&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;Ca&lt;/span&gt;(params, inputs):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"Neural network functions"&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;for&lt;/span&gt; W, b &lt;span style="color: #0000FF;"&gt;in&lt;/span&gt; params:
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;outputs&lt;/span&gt; = np.dot(inputs, W) + b
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;inputs&lt;/span&gt; = swish(outputs)    
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; outputs

&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   
&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;Here is our initial guess of params:&lt;/span&gt;
&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = init_random_params(0.1, layer_sizes=[1, 8, 1])

&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;Derivatives&lt;/span&gt;
&lt;span style="color: #BA36A5;"&gt;dCadt&lt;/span&gt; = elementwise_grad(Ca, 1)

&lt;span style="color: #BA36A5;"&gt;k&lt;/span&gt; = 0.23
&lt;span style="color: #BA36A5;"&gt;Ca0&lt;/span&gt; = 2.0
&lt;span style="color: #BA36A5;"&gt;t&lt;/span&gt; = np.linspace(0, 10).reshape((-1, 1))

&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;This is the function we seek to minimize&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;objective&lt;/span&gt;(params, step):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;These should all be zero at the solution&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;dCadt = -k * Ca(t)&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;zeq&lt;/span&gt; = dCadt(params, t) - (-k * Ca(params, t))
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;ic&lt;/span&gt; = Ca(params, 0) - Ca0
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; np.mean(zeq**2) + ic**2

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;callback&lt;/span&gt;(params, step, g):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; step % 1000 == 0:
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;"Iteration {0:3d} objective {1}"&lt;/span&gt;.&lt;span style="color: #006FE0;"&gt;format&lt;/span&gt;(step,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; objective(params, step)))

&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = adam(grad(objective), params,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; step_size=0.001, num_iters=5001, callback=callback) 


&lt;span style="color: #BA36A5;"&gt;tfit&lt;/span&gt; = np.linspace(0, 20).reshape(-1, 1)
&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; plt
plt.plot(tfit, Ca(params, tfit), label=&lt;span style="color: #008000;"&gt;'soln'&lt;/span&gt;)
plt.plot(tfit, Ca0 * np.exp(-k * tfit), &lt;span style="color: #008000;"&gt;'r--'&lt;/span&gt;, label=&lt;span style="color: #008000;"&gt;'analytical soln'&lt;/span&gt;)
plt.legend()
plt.xlabel(&lt;span style="color: #008000;"&gt;'time'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'$C_A$'&lt;/span&gt;)
plt.xlim([0, 20])
plt.savefig(&lt;span style="color: #008000;"&gt;'nn-ode.png'&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
Iteration   0 objective [[ 3.20374053]]
Iteration 1000 objective [[  3.13906829e-05]]
Iteration 2000 objective [[  1.95894699e-05]]
Iteration 3000 objective [[  1.60381564e-05]]
Iteration 4000 objective [[  1.39930673e-05]]
Iteration 5000 objective [[  1.03554970e-05]]

&lt;/pre&gt;


&lt;p&gt;
&lt;img src="/media/nn-ode.png"&gt; 
&lt;/p&gt;

&lt;p&gt;
Huh. Those two solutions are nearly indistinguishable. Since we used a neural network, let's hype it up and say we learned the solution to a differential equation! But seriously, note that although we got an "analytical" solution, we should only rely on it in the region we trained the solution on. You can see the solution above is not that good past t=10, even perhaps going negative (which is not even physically correct). That is a reminder that the function we have for the solution &lt;i&gt;is not the same as the analytical solution&lt;/i&gt;, it just approximates it really well over the region we solved over. Of course, you can expand that region to the region you care about, but the main point is don't rely on the solution outside where you know it is good.
&lt;/p&gt;

&lt;p&gt;
This idea isn't new. There are several papers in the literature on using neural networks to solve differential equations, e.g. &lt;a href="http://www.sciencedirect.com/science/article/pii/S0255270102002076"&gt;http://www.sciencedirect.com/science/article/pii/S0255270102002076&lt;/a&gt; and &lt;a href="https://arxiv.org/pdf/physics/9705023.pdf"&gt;https://arxiv.org/pdf/physics/9705023.pdf&lt;/a&gt;, and other blog posts that are similar (&lt;a href="https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c"&gt;https://becominghuman.ai/neural-networks-for-solving-differential-equations-fa230ac5e04c&lt;/a&gt;, even using autograd). That means to me that there is some merit to continuing to investigate this approach to solving differential equations.
&lt;/p&gt;

&lt;p&gt;
There are some interesting challenges for engineers to consider with this approach though. When is the solution accurate enough? How reliable are derivatives of the solution? What network architecture is appropriate or best? How do you know how good the solution is? Is it possible to build in solution features, e.g. asymptotes, or constraints on derivatives, or that the solution should be monotonic, etc. These would help us trust the solutions not to do weird things, and to extrapolate more reliably.
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2017 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2017/11/28/Solving-ODEs-with-a-neural-network-and-autograd.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.2&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>http://jkitchin.github.io/blog</uri>
    </author>
    <title type="html"><![CDATA[Solving BVPs with a neural network and autograd]]></title>
    <link rel="alternate" type="text/html" href="http://jkitchin.github.io/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd" />
    <id>http://jkitchin.github.io/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd</id>
    <updated>2017-11-27T20:00:53Z</updated>
    <published>2017-11-27T19:59:52Z</published>
    <category scheme="http://jkitchin.github.io/blog" term="autograd" />
    <category scheme="http://jkitchin.github.io/blog" term="bvp" />
    <summary type="html"><![CDATA[Solving BVPs with a neural network and autograd]]></summary>
    <content type="html" xml:base="http://jkitchin.github.io/blog/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd"><![CDATA[


&lt;p&gt;
In this &lt;a href="http://kitchingroup.cheme.cmu.edu/blog/2013/03/11/Solving-the-Blasius-equation/"&gt;post&lt;/a&gt; we solved a boundary value problem by discretizing it, and approximating the derivatives by finite differences. Here I explore using a neural network to approximate the unknown function, autograd to get the required derivatives, and using autograd to train the neural network to satisfy the differential equations. We will look at the Blasius equation again.
&lt;/p&gt;

\begin{eqnarray}
f''' + \frac{1}{2} f f'' &amp;=&amp; 0 \\
f(0) &amp;=&amp; 0 \\
f'(0) &amp;=&amp; 0 \\
f'(\infty) &amp;=&amp; 1
\end{eqnarray}

&lt;p&gt;
Here I setup a simple neural network
&lt;/p&gt;
&lt;div class="org-src-container"&gt;
&lt;pre class="src src-python"&gt;&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; autograd.numpy &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; np
&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; autograd &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; grad, elementwise_grad
&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; autograd.numpy.random &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; npr
&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; autograd.misc.optimizers &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; adam

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;init_random_params&lt;/span&gt;(scale, layer_sizes, rs=npr.RandomState(0)):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"""Build a list of (weights, biases) tuples, one for each layer."""&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; [(rs.randn(insize, outsize) * scale,   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;weight matrix&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;rs.randn(outsize) * scale)           &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;bias vector&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;for&lt;/span&gt; insize, outsize &lt;span style="color: #0000FF;"&gt;in&lt;/span&gt; &lt;span style="color: #006FE0;"&gt;zip&lt;/span&gt;(layer_sizes[:-1], layer_sizes[1:])]


&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;swish&lt;/span&gt;(x):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"see https://arxiv.org/pdf/1710.05941.pdf"&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x / (1.0 + np.exp(-x))


&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;f&lt;/span&gt;(params, inputs):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #036A07;"&gt;"Neural network functions"&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;for&lt;/span&gt; W, b &lt;span style="color: #0000FF;"&gt;in&lt;/span&gt; params:
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;outputs&lt;/span&gt; = np.dot(inputs, W) + b
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;inputs&lt;/span&gt; = swish(outputs)    
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; outputs

&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   
&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;Here is our initial guess of params:&lt;/span&gt;
&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = init_random_params(0.1, layer_sizes=[1, 8, 1])

&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;Derivatives&lt;/span&gt;
&lt;span style="color: #BA36A5;"&gt;fp&lt;/span&gt; = elementwise_grad(f, 1)
&lt;span style="color: #BA36A5;"&gt;fpp&lt;/span&gt; = elementwise_grad(fp, 1)
&lt;span style="color: #BA36A5;"&gt;fppp&lt;/span&gt; = elementwise_grad(fpp, 1)

&lt;span style="color: #BA36A5;"&gt;eta&lt;/span&gt; = np.linspace(0, 6).reshape((-1, 1))

&lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;This is the function we seek to minimize&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;objective&lt;/span&gt;(params, step):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;These should all be zero at the solution&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;f''' + 0.5 f'' f = 0&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;zeq&lt;/span&gt; = fppp(params, eta) + 0.5 * f(params, eta) * fpp(params, eta) 
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;bc0&lt;/span&gt; = f(params, 0.0)  &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;equal to zero at solution&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;bc1&lt;/span&gt; = fp(params, 0.0)  &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;equal to zero at solution&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #BA36A5;"&gt;bc2&lt;/span&gt; = fp(params, 6.0) - 1.0 &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;this is the one at "infinity"&lt;/span&gt;
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; np.mean(zeq**2) + bc0**2 + bc1**2 + bc2**2

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;callback&lt;/span&gt;(params, step, g):
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; step % 1000 == 0:
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;"Iteration {0:3d} objective {1}"&lt;/span&gt;.&lt;span style="color: #006FE0;"&gt;format&lt;/span&gt;(step,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; objective(params, step)))

&lt;span style="color: #BA36A5;"&gt;params&lt;/span&gt; = adam(grad(objective), params,
&lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt;   &lt;span style="color: #9B9B9B; background-color: #EDEDED;"&gt; &lt;/span&gt; step_size=0.001, num_iters=10000, callback=callback) 

&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'f(0) = {}'&lt;/span&gt;.&lt;span style="color: #006FE0;"&gt;format&lt;/span&gt;(f(params, 0.0)))
&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'fp(0) = {}'&lt;/span&gt;.&lt;span style="color: #006FE0;"&gt;format&lt;/span&gt;(fp(params, 0.0)))
&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'fp(6) = {}'&lt;/span&gt;.&lt;span style="color: #006FE0;"&gt;format&lt;/span&gt;(fp(params, 6.0)))
&lt;span style="color: #0000FF;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'fpp(0) = {}'&lt;/span&gt;.&lt;span style="color: #006FE0;"&gt;format&lt;/span&gt;(fpp(params, 0.0)))

&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; plt
plt.plot(eta, f(params, eta))
plt.xlabel(&lt;span style="color: #008000;"&gt;'$\eta$'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'$f(\eta)$'&lt;/span&gt;)
plt.xlim([0, 6])
plt.ylim([0, 4.5])
plt.savefig(&lt;span style="color: #008000;"&gt;'nn-blasius.png'&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Iteration   0 objective 1.11472535
Iteration 1000 objective 0.00049768
Iteration 2000 objective 0.0004579
Iteration 3000 objective 0.00041697
Iteration 4000 objective 0.00037408
Iteration 5000 objective 0.00033705
Iteration 6000 objective 0.00031016
Iteration 7000 objective 0.00029197
Iteration 8000 objective 0.00027585
Iteration 9000 objective 0.00024616
f(0) = -0.00014613
fp(0) = 0.0003518041251639459
fp(6) = 0.999518061473252
fpp(0) = 0.3263370503702663
&lt;/p&gt;

&lt;p&gt;
&lt;img src="/media/nn-blasius.png"&gt; 
I think it is worth discussing what we accomplished here. You can see we have arrived at an approximate solution to our differential equation and the boundary conditions. The boundary conditions seem pretty closely met, and the figure is approximately the same as the previous post. Even better, our solution is &lt;i&gt;an actual function&lt;/i&gt; and not a numeric solution that has to be interpolated. We can evaluate it any where we want, including its derivatives!
&lt;/p&gt;

&lt;p&gt;
We did not, however, have to convert the ODE into a system of first-order differential equations, and we did &lt;i&gt;not&lt;/i&gt; have to approximate the derivatives with finite differences.
&lt;/p&gt;

&lt;p&gt;
Note, however, that with finite differences we got &lt;code&gt;f''(0)=0.3325&lt;/code&gt;. This &lt;a href="https://www.calpoly.edu/~kshollen/ME347/Handouts/Blasius.pdf"&gt;site&lt;/a&gt; reports &lt;code&gt;f''(0)=0.3321&lt;/code&gt;. We get close to that here with &lt;code&gt;f''(0) = 0.3263&lt;/code&gt;. We could probably get closer to this with more training to reduce the objective function further, or with a finer grid. It is evident that even after 9000 steps, it is still decreasing. Getting accurate derivatives is a stringent test for this, as they are measures of the function curvature.
&lt;/p&gt;

&lt;p&gt;
It is hard to tell how broadly useful this is; I have not tried it beyond this example. In the past, I have found solving BVPs to be pretty sensitive to the initial guesses of the solution. Here we made almost no guess at all, and still got a solution. I find that pretty remarkable.
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2017 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2017/11/27/Solving-BVPs-with-a-neural-network-and-autograd.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.1.2&lt;/p&gt;]]></content>
  </entry>
</feed>
