<?xml version="1.0" encoding="UTF-8"?>

<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
  >
  <title type="text">The Kitchin Research Group</title>
  <subtitle type="text">Chemical Engineering at Carnegie Mellon University</subtitle>

  <updated>2024-06-04T13:47:50Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog" />
  <id>https://kitchingroup.cheme.cmu.edu/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="https://kitchingroup.cheme.cmu.edu/blog/feed/atom/" />
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Cyclic Steady-State Simulation and Waveform Design for Dynamic Programmable Catalysis]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2024/05/23/New-publication-Cyclic-Steady-State-Simulation-and-Waveform-Design-for-Dynamic-Programmable-Catalysis" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2024/05/23/New-publication-Cyclic-Steady-State-Simulation-and-Waveform-Design-for-Dynamic-Programmable-Catalysis</id>
    <updated>2024-05-23T16:53:22Z</updated>
    <published>2024-05-23T16:51:46Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <summary type="html"><![CDATA[New publication - Cyclic Steady-State Simulation and Waveform Design for Dynamic Programmable Catalysis]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2024/05/23/New-publication-Cyclic-Steady-State-Simulation-and-Waveform-Design-for-Dynamic-Programmable-Catalysis"><![CDATA[


&lt;p&gt;
You can get higher rates of reaction on a catalyst by dynamically changing the adsorbate and reaction energetics. It has been an open challenge though to find ways to obtain the optimal waveform. In this work we present a problem formulation that is easy to solve and optimize waveforms in programmable catalysis.
&lt;/p&gt;

&lt;p&gt;
&lt;a href="https://doi.org/10.1021/acs.jpcc.4c01543"&gt;https://doi.org/10.1021/acs.jpcc.4c01543&lt;/a&gt;
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;tedesco-2024-cyclic-stead&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Carolina Colombo Tedesco and John R. Kitchin and Carl D.
                  Laird},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Cyclic Steady-State Simulation and Waveform Design for
                  Dynamic/programmable Catalysis},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {The Journal of Physical Chemistry C},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {nil},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2024,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1021/acs.jpcc.4c01543&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1021/acs.jpcc.4c01543&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Thu May 23 16:35:52 2024},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1021/acs.jpcc.4c01543'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/33VyAgDmSNo?si=otdqN8p6X-yX_A2V" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2024 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2024/05/23/New-publication---Cyclic-Steady-State-Simulation-and-Waveform-Design-for-Dynamic-Programmable-Catalysis.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[Kolmogorov-Arnold Networks (KANs) and Lennard Jones]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2024/05/05/Kolmogorov-Arnold-Networks-KANs-and-Lennard-Jones" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2024/05/05/Kolmogorov-Arnold-Networks-KANs-and-Lennard-Jones</id>
    <updated>2024-05-05T11:06:22Z</updated>
    <published>2024-05-05T11:06:22Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="uncategorized" />
    <summary type="html"><![CDATA[Kolmogorov-Arnold Networks (KANs) and Lennard Jones]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2024/05/05/Kolmogorov-Arnold-Networks-KANs-and-Lennard-Jones"><![CDATA[


&lt;div id="table-of-contents" role="doc-toc"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents" role="doc-toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#org6ed4d7a"&gt;1. Create a dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orgca2cb78"&gt;2. Create and train the model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
KANs have been a hot topic of discussion recently (&lt;a href="https://arxiv.org/abs/2404.19756"&gt;https://arxiv.org/abs/2404.19756&lt;/a&gt;). Here I explore using them as an alternative to a neural network for a simple atomistic potential using Lennard Jones data. I adapted this code from  &lt;a href="https://github.com/KindXiaoming/pykan/blob/master/hellokan.ipynb"&gt;https://github.com/KindXiaoming/pykan/blob/master/hellokan.ipynb&lt;/a&gt;. 
&lt;/p&gt;

&lt;p&gt;
TL;DR It was easy to make the model, and it fit this simple data very well. It does not extrapolate in this example, and it is not obvious what the extrapolation behavior should be.
&lt;/p&gt;

&lt;div id="outline-container-org6ed4d7a" class="outline-2"&gt;
&lt;h2 id="org6ed4d7a"&gt;&lt;span class="section-number-2"&gt;1.&lt;/span&gt; Create a dataset&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
We leverage the &lt;code&gt;create_dataset&lt;/code&gt; function to generate the dataset here. I chose a range with some modest nonlinearity, and the minimum.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; plt
&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; torch
&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; kan &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; create_dataset, KAN

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;LJ&lt;/span&gt;(r):
    &lt;span style="color: #BA36A5;"&gt;r6&lt;/span&gt; = r**6
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; 1 / r6**2 - 1 / r6

&lt;span style="color: #BA36A5;"&gt;dataset&lt;/span&gt; = create_dataset(LJ, n_var=1, ranges=[0.95, 2.0],
                         train_num=50)

plt.plot(dataset[&lt;span style="color: #008000;"&gt;'train_input'&lt;/span&gt;], dataset[&lt;span style="color: #008000;"&gt;'train_label'&lt;/span&gt;], &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;)
plt.xlabel(&lt;span style="color: #008000;"&gt;'r'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/0db7627856ef3cacbeb19cba9e64a53fb49bf422.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-orgca2cb78" class="outline-2"&gt;
&lt;h2 id="orgca2cb78"&gt;&lt;span class="section-number-2"&gt;2.&lt;/span&gt; Create and train the model&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
We start by making the model. We are going to model a Lennard-Jones potential with one input, the distance between two atoms, and one output. We start with a width of 2 "neurons".
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;model&lt;/span&gt; = KAN(width=[1, 2, 1])
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Training is easy. You can even run this cell several times.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;model.train(dataset, opt=&lt;span style="color: #008000;"&gt;"LBFGS"&lt;/span&gt;, steps=20);

model.plot()
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
train loss: 1.64e-04 | test loss: 1.46e-02 | reg: 6.72e+00 : 100%|██| 20/20 [00:03&amp;lt;00:00,  5.61it/s]

&lt;/pre&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/0cea2b134045cc964f990ac28b524c32d441976b.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;


&lt;p&gt;
We can see here that the fit looks very good.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;X&lt;/span&gt; = torch.linspace(dataset[&lt;span style="color: #008000;"&gt;'train_input'&lt;/span&gt;].&lt;span style="color: #006FE0;"&gt;min&lt;/span&gt;(),
                   dataset[&lt;span style="color: #008000;"&gt;'train_input'&lt;/span&gt;].&lt;span style="color: #006FE0;"&gt;max&lt;/span&gt;(), 100)[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]

plt.plot(dataset[&lt;span style="color: #008000;"&gt;'train_input'&lt;/span&gt;], dataset[&lt;span style="color: #008000;"&gt;'train_label'&lt;/span&gt;], &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;, label=&lt;span style="color: #008000;"&gt;'data'&lt;/span&gt;)

plt.plot(X, model(X).detach().numpy(), &lt;span style="color: #008000;"&gt;'r-'&lt;/span&gt;, label=&lt;span style="color: #008000;"&gt;'fit'&lt;/span&gt;)
plt.legend()
plt.xlabel(&lt;span style="color: #008000;"&gt;'r'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/24eddff0ce69063a1aaabc80060e78b56ecef0b5.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
KANs do not save us from extrapolation issues though. I think a downside of KANs is it is not obvious what extrapolation behavior to expect. I guess it could be related to what happens in the spline representation of the functions. Eventually those have to extrapolate too.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;X&lt;/span&gt; = torch.linspace(0, 5, 1000)[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]
plt.plot(dataset[&lt;span style="color: #008000;"&gt;'train_input'&lt;/span&gt;], dataset[&lt;span style="color: #008000;"&gt;'train_label'&lt;/span&gt;], &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;)
plt.plot(X, model(X).detach().numpy(), &lt;span style="color: #008000;"&gt;'r-'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/a16818596b6a60ea026406808143fcddcfae54f9.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;


&lt;p&gt;
It is early days for KANs, so many things we know about MLPs are still unknown for KANs. For example, with MLPs we know they extrapolate like the activation functions. Probably there is some insight like that to be had here, but it needs to be uncovered. With MLPs there are a lot of ways to regularize them for desired behavior. Probably that is true here too, and will be discovered. Similarly, there are many ways people have approached uncertainty quantification in MLPs that probably have some analog in KANs. 
Still, the ease of use suggests it could be promising for some applications.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copyright (C) 2024 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2024/05/05/Kolmogorov-Arnold-Networks-(KANs)-and-Lennard-Jones.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[Generalization of Graph-Based Active Learning Relaxation Strategies Across Materials]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2024/04/20/Generalization-of-Graph-Based-Active-Learning-Relaxation-Strategies-Across-Materials" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2024/04/20/Generalization-of-Graph-Based-Active-Learning-Relaxation-Strategies-Across-Materials</id>
    <updated>2024-04-20T09:20:07Z</updated>
    <published>2024-04-20T09:20:07Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <summary type="html"><![CDATA[Generalization of Graph-Based Active Learning Relaxation Strategies Across Materials]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2024/04/20/Generalization-of-Graph-Based-Active-Learning-Relaxation-Strategies-Across-Materials"><![CDATA[


&lt;p&gt;
Geometry optimization is an expensive part of DFT; each step requires a DFT step. The Open Catalyst Project provides pre-trained machine learned potentials that provide cheap forces for a broad range of metallic, intermetallic materials. In this work we use models trained on the OC20 dataset to accelerate geometry optimization of materials outside that domain including larger adsorbates, oxides, and zeolites.  With fine-tuning, we are able to reduce the number of DFT calls required substantially for these systems.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;10.1088/2632-2153/ad37f0&lt;/span&gt;,
        &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt;={Wang, Xiaoxiao and Musielewicz, Joseph and Tran, Richard and Ethirajan, Sudheesh Kumar and Fu, Xiaoyan and Mera, Hilda and Kitchin, John R and Kurchin, Rachel and Ulissi, Zachary W},
        &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt;={Generalization of Graph-Based Active Learning Relaxation Strategies Across Materials},
        &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt;={Machine Learning: Science and Technology},
        &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt;={&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://iopscience.iop.org/article/10.1088/2632-2153/ad37f0&lt;/span&gt;},
        &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt;={2024}
}
&lt;/pre&gt;
&lt;/div&gt;


&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1088/2632-2153/ad37f0'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/xQk59F2HTwQ?si=xLhbVO585CRa4YF_" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2024 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2024/04/20/Generalization-of-Graph-Based-Active-Learning-Relaxation-Strategies-Across-Materials.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[A little more than a decade of the Kitchingroup blog]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2024/04/03/A-little-more-than-a-decade-of-the-Kitchingroup-blog" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2024/04/03/A-little-more-than-a-decade-of-the-Kitchingroup-blog</id>
    <updated>2024-04-03T08:38:34Z</updated>
    <published>2024-04-03T08:38:34Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="uncategorized" />
    <summary type="html"><![CDATA[A little more than a decade of the Kitchingroup blog]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2024/04/03/A-little-more-than-a-decade-of-the-Kitchingroup-blog"><![CDATA[


&lt;p&gt;
There are a few early entries I backdated, but this blog got started in its present form in January 2013. This entry marks entry #594. I started this blog as part of an exercise in switching from Matlab to Python, and the first hundred entries or so are just me solving a problem in Python that I had previously solved in Matlab. It then expanded to include lots of entries on Emacs and org-mode, and other research related topics from my group. Many entries simply document something I spent time working out and that I wanted to be able to find by Google later.
&lt;/p&gt;


&lt;p&gt;
When I set the blog up, I enabled Google Analytics to see if anyone would look at. Recently Google announced they are shutting down the version of analytics I was using, and transitioning to a newer approach. They no longer collect data with the version this blog is using (since Oct last year), and they will delete the data this summer, so today I downloaded some of it to see what has happened over the past decade.
&lt;/p&gt;

&lt;p&gt;
Anecdotally many people from around the world have told me how useful the blog was for them. Now, I have data to see how many people have been impacted by this blog. This figure shows that a lot of people spent time in some part of the blog over the past decade! The data suggests over 1M people viewed these pages over 2M times. 
&lt;/p&gt;


&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-03-04-2024-time-08-26-10.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;


&lt;p&gt;
The peak usage was around 2020, and it has been trailing off since then. I have not been as active in posting since then. You can also see there is a very long build up to that peak.
&lt;/p&gt;

&lt;p&gt;
The user group for the blog is truly world wide, including almost every country in this map. That is amazing!
&lt;/p&gt;


&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-03-04-2024-time-08-33-54.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
Finally, I found the pages that were most viewed. It is interesting most of them are the older pages, and all about Python. I guess that means I should write more posts on Python.
&lt;/p&gt;


&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-03-04-2024-time-08-35-06.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;


&lt;p&gt;
I don't know what the future of the blog is. It is in need of an overhaul. The packages that build it still work, but are not actively maintained. I have also spent more time writing with Jupyter Book lately than the way I wrote this blog. It isn't likely to disappear any time soon, it sits rent-free in GitHUB pages.
&lt;/p&gt;

&lt;p&gt;
To conclude, to everyone who has read these pages, thank you! It has been a lot of work to put together over the years, and I am glad to see many people have taken a look at it.
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2024 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2024/04/03/A-little-more-than-a-decade-of-the-Kitchingroup-blog.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Circumventing data imbalance in magnetic ground state data for magnetic moment predictions]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2024/02/17/New-publication-Circumventing-data-imbalance-in-magnetic-ground-state-data-for-magnetic-moment-predictions" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2024/02/17/New-publication-Circumventing-data-imbalance-in-magnetic-ground-state-data-for-magnetic-moment-predictions</id>
    <updated>2024-02-17T09:59:30Z</updated>
    <published>2024-02-17T09:59:30Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <summary type="html"><![CDATA[New publication - Circumventing data imbalance in magnetic ground state data for magnetic moment predictions]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2024/02/17/New-publication-Circumventing-data-imbalance-in-magnetic-ground-state-data-for-magnetic-moment-predictions"><![CDATA[


&lt;p&gt;
Modeling magnetic materials with DFT is hard. In this work we develop a machine learning approach to predicting magnetic properties of materials based on their structure. Our two stage model first predicts if a material is magnetic, and then if it is, what the magnetic moments on each atom are. We show this can lead to faster and lower energy DFT solutions.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;sanspeur-2024-circum-data&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Rohan Yuri Sanspeur and John R Kitchin},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Circumventing Data Imbalance in Magnetic Ground State Data for
                  Magnetic Moment Predictions},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Machine Learning: Science and Technology},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {5},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {1},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {015023},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2024,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1088/2632-2153/ad23fb&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1088/2632-2153/ad23fb&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Tue Feb 6 20:13:47 2024},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1088/2632-2153/ad23fb'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/FaOwCbkc3zc?si=77Bz5Xfmbz7FSZFq" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2024 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2024/02/17/New-publication---Circumventing-data-imbalance-in-magnetic-ground-state-data-for-magnetic-moment-predictions.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Applying Large Graph Neural Networks to Predict Transition Metal Complex Energies Using the tmQM_wB97MV Data Set]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2023/12/10/New-publication-Applying-Large-Graph-Neural-Networks-to-Predict-Transition-Metal-Complex-Energies-Using-the-tmQM-wB97MV-Data-Set" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2023/12/10/New-publication-Applying-Large-Graph-Neural-Networks-to-Predict-Transition-Metal-Complex-Energies-Using-the-tmQM-wB97MV-Data-Set</id>
    <updated>2023-12-10T14:53:33Z</updated>
    <published>2023-12-10T14:53:33Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <summary type="html"><![CDATA[New publication - Applying Large Graph Neural Networks to Predict Transition Metal Complex Energies Using the tmQM_wB97MV Data Set]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2023/12/10/New-publication-Applying-Large-Graph-Neural-Networks-to-Predict-Transition-Metal-Complex-Energies-Using-the-tmQM-wB97MV-Data-Set"><![CDATA[


&lt;p&gt;
In this work, we show that we can use large graph neural networks to predict transition metal complex energies. We developed an improved dataset at a higher level of theory, and tested models ranging from GemNet-T (best) to SchNet (worst). The model performance saturates with the size of neutral structures, and improves with increasing size of charged structures. Finally, we showed that a pre-trained model from OC20 was even better than training from scratch. This indicates a degree of transferability from heterogeneous catalyst models to homogeneous molecular catalysts.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;garrison-2023-apply-large&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Garrison, Aaron G. and Heras-Domingo, Javier and Kitchin, John
                  R. and dos Passos Gomes, Gabriel and Ulissi, Zachary W. and
                  Blau, Samuel M.},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Applying Large Graph Neural Networks To Predict Transition
                  Metal Complex Energies Using the tmQM\_wB97MV Data Set},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Journal of Chemical Information and Modeling},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       0,
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       0,
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {null},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2023,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1021/acs.jcim.3c01226&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;URL&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1021/acs.jcim.3c01226&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;eprint&lt;/span&gt; =       {https://doi.org/10.1021/acs.jcim.3c01226},
  &lt;span style="color: #BA36A5;"&gt;note&lt;/span&gt; =         {PMID: 38049389},
}

&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1021/acs.jcim.3c01226'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/p_DpuIdcelY?si=HaBtCUlByRjuJU7i" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2023 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2023/12/10/New-publication---Applying-Large-Graph-Neural-Networks-to-Predict-Transition-Metal-Complex-Energies-Using-the-tmQM_wB97MV-Data-Set.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Chemical Properties from Graph Neural Network-Predicted Electron Densities]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2023/12/03/New-publication-Chemical-Properties-from-Graph-Neural-Network-Predicted-Electron-Densities" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2023/12/03/New-publication-Chemical-Properties-from-Graph-Neural-Network-Predicted-Electron-Densities</id>
    <updated>2023-12-03T14:46:11Z</updated>
    <published>2023-12-03T14:46:11Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <summary type="html"><![CDATA[New publication - Chemical Properties from Graph Neural Network-Predicted Electron Densities]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2023/12/03/New-publication-Chemical-Properties-from-Graph-Neural-Network-Predicted-Electron-Densities"><![CDATA[


&lt;p&gt;
The electron density is one of the most important quantities we use DFT to calculate. It is the foundation of how we compute energy, forces, and many other properties. DFT is expensive though, so in this work we show that we can build a graph neural network that can be used to predict electron densities directly from the atomic coordinates of a system. We show that the predicted densities can also be used to estimate dipole moments and Bader charges.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;sunshine-2023-chemic-proper&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Sunshine, Ethan M. and Shuaibi, Muhammed and Ulissi, Zachary
                  W. and Kitchin, John R.},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Chemical Properties From Graph Neural Network-Predicted
                  Electron Densities},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {The Journal of Physical Chemistry C},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       0,
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       0,
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {null},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2023,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1021/acs.jpcc.3c06157&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1021/acs.jpcc.3c06157&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;eprint&lt;/span&gt; =       {https://doi.org/10.1021/acs.jpcc.3c06157},
}

&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1021/acs.jpcc.3c06157'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/nTxqdfY28iQ?si=QYijk-wra9ePHXWA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2023 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2023/12/03/New-publication---Chemical-Properties-from-Graph-Neural-Network-Predicted-Electron-Densities.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Beyond Independent Error Assumptions in Large GNN Atomistic Models]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2023/09/24/New-publication-Beyond-Independent-Error-Assumptions-in-Large-GNN-Atomistic-Models" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2023/09/24/New-publication-Beyond-Independent-Error-Assumptions-in-Large-GNN-Atomistic-Models</id>
    <updated>2023-09-24T15:04:34Z</updated>
    <published>2023-09-24T15:04:34Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <summary type="html"><![CDATA[New publication - Beyond Independent Error Assumptions in Large GNN Atomistic Models]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2023/09/24/New-publication-Beyond-Independent-Error-Assumptions-in-Large-GNN-Atomistic-Models"><![CDATA[


&lt;p&gt;
In this work we show that prediction errors from graph neural networks for related atomistic systems tend to be correlated, and as a result the differences in energy are more accurate than the absolute energies. This is similar to what is observed in DFT calculations where systematic errors also cancel in differences. We show this quantitatively through differences of systems with systematically different levels of similarity. This article is part of the Special Collection: 2023 JCP Emerging Investigators Special Collection.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;ock-2023-beyon-indep&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Janghoon Ock and Tian Tian and John Kitchin and Zachary
                  Ulissi},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Beyond Independent Error Assumptions in Large {GNN} Atomistic
                  Models},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {The Journal of Chemical Physics},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       158,
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       21,
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {nil},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2023,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1063/5.0151159&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1063/5.0151159&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Sun Sep 24 14:53:46 2023},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1063/5.0151159'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/OlgAxDRI2lA?si=lywkJh8mPeieAyvf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2023 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2023/09/24/New-publication---Beyond-Independent-Error-Assumptions-in-Large-GNN-Atomistic-Models.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[Adding new backends to hashcache]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2023/09/24/Adding-new-backends-to-hashcache" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2023/09/24/Adding-new-backends-to-hashcache</id>
    <updated>2023-09-24T13:14:40Z</updated>
    <published>2023-09-24T13:14:40Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="programming" />
    <summary type="html"><![CDATA[Adding new backends to hashcache]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2023/09/24/Adding-new-backends-to-hashcache"><![CDATA[


&lt;div id="table-of-contents" role="doc-toc"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents" role="doc-toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#org5fd57a7"&gt;1. Alternative backends for hashcache&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orgac9f2ab"&gt;2. a shelve version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org3be8901"&gt;3. sqlite version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org32e7950"&gt;4. redis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org29b3384"&gt;5. Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
I have been working on hashcache to make it more flexible. I like the base functionality that uses the filesystem for caching. That still works.
&lt;/p&gt;

&lt;p&gt;
Here I set up a timeit decorator to show how this works.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; pycse.hashcache &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; hashcache
&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; time

!rm -fr ./cache

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;timeit&lt;/span&gt;(func):
    &lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;wrapper&lt;/span&gt;(*args, **kwargs):
        &lt;span style="color: #BA36A5;"&gt;t0&lt;/span&gt; = time.time()
        &lt;span style="color: #BA36A5;"&gt;res&lt;/span&gt; = func(*args, **kwargs)
        &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(f&lt;span style="color: #008000;"&gt;'Elapsed time = &lt;/span&gt;{time.time() - t0}&lt;span style="color: #008000;"&gt;s'&lt;/span&gt;)
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; res
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; wrapper
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Now we decorate a function that is "expensive". The first time we run it, it takes a long time.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #6434A3;"&gt;@timeit&lt;/span&gt;
&lt;span style="color: #6434A3;"&gt;@hashcache&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;expensive_func&lt;/span&gt;(x):
    time.sleep(3)
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x

expensive_func(2)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Elapsed time = 3.007030963897705s
2
&lt;/p&gt;

&lt;p&gt;
The second time is very fast, since we just look it up.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;expensive_func(2)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Elapsed time = 0.0012097358703613281s
2
&lt;/p&gt;

&lt;p&gt;
Where did we look it up from? It is stored on disk. You can see where by adding a verbose option to the decorator. This shows you all the data that was stored in the cache.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #6434A3;"&gt;@hashcache&lt;/span&gt;(verbose=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;expensive_func&lt;/span&gt;(x):
    time.sleep(3)
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x

expensive_func(2)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
{   'args': (2,),
    'cwd': '/Users/jkitchin/Dropbox/emacs/journal/2023/09/23',
    'elapsed_time': 3.0048787593841553,
    'func': 'expensive_func',
    'hash': 'b5436cc21714a7ea619729cc9768b8c5b3a03307',
    'kwargs': {},
    'module': '&lt;span class="underline"&gt;&lt;span class="underline"&gt;main&lt;/span&gt;&lt;/span&gt;',
    'output': 2,
    'run-at': 1695572717.2020931,
    'run-at-human': 'Sun Sep 24 12:25:17 2023',
    'standardized-kwargs': {'x': 2},
    'user': 'jkitchin',
    'version': '0.0.2'}
2
&lt;/p&gt;

&lt;div id="outline-container-org5fd57a7" class="outline-2"&gt;
&lt;h2 id="org5fd57a7"&gt;&lt;span class="section-number-2"&gt;1.&lt;/span&gt; Alternative backends for hashcache&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
The file system is an amazing cache with many benefits. There are few reasons you might like something different though. For example, it is slow to search if you have to iterate over all the directories and read the files, and it might be slow to sync lots of directories to another place. 
&lt;/p&gt;

&lt;p&gt;
hashcache is more flexible now, so you can define the functions that load and dump the cache. Here we use lmdb as a key-value database. lmdb expects the keys and values to be bytes, so we do some tricks with io.BytesIO to get these as strings from joblib.dump which expects to write to a file.
&lt;/p&gt;

&lt;p&gt;
The load function has the signature (&lt;code&gt;hash&lt;/code&gt;, &lt;code&gt;verbose&lt;/code&gt;), and the dump function has the signature (&lt;code&gt;hash&lt;/code&gt;, &lt;code&gt;data&lt;/code&gt;, &lt;code&gt;verbose&lt;/code&gt;). In both cases, &lt;code&gt;hash&lt;/code&gt; will be a string for the key to save data in. &lt;code&gt;data&lt;/code&gt; will be a dictionary that should be saved in a way that it can be reloaded. &lt;code&gt;verbose&lt;/code&gt; is a flag that you can ignore or use to provide some kind of logging.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; pycse.hashcache &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; hashcache

&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; io, joblib, lmdb

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;lmdb_dump&lt;/span&gt;(hsh, data, verbose=&lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;):
    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; verbose:
        &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'running lmdb_dump'&lt;/span&gt;)
    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; io.BytesIO() &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; f:
        joblib.dump(data, f)
        value = f.getvalue()

    db = lmdb.Environment(hashcache.cache)
    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; db.begin(write=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;) &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; txn:
        txn.put(hsh.encode(&lt;span style="color: #008000;"&gt;'utf-8'&lt;/span&gt;), value)

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;lmdb_load&lt;/span&gt;(hsh, verbose=&lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;):
    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; verbose:
        &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'running lmdb_load'&lt;/span&gt;)
    db = lmdb.Environment(hashcache.cache)
    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; db.begin() &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; txn:
        val = txn.get(hsh.encode(&lt;span style="color: #008000;"&gt;'utf-8'&lt;/span&gt;))
        &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; val &lt;span style="color: #0000FF;"&gt;is&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;:
            &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;
        &lt;span style="color: #0000FF;"&gt;else&lt;/span&gt;:
            &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;, joblib.load(io.BytesIO(val))[&lt;span style="color: #008000;"&gt;'output'&lt;/span&gt;]
                                    
! rm -fr cache.lmdb

hashcache.cache = &lt;span style="color: #008000;"&gt;'cache.lmdb'&lt;/span&gt;


&lt;span style="color: #6434A3;"&gt;@hashcache&lt;/span&gt;(loader=lmdb_load, dumper=lmdb_dump, verbose=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;f&lt;/span&gt;(x):
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x

f(2)   
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
running lmdb_load
running lmdb_dump
2
&lt;/p&gt;

&lt;p&gt;
And we can recall the result as easily.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;f(2)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
running lmdb_load
2
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgac9f2ab" class="outline-2"&gt;
&lt;h2 id="orgac9f2ab"&gt;&lt;span class="section-number-2"&gt;2.&lt;/span&gt; a shelve version&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
Maybe you prefer a built in library like shelve. This is also quite simple.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; pycse.hashcache &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; hashcache

&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; io, joblib, shelve

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;shlv_dump&lt;/span&gt;(hsh, data, verbose=&lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;):
    &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'running shlv_dump'&lt;/span&gt;)
    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; io.BytesIO() &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; f:
        joblib.dump(data, f)
        value = f.getvalue()

    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; shelve.&lt;span style="color: #006FE0;"&gt;open&lt;/span&gt;(hashcache.cache) &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; db:
        db[&lt;span style="color: #BA36A5;"&gt;hsh&lt;/span&gt;] = value

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;shlv_load&lt;/span&gt;(hsh, verbose=&lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;):
    &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'running shlv_load'&lt;/span&gt;)
    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; shelve.&lt;span style="color: #006FE0;"&gt;open&lt;/span&gt;(hashcache.cache) &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; db:
        &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; hsh &lt;span style="color: #0000FF;"&gt;in&lt;/span&gt; db:
            &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;, joblib.load(io.BytesIO(db[hsh]))[&lt;span style="color: #008000;"&gt;'output'&lt;/span&gt;]
        &lt;span style="color: #0000FF;"&gt;else&lt;/span&gt;:
            &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;

hashcache.cache = &lt;span style="color: #008000;"&gt;'cache.shlv'&lt;/span&gt;
! rm -f cache.shlv.db

&lt;span style="color: #6434A3;"&gt;@hashcache&lt;/span&gt;(loader=shlv_load, dumper=shlv_dump)
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;f&lt;/span&gt;(x):
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x

f(2)
    
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
running shlv_load
running shlv_dump
2
&lt;/p&gt;

&lt;p&gt;
And again loading is easy.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;f(2)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
running shlv_load
2
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org3be8901" class="outline-2"&gt;
&lt;h2 id="org3be8901"&gt;&lt;span class="section-number-2"&gt;3.&lt;/span&gt; sqlite version&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
I am a big fan of sqlite. Here I use a simple table mapping a key to a value. I think it could be interesting to consider storing the value as &lt;a href="https://www.sqlite.org/json1.html"&gt;json&lt;/a&gt; that would make it more searchable, or you could make a more complex table, but here we keep it simple.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; pycse.hashcache &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; hashcache

&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; io, joblib, sqlite3

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;sql_dump&lt;/span&gt;(hsh, data, verbose=&lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;):
    &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'running sql_dump'&lt;/span&gt;)
    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; io.BytesIO() &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; f:
        joblib.dump(data, f)
        value = f.getvalue()

    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; con:
        con.execute(&lt;span style="color: #008000;"&gt;"INSERT INTO cache(hash, value) VALUES(?, ?)"&lt;/span&gt;,
                    (hsh, value))

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;sql_load&lt;/span&gt;(hsh, verbose=&lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;):
    &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'running sql_load'&lt;/span&gt;)
    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; con:        
        cur = con.execute(&lt;span style="color: #008000;"&gt;"SELECT value FROM cache WHERE hash = ?"&lt;/span&gt;,
                          (hsh,))
        value = cur.fetchone()
        &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; value &lt;span style="color: #0000FF;"&gt;is&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;:
            &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;
        &lt;span style="color: #0000FF;"&gt;else&lt;/span&gt;:
            &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;, joblib.load(io.BytesIO(value[0]))[&lt;span style="color: #008000;"&gt;'output'&lt;/span&gt;]

! rm -f cache.sql
hashcache.cache = &lt;span style="color: #008000;"&gt;'cache.sql'&lt;/span&gt;
con = sqlite3.connect(hashcache.cache)
con.execute(&lt;span style="color: #008000;"&gt;"CREATE TABLE cache(hash TEXT unique, value BLOB)"&lt;/span&gt;)
        
&lt;span style="color: #6434A3;"&gt;@hashcache&lt;/span&gt;(loader=sql_load, dumper=sql_dump)
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;f&lt;/span&gt;(x):
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x

f(2)    
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
running sql_load
running sql_dump
2
&lt;/p&gt;

&lt;p&gt;
Once again, running is easy.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;f(2)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
running sql_load
2
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org32e7950" class="outline-2"&gt;
&lt;h2 id="org32e7950"&gt;&lt;span class="section-number-2"&gt;4.&lt;/span&gt; redis&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
Finally, you might like a server to cache in. This opens the door to running the server remotely so it is accessible by multiple processes using the cache on different machines. We use redis for this example, but only run it locally. Make sure you run &lt;code&gt;redis-server --daemonize yes&lt;/code&gt;
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; pycse.hashcache &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; hashcache

&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; io, joblib, redis

&lt;span style="color: #BA36A5;"&gt;db&lt;/span&gt; = redis.Redis(host=&lt;span style="color: #008000;"&gt;'localhost'&lt;/span&gt;, port=6379)

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;redis_dump&lt;/span&gt;(hsh, data, verbose=&lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;):
    &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'running redis_dump'&lt;/span&gt;)
    &lt;span style="color: #0000FF;"&gt;with&lt;/span&gt; io.BytesIO() &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; f:
        joblib.dump(data, f)
        value = f.getvalue()

    db.&lt;span style="color: #006FE0;"&gt;set&lt;/span&gt;(hsh, value)

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;redis_load&lt;/span&gt;(hsh, verbose=&lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;):
    &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'running redis_load'&lt;/span&gt;)
    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; &lt;span style="color: #0000FF;"&gt;not&lt;/span&gt; hsh &lt;span style="color: #0000FF;"&gt;in&lt;/span&gt; db:
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;False&lt;/span&gt;, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;
    &lt;span style="color: #0000FF;"&gt;else&lt;/span&gt;:
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;, joblib.load(io.BytesIO(db.get(hsh)))[&lt;span style="color: #008000;"&gt;'output'&lt;/span&gt;]

    
&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; functools    
hashcache_redis = functools.partial(hashcache,
                                    loader=redis_load,
                                    dumper=redis_dump)    

&lt;span style="color: #6434A3;"&gt;@hashcache_redis&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;f&lt;/span&gt;(x):
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; x

f(2)    
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
running redis_load
running redis_dump
2
&lt;/p&gt;

&lt;p&gt;
No surprise here, loading is the same as before.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;f(2)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
running redis_load
2
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org29b3384" class="outline-2"&gt;
&lt;h2 id="org29b3384"&gt;&lt;span class="section-number-2"&gt;5.&lt;/span&gt; Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
I have refactored hashcache to make it much easier to add new backends. You might do that for performance, ease of backup or transferability, to add new capabilities for searching, etc. The new code is a little cleaner than it was before IMO. I am not sure it is API-stable yet, but it is getting there.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copyright (C) 2023 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2023/09/24/Adding-new-backends-to-hashcache.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[A better manager for supervising Python functions]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2023/09/21/A-better-manager-for-supervising-Python-functions" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2023/09/21/A-better-manager-for-supervising-Python-functions</id>
    <updated>2023-09-21T13:43:17Z</updated>
    <published>2023-09-21T13:42:13Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="programming" />
    <summary type="html"><![CDATA[A better manager for supervising Python functions]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2023/09/21/A-better-manager-for-supervising-Python-functions"><![CDATA[


&lt;div id="table-of-contents" role="doc-toc"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents" role="doc-toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#org0250367"&gt;1. The previous examples with manager&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orgbc4f0a8"&gt;2. Stateful supervision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org5218dd5"&gt;3. Handling exceptions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org677186e"&gt;4. Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
In the previous &lt;a href="https://kitchingroup.cheme.cmu.edu/blog/2023/09/20/Supervising-Python-functions/index.html"&gt;post&lt;/a&gt; I introduced a supervisor decorator to automate rerunning functions with new arguments to fix issues in them. Almost immediately after posting it, two things started bugging me. First, I thought it was annoying to have two separate arguments for results and exceptions. I would prefer one list of functions that do the right thing. Second, and most annoying, you have to be very careful in writing your checker functions to be consistent with how you called the function so you use exactly the same positional and keyword arguments. That is tedious and limits reusability/flexibility.
&lt;/p&gt;

&lt;p&gt;
So, I wrote a new &lt;code&gt;manager&lt;/code&gt; decorator that solves these two problems. Now, you can write checker functions that work on all the arguments of a function. You decorate the checker functions to indicate if they are for results or exceptions. This was a little more of a rabbit hole than I anticipated, but I persevered, and got to a solution that works for these examples. You can find all the code &lt;a href="https://github.com/jkitchin/pycse/blob/master/pycse/supyrvisor.py#L99"&gt;here&lt;/a&gt;.
&lt;/p&gt;

&lt;p&gt;
Here is an example where we have a test function that we want to run with new arguments until we get a positive result. We start in a way that it is possible to get a ZeroDivisionError, and we handle that too.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; pycse.supyrvisor &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; manager, check_result, check_exception

&lt;span style="color: #6434A3;"&gt;@check_exception&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;check1&lt;/span&gt;(args, exc):
    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; &lt;span style="color: #006FE0;"&gt;isinstance&lt;/span&gt;(exc, &lt;span style="color: #6434A3;"&gt;ZeroDivisionError&lt;/span&gt;):
        &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(&lt;span style="color: #008000;"&gt;'ooo. caught 1/0, incrementing x'&lt;/span&gt;)
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; {&lt;span style="color: #008000;"&gt;'x'&lt;/span&gt;: 1}

&lt;span style="color: #6434A3;"&gt;@check_result&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;check2&lt;/span&gt;(args, result):
    &lt;span style="color: #006FE0;"&gt;print&lt;/span&gt;(args)
    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; result &amp;lt; 0:
        &lt;span style="color: #BA36A5;"&gt;args&lt;/span&gt;[&lt;span style="color: #008000;"&gt;'x'&lt;/span&gt;] += 1
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; args
        

&lt;span style="color: #6434A3;"&gt;@manager&lt;/span&gt;(checkers=[check1, check2])
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;test&lt;/span&gt;(x, a=1):
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; a / x

test(-1)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
{'x': -1, 'a': 1}
ooo. caught 1/0, incrementing x
{'x': 1}
1.0
&lt;/p&gt;

&lt;p&gt;
This also works, so you can see this is better than the previous version which would not work if you change the signature.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;test(a=1, x=-1)
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
{'x': -1, 'a': 1}
ooo. caught 1/0, incrementing x
{'x': 1}
1.0
&lt;/p&gt;


&lt;div id="outline-container-org0250367" class="outline-2"&gt;
&lt;h2 id="org0250367"&gt;&lt;span class="section-number-2"&gt;1.&lt;/span&gt; The previous examples with manager&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
Here is the new syntax with manager.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; numpy &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; np
&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; scipy.optimize &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; minimize

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;objective&lt;/span&gt;(x):
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; np.exp(x**2) - 10 * np.exp(x)


&lt;span style="color: #6434A3;"&gt;@check_result&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;maxIterationsExceeded&lt;/span&gt;(args, sol):
    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; sol.message == &lt;span style="color: #008000;"&gt;'Maximum number of iterations has been exceeded.'&lt;/span&gt;:
        args[&lt;span style="color: #008000;"&gt;'maxiter'&lt;/span&gt;] *= 2
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; args

&lt;span style="color: #6434A3;"&gt;@manager&lt;/span&gt;(checkers=[maxIterationsExceeded], verbose=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;get_result&lt;/span&gt;(maxiter=2):
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; minimize(objective, 0.0, options={&lt;span style="color: #008000;"&gt;'maxiter'&lt;/span&gt;: maxiter})

get_result(2)
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example" id="orgc74758e"&gt;
Proposed fix in wrapper: {'maxiter': 4}
Proposed fix in wrapper: {'maxiter': 8}
  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: -36.86307468296428
        x: [ 1.662e+00]
      nit: 5
      jac: [-4.768e-07]
 hess_inv: [[ 6.481e-03]]
     nfev: 26
     njev: 13
&lt;/pre&gt;


&lt;p&gt;
It works!
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-orgbc4f0a8" class="outline-2"&gt;
&lt;h2 id="orgbc4f0a8"&gt;&lt;span class="section-number-2"&gt;2.&lt;/span&gt; Stateful supervision&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
In this example, we aim to find the steady state concentrations of two species by integrating a mass balance to steady state. This is visually easy to see below, the concentrations are essentially flat after 10 min or so. Computationally this is somewhat tricky to find though. A way to do it is to compare some windows of integration to see if the values are not changing very fast. For instance you could average the values from 10 to 11, and compare that to the values in 11 to 12, and keep doing that until they are close enough to the same.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;ode&lt;/span&gt;(t, C):
    &lt;span style="color: #BA36A5;"&gt;Ca&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;Cb&lt;/span&gt; = C
    &lt;span style="color: #BA36A5;"&gt;dCadt&lt;/span&gt; = -0.2 * Ca + 0.3 * Cb
    &lt;span style="color: #BA36A5;"&gt;dCbdt&lt;/span&gt; = -0.3 * Cb + 0.2 * Ca
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; dCadt, dCbdt

&lt;span style="color: #BA36A5;"&gt;tspan&lt;/span&gt; = (0, 20)

&lt;span style="color: #0000FF;"&gt;from&lt;/span&gt; scipy.integrate &lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; solve_ivp
&lt;span style="color: #BA36A5;"&gt;sol&lt;/span&gt; = solve_ivp(ode, tspan, (1, 0))

&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; plt
plt.plot(sol.t, sol.y.T)
plt.xlabel(&lt;span style="color: #008000;"&gt;'t (min)'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'C'&lt;/span&gt;)
plt.legend([&lt;span style="color: #008000;"&gt;'A'&lt;/span&gt;, &lt;span style="color: #008000;"&gt;'B'&lt;/span&gt;]);
sol.y.T[-1]
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
array([0.60003278, 0.39996722])
&lt;/pre&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/f3c33e97d249f9a4832ababa88b2ee4e697c9cad.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;


&lt;p&gt;
It is not crucial to use a class here; you could also use global variables, or function attributes. A class is a standard way of encapsulating state though. We just have to make the class callable so it acts like a function when we need it to.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;class&lt;/span&gt; &lt;span style="color: #6434A3;"&gt;ReachedSteadyState&lt;/span&gt;:        
    &lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;__init__&lt;/span&gt;(&lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;, tolerance=0.01):
        &lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.tolerance = tolerance
        &lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.last_solution = &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;
        &lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.count = 0

    &lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;__str__&lt;/span&gt;(&lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;):
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; &lt;span style="color: #008000;"&gt;'ReachedSteadyState'&lt;/span&gt;

    &lt;span style="color: #6434A3;"&gt;@check_result&lt;/span&gt;
    &lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;__call__&lt;/span&gt;(&lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;, args, sol):
        &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; &lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.last_solution &lt;span style="color: #0000FF;"&gt;is&lt;/span&gt; &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;:
            &lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.last_solution = sol
            &lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.count += 1
            args[&lt;span style="color: #008000;"&gt;'C0'&lt;/span&gt;] = sol.y.T[-1]
            &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; args

        &lt;span style="color: #8D8D84;"&gt;# &lt;/span&gt;&lt;span style="color: #8D8D84; font-style: italic;"&gt;we have a previous solution&lt;/span&gt;
        &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; &lt;span style="color: #0000FF;"&gt;not&lt;/span&gt; np.allclose(&lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.last_solution.y.mean(axis=1),
                           sol.y.mean(axis=1),
                           rtol=&lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.tolerance,
                           atol=&lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.tolerance):
            &lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.last_solution = sol
            &lt;span style="color: #0000FF;"&gt;self&lt;/span&gt;.count += 1
            args[&lt;span style="color: #008000;"&gt;'C0'&lt;/span&gt;] = sol.y.T[-1]
            &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; args

rss = ReachedSteadyState(0.0001)

&lt;span style="color: #6434A3;"&gt;@manager&lt;/span&gt;(checkers=[rss], max_errors=20, verbose=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)        
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;get_sol&lt;/span&gt;(C0=(1, 0), window=1):
    sol = solve_ivp(ode, t_span=(0, window), y0=C0)
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; sol

sol = get_sol((1, 0), window=2)
sol
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
Proposed fix in ReachedSteadyState: {'C0': array([0.74716948, 0.25283052]), 'window': 2}
Proposed fix in ReachedSteadyState: {'C0': array([0.65414484, 0.34585516]), 'window': 2}
Proposed fix in ReachedSteadyState: {'C0': array([0.61992776, 0.38007224]), 'window': 2}
Proposed fix in ReachedSteadyState: {'C0': array([0.60733496, 0.39266504]), 'window': 2}
Proposed fix in ReachedSteadyState: {'C0': array([0.60269957, 0.39730043]), 'window': 2}
Proposed fix in ReachedSteadyState: {'C0': array([0.60099346, 0.39900654]), 'window': 2}
Proposed fix in ReachedSteadyState: {'C0': array([0.60036557, 0.39963443]), 'window': 2}
Proposed fix in ReachedSteadyState: {'C0': array([0.60013451, 0.39986549]), 'window': 2}
Proposed fix in ReachedSteadyState: {'C0': array([0.60004949, 0.39995051]), 'window': 2}
&lt;/p&gt;
&lt;pre class="example" id="orgee266a9"&gt;
  message: The solver successfully reached the end of the integration interval.
  success: True
   status: 0
        t: [ 0.000e+00  7.179e-01  2.000e+00]
        y: [[ 6.000e-01  6.000e-01  6.000e-01]
            [ 4.000e-01  4.000e-01  4.000e-01]]
      sol: None
 t_events: None
 y_events: None
     nfev: 14
     njev: 0
      nlu: 0
&lt;/pre&gt;

&lt;p&gt;
We can plot the two solutions to see how different they are. This shows they are close.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style="color: #0000FF;"&gt;as&lt;/span&gt; plt
plt.plot(rss.last_solution.t, rss.last_solution.y.T, label=[&lt;span style="color: #008000;"&gt;'A previous'&lt;/span&gt; ,&lt;span style="color: #008000;"&gt;'B previous'&lt;/span&gt;])
plt.plot(sol.t, sol.y.T, &lt;span style="color: #008000;"&gt;'--'&lt;/span&gt;, label=[&lt;span style="color: #008000;"&gt;'A current'&lt;/span&gt;, &lt;span style="color: #008000;"&gt;'B current'&lt;/span&gt;])
plt.legend()
plt.xlabel(&lt;span style="color: #008000;"&gt;'relative t'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'C'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/3c544cf4265650554cef24240a0c6272dcc8fdae.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
Those look pretty similar on this graph.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org5218dd5" class="outline-2"&gt;
&lt;h2 id="org5218dd5"&gt;&lt;span class="section-number-2"&gt;3.&lt;/span&gt; Handling exceptions&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
Suppose you have a function that randomly fails. This could be because something does not converge with a randomly chosen initial guess, converges to an unphysical answer, etc. In these cases, it makes sense to simply try again with a new initial guess.
&lt;/p&gt;

&lt;p&gt;
For this example, say we have this objective function with two minima. We will say that any solution above 0.5 is unphysical.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;f&lt;/span&gt;(x):
    &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; -(np.exp(-50 * (x - 0.25)**2) + 0.5 * np.exp(-100 * (x - 0.75)**2))


&lt;span style="color: #BA36A5;"&gt;x&lt;/span&gt; = np.linspace(0, 1)
plt.plot(x, f(x))
plt.xlabel(&lt;span style="color: #008000;"&gt;'x'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'y'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/1749ee4492947f204b2e25cc2f9059edd2929869.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
Here we define a function that takes a guess, and gets a solution. If the solution is unphysical, we raise an exception. We define a custom exception so we can handle it specifically.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF;"&gt;class&lt;/span&gt; &lt;span style="color: #6434A3;"&gt;UnphysicalSolution&lt;/span&gt;(&lt;span style="color: #6434A3;"&gt;Exception&lt;/span&gt;):
    &lt;span style="color: #0000FF;"&gt;pass&lt;/span&gt;

&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;get_minima&lt;/span&gt;(guess):
    &lt;span style="color: #BA36A5;"&gt;sol&lt;/span&gt; = minimize(f, guess)

    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; sol.x &amp;gt; 0.5:
        &lt;span style="color: #0000FF;"&gt;raise&lt;/span&gt; UnphysicalSolution
    &lt;span style="color: #0000FF;"&gt;else&lt;/span&gt;:
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; sol

&lt;span style="color: #6434A3;"&gt;@check_exception&lt;/span&gt;
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;try_again&lt;/span&gt;(args, exc):
    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; &lt;span style="color: #006FE0;"&gt;isinstance&lt;/span&gt;(exc, UnphysicalSolution):
        args[&lt;span style="color: #008000;"&gt;'guess'&lt;/span&gt;] = np.random.random()
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; args
  
&lt;span style="color: #6434A3;"&gt;@manager&lt;/span&gt;(checkers=(try_again,), verbose=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)    
&lt;span style="color: #0000FF;"&gt;def&lt;/span&gt; &lt;span style="color: #006699;"&gt;get_minima&lt;/span&gt;(guess):
    sol = minimize(f, guess)

    &lt;span style="color: #0000FF;"&gt;if&lt;/span&gt; sol.x &amp;gt; 0.5:
        &lt;span style="color: #0000FF;"&gt;raise&lt;/span&gt; UnphysicalSolution
    &lt;span style="color: #0000FF;"&gt;else&lt;/span&gt;:
        &lt;span style="color: #0000FF;"&gt;return&lt;/span&gt; sol

get_minima(np.random.random())
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example" id="org6d987ce"&gt;
Proposed fix in wrapper: {'guess': 0.03789731690063758}
  message: Optimization terminated successfully.
  success: True
   status: 0
      fun: -1.0000000000069411
        x: [ 2.500e-01]
      nit: 4
      jac: [ 0.000e+00]
 hess_inv: [[ 1.000e-02]]
     nfev: 18
     njev: 9
&lt;/pre&gt;


&lt;p&gt;
You can see it took four iterations to find a solution. Other times it might take zero or one, or maybe more, it depends on where the guesses fall.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-org677186e" class="outline-2"&gt;
&lt;h2 id="org677186e"&gt;&lt;span class="section-number-2"&gt;4.&lt;/span&gt; Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
This solution works as well as &lt;code&gt;supervisor&lt;/code&gt; did. It was a little deeper rabbit hole to go down, mostly because of some subtlety in making the result and exception decorators work for both functions and class methods. I think it is more robust now, as it should not matter how you call the function, and any combination of args and kwargs should be working.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copyright (C) 2023 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2023/09/21/A-better-manager-for-supervising-Python-functions.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.7-pre&lt;/p&gt;]]></content>
  </entry>
</feed>
