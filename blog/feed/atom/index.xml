<?xml version="1.0" encoding="UTF-8"?>

<feed
  xmlns="http://www.w3.org/2005/Atom"
  xmlns:thr="http://purl.org/syndication/thread/1.0"
  xml:lang="en"
  >
  <title type="text">The Kitchin Research Group</title>
  <subtitle type="text">Chemical Engineering at Carnegie Mellon University</subtitle>

  <updated>2025-07-09T19:46:15Z</updated>
  <generator uri="http://blogofile.com/">Blogofile</generator>

  <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog" />
  <id>https://kitchingroup.cheme.cmu.edu/blog/feed/atom/</id>
  <link rel="self" type="application/atom+xml" href="https://kitchingroup.cheme.cmu.edu/blog/feed/atom/" />
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Hyperplane decision trees as piecewise linear surrogate models for chemical process design]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/07/09/New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/07/09/New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design</id>
    <updated>2025-07-09T14:52:57Z</updated>
    <published>2025-07-09T14:52:57Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New publication - Hyperplane decision trees as piecewise linear surrogate models for chemical process design]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/07/09/New-publication-Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design"><![CDATA[


&lt;p&gt;
We’ve developed a new kind of decision-tree model that’s both smart and practical for tackling tough engineering problems. First, we take raw data and "lift" it into a richer feature space so we can slice it more cleverly including angular shapes. Next, we grow a friendly “hyperplane” tree that splits data along these angled cuts, fitting simple linear models in each branch. The result is a piecewise-linear surrogate that behaves a lot like the real system but runs orders of magnitude faster. Finally, because each piece is just a linear model, we can plug the whole thing straight into an optimizer that finds the very best solution under complex rules. That means we can design chemical processes, heat exchangers, or any engineering system more reliably and sustainably - saving time, energy, and cost.
&lt;/p&gt;


&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-09-07-2025-time-14-19-14.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;



&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;sunshine-2025-hyper-decis&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Ethan M. Sunshine and Carolina Colombo Tedesco and Sneha A.
                  Akhade and Matthew J. McNenly and John R. Kitchin and Carl D.
                  Laird},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Hyperplane Decision Trees As Piecewise Linear Surrogate Models
                  for Chemical Process Design},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Computers \&amp;amp; Chemical Engineering},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        109204,
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1016/j.compchemeng.2025.109204&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1016/j.compchemeng.2025.109204&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Wed Jul 9 14:14:17 2025},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1016/j.compchemeng.2025.109204'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/07/09/New-publication---Hyperplane-decision-trees-as-piecewise-linear-surrogate-models-for-chemical-process-design.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[Lies, damn lies, statistics and Bayesian statistics]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics</id>
    <updated>2025-06-23T13:33:00Z</updated>
    <published>2025-06-22T11:14:23Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="machine-learning" />
    <summary type="html"><![CDATA[Lies, damn lies, statistics and Bayesian statistics]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/06/22/Lies-damn-lies-statistics-and-Bayesian-statistics"><![CDATA[


&lt;div id="table-of-contents" role="doc-toc"&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;div id="text-table-of-contents" role="doc-toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#org035da70"&gt;1. The data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#org28ad175"&gt;2. GPR with a RBF kernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orga7fb619"&gt;3. a better kernel solves these issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orgca7200a"&gt;4. How about with feature engineering?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#orgdd16d18"&gt;5. Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
This post on LinkedIn (&lt;a href="https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI"&gt;https://www.linkedin.com/posts/activity-7341134401705041920-gaEd?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAfqmO0BzyXpJw8w7yyHwkoMSiaKfGg-sKI&lt;/a&gt;) reminded me of a quip I often make of "Lies, damn lies, statistics, and Bayesian statistics". I am frequently skeptical of claims about "Bayesian something something", especially when the claim is about uncertainty quantification. That skepticism comes from practical experience of mine that "Bayesian something something" is rarely as well behaved and informative as advertised (in my hands of course).
&lt;/p&gt;

&lt;p&gt;
To illustrate, I will use some noisy, 1d data from a Lennard-Jones function and Gaussian process regression to fit the data.
&lt;/p&gt;
&lt;div id="outline-container-org035da70" class="outline-2"&gt;
&lt;h2 id="org035da70"&gt;&lt;span class="section-number-2"&gt;1.&lt;/span&gt; The data&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
We get our data by sampling a Lennard-Jones function, adding some noise, and removing a gap in the data. The gap in the middle might be classically considered an interpolation region.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; numpy &lt;span style="color: #0000FF; font-weight: bold;"&gt;as&lt;/span&gt; np
&lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style="color: #0000FF; font-weight: bold;"&gt;as&lt;/span&gt; plt

&lt;span style="color: #BA36A5;"&gt;r&lt;/span&gt; = np.linspace(0.95, 3, 200)

&lt;span style="color: #BA36A5;"&gt;eps&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;sig&lt;/span&gt; = 1, 1
&lt;span style="color: #BA36A5;"&gt;y&lt;/span&gt; = 4 * eps * ((1 / r)**12 - (1 / r)**6) + np.random.normal(0, 0.03, size=r.shape)


&lt;span style="color: #BA36A5;"&gt;ind&lt;/span&gt; = ((r &amp;gt; 1) &amp;amp; (r &amp;lt; 1.25)) | ((r &amp;gt; 2) &amp;amp; (r &amp;lt; 2.5))
&lt;span style="color: #BA36A5;"&gt;_R&lt;/span&gt; = r[ind][:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]
&lt;span style="color: #BA36A5;"&gt;_y&lt;/span&gt; = y[ind]
plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)
plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/653165863df7654b10ddaca2f7645560768bd870.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-org28ad175" class="outline-2"&gt;
&lt;h2 id="org28ad175"&gt;&lt;span class="section-number-2"&gt;2.&lt;/span&gt; GPR with a RBF kernel&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
The RBF kernel is the most standard kernel. It does an ok job fitting here, although I see evidence of overfitting (the wiggles are caused by the noise). You can reduce the overfitting by using a larger &lt;code&gt;alpha&lt;/code&gt; value in the gpr, but that requires you to know in advance how smooth it should be.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF; font-weight: bold;"&gt;from&lt;/span&gt; sklearn.gaussian_process &lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; GaussianProcessRegressor
&lt;span style="color: #0000FF; font-weight: bold;"&gt;from&lt;/span&gt; sklearn.gaussian_process.kernels &lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; RBF, WhiteKernel
&lt;span style="color: #BA36A5;"&gt;kernel&lt;/span&gt; = RBF() + WhiteKernel()
&lt;span style="color: #BA36A5;"&gt;gpr&lt;/span&gt; = GaussianProcessRegressor(kernel=kernel,
                               random_state=0, normalize_y=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;).fit(_R, _y)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;)
plt.plot(r, y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;, alpha=0.2)

&lt;span style="color: #BA36A5;"&gt;yp&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;se&lt;/span&gt; = gpr.predict(r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;], return_std=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;, r, yp - 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;);
plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)
plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);

gpr.kernel_
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
RBF(length_scale=0.0948) + WhiteKernel(noise_level=0.00635)
&lt;/pre&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/6acc7dccc37ee773aeb4c97c62929401733a02f6.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
The uncertainty here is primarily related to the model, i.e. it is constrained to be correct where there is data, but with no data, the model is not the right one.
&lt;/p&gt;

&lt;p&gt;
The model does well in the region where there is data, but is qualitatively wrong in the gap (even though classically this would be considered interpolation), and overestimates the uncertainty in this region. The problem is the covariance kernel decays to 0 about two length scales away from the last point, which means there is no data to inform what the weights in that region should look like.  That causes the model to revert to the mean of the data.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;gpr.predict([[1.8]]), gpr.predict([[3.0]]), np.mean(_y)
&lt;/pre&gt;
&lt;/div&gt;

&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col  class="org-left" /&gt;

&lt;col  class="org-left" /&gt;

&lt;col  class="org-left" /&gt;

&lt;col  class="org-left" /&gt;

&lt;col  class="org-right" /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="org-left"&gt;array&lt;/td&gt;
&lt;td class="org-left"&gt;((-0.2452041))&lt;/td&gt;
&lt;td class="org-left"&gt;array&lt;/td&gt;
&lt;td class="org-left"&gt;((-0.29363654))&lt;/td&gt;
&lt;td class="org-right"&gt;-0.2936364964541409&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;
Why is this happening? It is not that tricky. You can think of the GP as an expansion of the data in basis functions. The kernel trick effectively makes this expansion in the infinite limit. What are those basis functions? We can draw samples of them, which we show here. You can see that where there is no data, the basis functions are "wiggly". That means they are simply not good at making predictions here.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;y_samples&lt;/span&gt; = gpr.sample_y(r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;], n_samples=15, random_state=0)

plt.plot(r, yp)
plt.plot(r, yp + 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;, r, yp - 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;);
plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)

plt.plot(r, y_samples, &lt;span style="color: #008000;"&gt;'k'&lt;/span&gt;, alpha=0.2);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/cff4cfa5cacedcef6ecde8ec2b63dcee659949fb.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;


&lt;p&gt;
This kernel simply cannot be used for extrapolation, or any predictions more than about two length scales away from the nearest point. Calling it Bayesian doesn't make it better. For similar reasons, this model will not work well outside the data range.
&lt;/p&gt;

&lt;p&gt;
A practical person would still consider using this model, and might even rely on the uncertainty being too large to identify regions of low reliability.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orga7fb619" class="outline-2"&gt;
&lt;h2 id="orga7fb619"&gt;&lt;span class="section-number-2"&gt;3.&lt;/span&gt; a better kernel solves these issues&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
Not all is lost, if we know more. In this case we can construct a kernel that reflects our understanding that the data came from a Lennard Jones like interaction model. You can construct kernels by adding and multiplying kernels. Here we consider a linear kernel, the &lt;code&gt;DotProduct&lt;/code&gt; kernel, and construct a new kernel that is a sum of the linear kernel to the 12&lt;sup&gt;th&lt;/sup&gt; power, a linear kernel to the 6&lt;sup&gt;th&lt;/sup&gt; power, and a &lt;code&gt;WhiteKernel&lt;/code&gt; for the noise. It is a little subtle that this kernel should work in \(1 / r\) space, so in addition to kernel engineering, we also do feature engineering.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #0000FF; font-weight: bold;"&gt;from&lt;/span&gt; sklearn.gaussian_process.kernels &lt;span style="color: #0000FF; font-weight: bold;"&gt;import&lt;/span&gt; DotProduct

&lt;span style="color: #BA36A5;"&gt;kernel&lt;/span&gt; = DotProduct()**12 + DotProduct()**6 +  WhiteKernel()
&lt;span style="color: #BA36A5;"&gt;gpr&lt;/span&gt; = GaussianProcessRegressor(kernel=kernel, normalize_y=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;).fit(1 / _R, _y)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;)
plt.plot(r, y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;, alpha=0.2)


&lt;span style="color: #BA36A5;"&gt;yp&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;se&lt;/span&gt; = gpr.predict(1 / r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;], return_std=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;, r, yp - 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);

gpr.kernel_
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
DotProduct(sigma_0=0.0281) ** 12 + DotProduct(sigma_0=0.936) ** 6 + WhiteKernel(noise_level=0.0077)
&lt;/pre&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/70e91f8419a473ed578a14442694e67a3409bd1e.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
Note that this GPR does fine in the gap, including the right level of uncertainty there. This model is better because we used the kernel to constrain what forms the model can have. This model actually extrapolates correctly outside the data. It is worth noting that although this model has great predictive and UQ properties, it does not tell us anything about the values of &amp;epsilon; and &amp;sigma; in the Lennard Jones model. Although we might say the kernel is physics-based, i.e. it is based on the relevant features and equation, it does not have physical parameters in it.
&lt;/p&gt;

&lt;p&gt;
How about those basis functions here? You can see that all of them basically look like the LJ potential. That means they are good basis functions to expand this data set in.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;y_samples&lt;/span&gt; = gpr.sample_y(1 / r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;], n_samples=15, random_state=0)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)

plt.plot(r, y_samples, &lt;span style="color: #008000;"&gt;'k'&lt;/span&gt;, alpha=0.2);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/e7fe34a01c52cb228cbbcde85e5f334e7f8237a1.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgca7200a" class="outline-2"&gt;
&lt;h2 id="orgca7200a"&gt;&lt;span class="section-number-2"&gt;4.&lt;/span&gt; How about with feature engineering?&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
Can we do even better with feature engineering here? Motivated by &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7342573774774386688?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7342573774774386688%2C7342949865590530052%29&amp;amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287342949865590530052%2Curn%3Ali%3Aactivity%3A7342573774774386688%29"&gt;this comment&lt;/a&gt; by Cory Simon, we cast the problem as a linear regression in [1 / r&lt;sup&gt;6&lt;/sup&gt;, 1 / r&lt;sup&gt;12&lt;/sup&gt;] feature space. This is also a perfectly reasonable thing to do. Since our output is linear in these features, we simply use a linear kernel (aka the DotProduct kernel in sklearn).
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;r6&lt;/span&gt; = 1 / _R**6
&lt;span style="color: #BA36A5;"&gt;r12&lt;/span&gt; = r6**2

&lt;span style="color: #BA36A5;"&gt;kernel&lt;/span&gt; = DotProduct() + WhiteKernel()

&lt;span style="color: #BA36A5;"&gt;gpr&lt;/span&gt; = GaussianProcessRegressor(kernel=kernel, normalize_y=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;).fit(np.hstack([r6, r12]), _y)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;)
plt.plot(r, y, &lt;span style="color: #008000;"&gt;'b.'&lt;/span&gt;, alpha=0.2)

&lt;span style="color: #BA36A5;"&gt;fr6&lt;/span&gt; = 1 / r[:, &lt;span style="color: #D0372D;"&gt;None&lt;/span&gt;]**6
&lt;span style="color: #BA36A5;"&gt;fr12&lt;/span&gt; = fr6**2

&lt;span style="color: #BA36A5;"&gt;yp&lt;/span&gt;, &lt;span style="color: #BA36A5;"&gt;se&lt;/span&gt; = gpr.predict(np.hstack([fr6, fr12]), return_std=&lt;span style="color: #D0372D;"&gt;True&lt;/span&gt;)
plt.plot(r, yp)
plt.plot(r, yp + 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;, r, yp - 2 * se, &lt;span style="color: #008000;"&gt;'k--'&lt;/span&gt;);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);

gpr.kernel_
&lt;/pre&gt;
&lt;/div&gt;

&lt;pre class="example"&gt;
DotProduct(sigma_0=0.74) + WhiteKernel(noise_level=0.00654)
&lt;/pre&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/d8769fe652b9e902e3d349ce26cdbd7d8050b190.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;p&gt;
We can't easily plot these basis functions the same way, so we reduce them to a 1-d plot. You can see here that these basis functions practically the same as the one with the advanced kernel.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-jupyter-python"&gt;&lt;span style="color: #BA36A5;"&gt;y_samples&lt;/span&gt; = gpr.sample_y(np.hstack([fr6, fr12]),
                         n_samples=15, random_state=0)

plt.plot(_R, _y, &lt;span style="color: #008000;"&gt;'.'&lt;/span&gt;)

plt.plot(r, y_samples, &lt;span style="color: #008000;"&gt;'k'&lt;/span&gt;, alpha=0.2);

plt.xlabel(&lt;span style="color: #008000;"&gt;'R'&lt;/span&gt;)
plt.ylabel(&lt;span style="color: #008000;"&gt;'E'&lt;/span&gt;);
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/f777586bb8e17bac5ca3dadbfba97119addeb46b.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;



&lt;p&gt;
This also works quite well, and is another way to leverage knowledge about what we are building a model for.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-orgdd16d18" class="outline-2"&gt;
&lt;h2 id="orgdd16d18"&gt;&lt;span class="section-number-2"&gt;5.&lt;/span&gt; Summary&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
Naive use of GPR can provide useful models when you have enough data, but these models likely do not accurately capture uncertainty outside that data, nor is it likely they are reliable in extrapolation. It is possible to do better than this, when you know what to do. Through feature and kernel engineering, you can sometimes create situations where the problem essentially becomes linear regression, where a simple linear kernel is what you want, or you develop a kernel that represents the underlying model. Kernel engineering is generally hard, with limited opportunities to be flexible. See &lt;a href="https://www.cs.toronto.edu/~duvenaud/cookbook/"&gt;https://www.cs.toronto.edu/~duvenaud/cookbook/&lt;/a&gt; for examples of kernels and combining them.
&lt;/p&gt;

&lt;p&gt;
You can see it is not adequate to say "we used Gaussian process regression". That is about as informative as saying linear regression without identifying the features, or nonlinear regression and not saying what model. You have to be specific about the kernel, and thoughtful about how you know if a prediction is reliable or not. Just because you get an uncertainty prediction doesn't mean its right.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/06/22/Lies,-damn-lies,-statistics-and-Bayesian-statistics.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New Publication - Solving an inverse problem with generative models]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/06/17/New-Publication-Solving-an-inverse-problem-with-generative-models" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/06/17/New-Publication-Solving-an-inverse-problem-with-generative-models</id>
    <updated>2025-06-17T13:24:43Z</updated>
    <published>2025-06-17T13:24:43Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New Publication - Solving an inverse problem with generative models]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/06/17/New-Publication-Solving-an-inverse-problem-with-generative-models"><![CDATA[


&lt;p&gt;
Inverse problems—where we aim to find inputs that produce a desired output—are notoriously challenging in science and engineering. In this study, I explore how generative AI models can tackle these problems by comparing four approaches: a forward model combined with nonlinear optimization, a backward model using partial least squares regression, and two generative methods based on Gaussian mixture models and diffusion-based flow transformations. Using data from a simple RGB-controlled light sensor, the paper demonstrates that generative models can accurately and flexibly infer input settings for target outputs, with advantages such as uncertainty quantification and the ability to condition on partial outputs. This work showcases the promise of generative modeling in reshaping how we approach inverse problems across disciplines.
&lt;/p&gt;


&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/zx6FHzx8V-Y?si=1OQBQ25Ze8e5mzZl" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;kitchin-2025-solvin-inver&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       "Kitchin, John R.",
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Solving an Inverse Problem With Generative Models},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      "Digital Discovery",
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        "-",
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          "&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1039/D5DD00137D&lt;/span&gt;",
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          "&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1039/D5DD00137D&lt;/span&gt;",
  &lt;span style="color: #BA36A5;"&gt;abstract&lt;/span&gt; =     "Inverse problems{,} where we seek the values of inputs to a
                  model that lead to a desired set of outputs{,} are a
                  challenges subset of problems in science and engineering. In
                  this work we demonstrate the use of two generative AI methods
                  to solve inverse problems. We compare this approach to two
                  more conventional approaches that use a forward model with
                  nonlinear programming{,} and the use of a backward model. We
                  illustrate each method on a dataset obtained from a simple
                  remote instrument that has three inputs: the setting of the
                  red{,} green and blue channels of an RGB LED. We focus on
                  several outputs from a light sensor that measures intensity at
                  445 nm{,} 515 nm{,} 590 nm{,} and 630 nm. The speci&amp;#64257;c problem
                  we solve is identifying inputs that lead to a speci&amp;#64257;c
                  intensity in three of those channels. We show that generative
                  models can be used to solve this kind of inverse problem{,}
                  and they have some advantages over the conventional
                  approaches.",
  &lt;span style="color: #BA36A5;"&gt;publisher&lt;/span&gt; =    "RSC",
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1039/D5DD00137D'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/06/17/New-Publication---Solving-an-inverse-problem-with-generative-models.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - The Evolving Role of Programming and Llms in the Development of Self-Driving Laboratories]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/05/07/New-publication-The-Evolving-Role-of-Programming-and-Llms-in-the-Development-of-Self-Driving-Laboratories" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/05/07/New-publication-The-Evolving-Role-of-Programming-and-Llms-in-the-Development-of-Self-Driving-Laboratories</id>
    <updated>2025-05-07T07:00:01Z</updated>
    <published>2025-05-07T06:58:27Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New publication - The Evolving Role of Programming and Llms in the Development of Self-Driving Laboratories]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/05/07/New-publication-The-Evolving-Role-of-Programming-and-Llms-in-the-Development-of-Self-Driving-Laboratories"><![CDATA[


&lt;p&gt;
In this paper, I introduce Claude-Light, a lightweight self-driving lab prototype built on a Raspberry Pi with an RGB LED and ten-channel photometer, all accessible via a simple REST API and Python library. By demonstrating structured automation—from basic scripting and statistical design of experiments through Gaussian process active learning—and exploring large language models for instrument selection, structured data extraction, function calling, and code generation, I showcase both the opportunities and challenges LLMs bring to lab automation (reproducibility, security, and reliability). Claude-Light lowers the barrier for students and researchers to prototype and test automation and AI-driven experimentation before scaling to full self-driving laboratories.
&lt;/p&gt;

&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-05-05-2025-time-12-10-14.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;

&lt;pre class="example" id="org049f42f"&gt;
@article{kitchin-2025-evolv-role,
  author =	 {John R. Kitchin},
  title =	 {The Evolving Role of Programming and LLMs in the Development
                  of Self-Driving Laboratories},
  journal =	 {APL Machine Learning},
  volume =	 3,
  number =	 2,
  pages =	 {026111},
  year =	 2025,
  doi =		 {10.1063/5.0266757},
  url =		 {http://dx.doi.org/10.1063/5.0266757},
  DATE_ADDED =	 {Thu May 1 09:22:44 2025},
}
&lt;/pre&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1063/5.0266757'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/05/07/New-publication---The-Evolving-Role-of-Programming-and-Llms-in-the-Development-of-Self-Driving-Laboratories.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - A Classification-based Methodology for the Estimation of Binary Surfactant Critical Micelle Concentrations]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/04/11/New-publication-A-Classification-based-Methodology-for-the-Estimation-of-Binary-Surfactant-Critical-Micelle-Concentrations" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/04/11/New-publication-A-Classification-based-Methodology-for-the-Estimation-of-Binary-Surfactant-Critical-Micelle-Concentrations</id>
    <updated>2025-04-11T09:27:37Z</updated>
    <published>2025-04-11T09:27:37Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New publication - A Classification-based Methodology for the Estimation of Binary Surfactant Critical Micelle Concentrations]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/04/11/New-publication-A-Classification-based-Methodology-for-the-Estimation-of-Binary-Surfactant-Critical-Micelle-Concentrations"><![CDATA[


&lt;p&gt;
In our latest paper, we developed a high-throughput method to efficiently determine the critical micelle concentration (CMC) of binary surfactant mixtures using a 96-well plate setup. Instead of relying on traditional regression techniques, we used a physics-informed classification approach based on regular solution theory to identify the micellization boundary. By combining model-driven experimental design with a dye solubilization assay, we mapped out the CMC across mixture compositions and accurately extracted the binary interaction parameter, β. We validated the method using the SDS-C8E4 system, and extended it to an electrolyte-rich environment, showing less than 15–18% deviation from literature values. This approach not only accelerates formulation screening but also lays the groundwork for analyzing more complex surfactant systems in the future.
&lt;/p&gt;



&lt;p&gt;
&lt;figure&gt;&lt;img src="/media/date-11-04-2025-time-09-10-54.png"&gt;&lt;/figure&gt; 
&lt;/p&gt;



&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;D5DD00058K&lt;/span&gt;,
        &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; = {Chilkunda, Chetan R and Kitchin, John R. and Tilton, Robert D.},
        &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; = {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1039/D5DD00058K&lt;/span&gt;},
        &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; = {Digital Discovery},
        &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; = {-},
        &lt;span style="color: #BA36A5;"&gt;publisher&lt;/span&gt; = {RSC},
        &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; = {A Classification-based Methodology for the Estimation of Binary Surfactant Critical Micelle Concentrations},
        &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; = {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1039/D5DD00058K&lt;/span&gt;},
        &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; = {2025}}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1039/D5DD00058K'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/04/11/New-publication---A-Classification-based-Methodology-for-the-Estimation-of-Binary-Surfactant-Critical-Micelle-Concentrations.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - CatTsunami Accelerating Transition State Energy Calculations With Pretrained Graph Neural Networks]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/03/17/New-publication-CatTsunami-Accelerating-Transition-State-Energy-Calculations-With-Pretrained-Graph-Neural-Networks" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/03/17/New-publication-CatTsunami-Accelerating-Transition-State-Energy-Calculations-With-Pretrained-Graph-Neural-Networks</id>
    <updated>2025-03-17T21:06:55Z</updated>
    <published>2025-03-17T20:58:15Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New publication - CatTsunami Accelerating Transition State Energy Calculations With Pretrained Graph Neural Networks]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/03/17/New-publication-CatTsunami-Accelerating-Transition-State-Energy-Calculations-With-Pretrained-Graph-Neural-Networks"><![CDATA[


&lt;p&gt;
In this work, we tackled the challenge of accelerating catalyst discovery by focusing on transition state energy calculations. We show that a graph neural network potential, despite being trained on a different task, could accurately predict transition states—a crucial step in catalyst discovery—with a remarkable 28x speedup over traditional methods. To provide a benchmark for machine learning model performance in this area, we also curated the Open Catalyst 2020 Nudged Elastic Band (OC20NEB) dataset, which includes 932 DFT nudged elastic band calculations. To showcase the effectiveness of our approach, we applied it to two case studies: reaction mechanism search and ammonia synthesis. These demonstrations highlighted the significant potential of machine learning to enhance and speed up catalyst research.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;wander-2025-catts&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Brook Wander and Muhammed Shuaibi and John R. Kitchin and
                  Zachary W. Ulissi and C. Lawrence Zitnick},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {{CatTsunami}: Accelerating Transition State Energy Calculations
                  With Pretrained Graph Neural Networks},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {ACS Catalysis},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {5283-5294},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1021/acscatal.4c04272&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1021/acscatal.4c04272&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Mon Mar 17 20:50:26 2025},
}
&lt;/pre&gt;
&lt;/div&gt;



&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1021/acscatal.4c04272'&gt;&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/03/17/New-publication---CatTsunami-Accelerating-Transition-State-Energy-Calculations-With-Pretrained-Graph-Neural-Networks.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Accessing Numerical Energy Hessians With Graph Neural Network Potentials and Their Application in Heterogeneous Catalysis]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/02/11/New-publication-Accessing-Numerical-Energy-Hessians-With-Graph-Neural-Network-Potentials-and-Their-Application-in-Heterogeneous-Catalysis" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/02/11/New-publication-Accessing-Numerical-Energy-Hessians-With-Graph-Neural-Network-Potentials-and-Their-Application-in-Heterogeneous-Catalysis</id>
    <updated>2025-02-11T07:48:57Z</updated>
    <published>2025-02-11T07:48:57Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New publication - Accessing Numerical Energy Hessians With Graph Neural Network Potentials and Their Application in Heterogeneous Catalysis]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/02/11/New-publication-Accessing-Numerical-Energy-Hessians-With-Graph-Neural-Network-Potentials-and-Their-Application-in-Heterogeneous-Catalysis"><![CDATA[


&lt;p&gt;
The ability to calculate energy Hessians has long been a cornerstone of understanding chemical reactions, but traditional methods like density functional theory (DFT) are computationally expensive. In this breakthrough study, researchers demonstrate how pretrained Graph Neural Network (GNN) models from the Open Catalyst Project (OCP) can effectively determine potential energy Hessians with remarkable accuracy. With an MAE of just 58 cm⁻¹, these machine-learned potentials enable efficient calculation of Gibbs free energy corrections, overcoming limitations of the harmonic approximation by incorporating translational entropy effects. Even in transition state searches, the models significantly improve convergence rates, making computational catalysis more accessible than ever. This research paves the way for AI-powered simulations to accelerate catalyst discovery and optimize reaction pathways at a fraction of the cost of traditional methods.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;wander-2025-acces-numer&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Wander, Brook and Musielewicz, Joseph and Cheula, Raffaele and
                  Kitchin, John R.},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Accessing Numerical Energy Hessians With Graph Neural Network
                  Potentials and Their Application in Heterogeneous Catalysis},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {The Journal of Physical Chemistry C},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       0,
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       0,
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {null},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1021/acs.jpcc.4c07477&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;URL&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;https://doi.org/10.1021/acs.jpcc.4c07477&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;eprint&lt;/span&gt; =       { https://doi.org/10.1021/acs.jpcc.4c07477 },
}
&lt;/pre&gt;
&lt;/div&gt;


&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1021/acs.jpcc.4c07477'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe title="Embed Player" src="https://play.libsyn.com/embed/episode/id/35236620/height/192/theme/modern/size/large/thumbnail/yes/custom-color/008080/time-start/00:00:00/hide-show/yes/hide-playlist/yes/hide-subscribe/yes/hide-share/yes/font-color/ffffff" height="192" width="100%" scrolling="no" allowfullscreen="" webkitallowfullscreen="true" mozallowfullscreen="true" oallowfullscreen="true" msallowfullscreen="true" style="border: none;"&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/02/11/New-publication---Accessing-Numerical-Energy-Hessians-With-Graph-Neural-Network-Potentials-and-Their-Application-in-Heterogeneous-Catalysis.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Beyond the Fourth Paradigm of Modeling in Chemical Engineering]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/02/05/New-publication-Beyond-the-Fourth-Paradigm-of-Modeling-in-Chemical-Engineering" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/02/05/New-publication-Beyond-the-Fourth-Paradigm-of-Modeling-in-Chemical-Engineering</id>
    <updated>2025-02-05T07:16:31Z</updated>
    <published>2025-02-05T07:16:31Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New publication - Beyond the Fourth Paradigm of Modeling in Chemical Engineering]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/02/05/New-publication-Beyond-the-Fourth-Paradigm-of-Modeling-in-Chemical-Engineering"><![CDATA[


&lt;p&gt;
Chemical engineering has undergone multiple modeling revolutions—from empirical correlations and manual analytical methods to computational techniques and, more recently, machine learning (ML). This article argues that we are now on the verge of a new era driven by differentiable programming, which merges domain knowledge with data-driven models in ways that can transform research, education, and industry.
&lt;/p&gt;

&lt;p&gt;
Differentiable programming, powered by automatic differentiation (AD), allows models to compute derivatives efficiently, making optimization, uncertainty quantification, and hybrid physics-informed ML models more accessible. This capability enables engineers to integrate physical principles with data-driven methods, moving beyond “black-box” ML models. The article highlights the role of AD in process modeling, molecular simulations, and control systems, emphasizing its potential to refine chemical engineering calculations beyond traditional regression methods.
&lt;/p&gt;

&lt;p&gt;
However, leveraging these tools requires a workforce skilled in ML, data science, and automation. The paper calls for educational reform, urging chemical engineering programs to integrate these technologies into curricula to prepare future engineers for this computational revolution.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;kitchin-2025-beyon-fourt&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {John R. Kitchin and Victor Alves and Carl D. Laird},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Beyond the Fourth Paradigm of Modeling in Chemical
                  Engineering},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Nature Chemical Engineering},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {nil},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1038/s44286-024-00170-x&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1038/s44286-024-00170-x&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Tue Jan 28 15:30:08 2025},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1038/s44286-024-00170-x'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe title="Embed Player" src="https://play.libsyn.com/embed/episode/id/34957390/height/192/theme/modern/size/large/thumbnail/yes/custom-color/008080/time-start/00:00:00/hide-show/yes/hide-playlist/yes/hide-subscribe/yes/hide-share/yes/font-color/ffffff" height="192" width="100%" scrolling="no" allowfullscreen="" webkitallowfullscreen="true" mozallowfullscreen="true" oallowfullscreen="true" msallowfullscreen="true" style="border: none;"&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/02/05/New-publication---Beyond-the-Fourth-Paradigm-of-Modeling-in-Chemical-Engineering.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Integrated systems-To-Atoms (S2A) Framework for Designing Resilient and Efficient Hydrogen Infrastructure Solutions]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/02/04/New-publication-Integrated-systems-To-Atoms-S2A-Framework-for-Designing-Resilient-and-Efficient-Hydrogen-Infrastructure-Solutions" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/02/04/New-publication-Integrated-systems-To-Atoms-S2A-Framework-for-Designing-Resilient-and-Efficient-Hydrogen-Infrastructure-Solutions</id>
    <updated>2025-02-04T07:19:06Z</updated>
    <published>2025-02-04T07:19:06Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New publication - Integrated systems-To-Atoms (S2A) Framework for Designing Resilient and Efficient Hydrogen Infrastructure Solutions]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/02/04/New-publication-Integrated-systems-To-Atoms-S2A-Framework-for-Designing-Resilient-and-Efficient-Hydrogen-Infrastructure-Solutions"><![CDATA[


&lt;p&gt;
The future of hydrogen infrastructure demands seamless integration of technology performance, operating conditions, and system configuration. This paper introduces a Systems-to-Atoms (S2A) framework, bridging material, device, and system design to optimize hydrogen storage and transport. A case study on using liquid organic hydrogen carriers (LOHCs) for refueling stations demonstrates that high reaction pressures, while not enhancing catalyst performance, can significantly reduce system costs. Interestingly, less efficient catalysts like copper may be preferred over palladium in certain conditions due to cost and supply chain considerations. The study highlights the importance of cross-scale optimization for resilient, cost-effective hydrogen solutions.
&lt;/p&gt;

&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;yuan-2025-integ-to&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Mengyao Yuan and Giovanna Bucci and Tanusree Chatterjee and
                  Shyam Deo and John R. Kitchin and Carl D. Laird and Wenqin Li
                  and Thomas Moore and Corey Myers and Wenyu Sun and Ethan M.
                  Sunshine and Bo-Xun Wang and Matthew J. McNenly and Sneha A.
                  Akhade},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Integrated &amp;lt;i&amp;gt;systems-To-Atoms&amp;lt;/i&amp;gt; (S2A) Framework for
                  Designing Resilient and Efficient Hydrogen Infrastructure
                  Solutions},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Energy \&amp;amp; Fuels},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;number&lt;/span&gt; =       {nil},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        {nil},
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2025,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1021/acs.energyfuels.4c05903&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1021/acs.energyfuels.4c05903&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Mon Feb 3 15:30:53 2025},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1021/acs.energyfuels.4c05903'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe title="Embed Player" src="https://play.libsyn.com/embed/episode/id/35135455/height/192/theme/modern/size/large/thumbnail/yes/custom-color/008080/time-start/00:00:00/hide-show/yes/hide-playlist/yes/hide-subscribe/yes/hide-share/yes/font-color/ffffff" height="192" width="100%" scrolling="no" allowfullscreen="" webkitallowfullscreen="true" mozallowfullscreen="true" oallowfullscreen="true" msallowfullscreen="true" style="border: none;"&gt;&lt;/iframe&gt;
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/02/04/New-publication---Integrated-systems-To-Atoms-(S2A)-Framework-for-Designing-Resilient-and-Efficient-Hydrogen-Infrastructure-Solutions.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
  <entry>
    <author>
      <name></name>
      <uri>https://kitchingroup.cheme.cmu.edu/blog</uri>
    </author>
    <title type="html"><![CDATA[New publication - Multiscale Optimization of Formic Acid Dehydrogenation Process via Linear Model Decision Tree Surrogates]]></title>
    <link rel="alternate" type="text/html" href="https://kitchingroup.cheme.cmu.edu/blog/2025/02/03/New-publication-Multiscale-Optimization-of-Formic-Acid-Dehydrogenation-Process-via-Linear-Model-Decision-Tree-Surrogates" />
    <id>https://kitchingroup.cheme.cmu.edu/blog/2025/02/03/New-publication-Multiscale-Optimization-of-Formic-Acid-Dehydrogenation-Process-via-Linear-Model-Decision-Tree-Surrogates</id>
    <updated>2025-02-03T06:50:27Z</updated>
    <published>2025-02-03T06:50:27Z</published>
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="news" />
    <category scheme="https://kitchingroup.cheme.cmu.edu/blog" term="publication" />
    <summary type="html"><![CDATA[New publication - Multiscale Optimization of Formic Acid Dehydrogenation Process via Linear Model Decision Tree Surrogates]]></summary>
    <content type="html" xml:base="https://kitchingroup.cheme.cmu.edu/blog/2025/02/03/New-publication-Multiscale-Optimization-of-Formic-Acid-Dehydrogenation-Process-via-Linear-Model-Decision-Tree-Surrogates"><![CDATA[


&lt;p&gt;
In a push to optimize hydrogen storage and release, this study explores the multiscale optimization of formic acid dehydrogenation using linear model decision tree (LMDT) surrogates. This method tackles the challenge of integrating models that operate at different scales—ranging from atomistic catalyst design to reactor operation and techno-economic assessment. By replacing complex physics-based models with data-driven surrogates, the researchers created a framework that enables simultaneous optimization across these scales. The study finds that co-optimizing all three scales together leads to a 40% cost reduction compared to optimizing each part separately. The model also confirms economies of scale in hydrogen production, showing that optimal catalyst choice shifts from platinum to copper as production capacity increases. While the approach simplifies multiscale optimization, its accuracy depends on high-quality training data, and future work may integrate uncertainty quantification to improve reliability. This breakthrough highlights the power of surrogate models in chemical process optimization, paving the way for more cost-effective hydrogen energy solutions.
&lt;/p&gt;


&lt;p&gt;
Ethan M. Sunshine, Giovanna Bucci, Tanusree Chatterjeed, Shyam Deod, Victoria M. Ehlinger, Wenqin Lib, Thomas Moore, Corey Myers, Wenyu Sun, Bo-Xun Wang, Mengyao Yuan, John R. Kitchin, Carl D. Laird, Matthew J. McNenly, Sneha A. Akhade, Multiscale Optimization of Formic Acid Dehydrogenation Process via Linear Model Decision Tree Surrogates, Computers and Chemical Engineering, 194, 108921 (2024) &lt;a href="https://doi.org/10.1016/j.compchemeng.2024.108921"&gt;https://doi.org/10.1016/j.compchemeng.2024.108921&lt;/a&gt;.
&lt;/p&gt;


&lt;div class="org-src-container"&gt;
&lt;pre class="src src-bibtex"&gt;&lt;span style="color: #006699;"&gt;@article&lt;/span&gt;{&lt;span style="color: #D0372D;"&gt;sunshine-2024-multis-optim&lt;/span&gt;,
  &lt;span style="color: #BA36A5;"&gt;author&lt;/span&gt; =       {Ethan M. Sunshine and Giovanna Bucci and Tanusree Chatterjee
                  and Shyam Deo and Victoria M. Ehlinger and Wenqin Li and
                  Thomas Moore and Corey Myers and Wenyu Sun and Bo-Xun Wang and
                  Mengyao Yuan and John R. Kitchin and Carl D. Laird and Matthew
                  J. McNenly and Sneha A. Akhade},
  &lt;span style="color: #BA36A5;"&gt;title&lt;/span&gt; =        {Multiscale Optimization of Formic Acid Dehydrogenation Process
                  Via Linear Model Decision Tree Surrogates},
  &lt;span style="color: #BA36A5;"&gt;journal&lt;/span&gt; =      {Computers &amp;amp;amp; Chemical Engineering},
  &lt;span style="color: #BA36A5;"&gt;volume&lt;/span&gt; =       {194},
  &lt;span style="color: #BA36A5;"&gt;pages&lt;/span&gt; =        108921,
  &lt;span style="color: #BA36A5;"&gt;year&lt;/span&gt; =         2024,
  &lt;span style="color: #BA36A5;"&gt;doi&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;10.1016/j.compchemeng.2024.108921&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;url&lt;/span&gt; =          {&lt;span style="color: #006DAF; text-decoration: underline;"&gt;http://dx.doi.org/10.1016/j.compchemeng.2024.108921&lt;/span&gt;},
  &lt;span style="color: #BA36A5;"&gt;DATE_ADDED&lt;/span&gt; =   {Mon Nov 25 14:10:36 2024},
}
&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;
&lt;script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'&gt;&lt;/script&gt;
&lt;div data-badge-type='medium-donut' class='altmetric-embed' data-badge-details='right' data-doi='10.1016/j.compchemeng.2024.108921'&gt;&lt;/div&gt;
&lt;/p&gt;

&lt;iframe title="Embed Player" src="https://play.libsyn.com/embed/episode/id/35111575/height/192/theme/modern/size/large/thumbnail/yes/custom-color/008080/time-start/00:00:00/hide-show/yes/hide-playlist/yes/hide-subscribe/yes/hide-share/yes/font-color/ffffff" height="192" width="100%" scrolling="no" allowfullscreen="" webkitallowfullscreen="true" mozallowfullscreen="true" oallowfullscreen="true" msallowfullscreen="true" style="border: none;"&gt;&lt;/iframe&gt;replace. Tab to end.
&lt;p&gt;Copyright (C) 2025 by John Kitchin. See the &lt;a href="/copying.html"&gt;License&lt;/a&gt; for information about copying.&lt;p&gt;
&lt;p&gt;&lt;a href="/org/2025/02/03/New-publication---Multiscale-Optimization-of-Formic-Acid-Dehydrogenation-Process-via-Linear-Model-Decision-Tree-Surrogates.org"&gt;org-mode source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Org-mode version = 9.8-pre&lt;/p&gt;]]></content>
  </entry>
</feed>
