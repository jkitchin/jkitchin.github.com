<?xml version="1.0" encoding="UTF-8"?><abstracts-retrieval-response xmlns="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:dn="http://www.elsevier.com/xml/svapi/abstract/dtd" xmlns:ait="http://www.elsevier.com/xml/ani/ait" xmlns:ce="http://www.elsevier.com/xml/ani/common" xmlns:cto="http://www.elsevier.com/xml/cto/dtd" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/2.0/" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><coredata><prism:url>https://api.elsevier.com/content/abstract/scopus_id/85205907710</prism:url><dc:identifier>SCOPUS_ID:85205907710</dc:identifier><eid>2-s2.0-85205907710</eid><prism:doi>10.1063/5.0202647</prism:doi><article-number>095031</article-number><dc:title>Masked pretraining strategy for neural potentials</dc:title><prism:aggregationType>Journal</prism:aggregationType><srctype>j</srctype><subtype>ar</subtype><subtypeDescription>Article</subtypeDescription><citedby-count>0</citedby-count><prism:publicationName>AIP Advances</prism:publicationName><dc:publisher>American Institute of Physics</dc:publisher><source-id>19900193962</source-id><prism:issn>21583226</prism:issn><prism:volume>14</prism:volume><prism:issueIdentifier>9</prism:issueIdentifier><prism:coverDate>2024-09-01</prism:coverDate><openaccess>1</openaccess><openaccessFlag>true</openaccessFlag><dc:creator><author seq="1" auid="59291745800"><ce:initials>Z.</ce:initials><ce:indexed-name>Zhang Z.</ce:indexed-name><ce:surname>Zhang</ce:surname><ce:given-name>Zehua</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Zhang Z.</ce:indexed-name><ce:surname>Zhang</ce:surname><ce:given-name>Zehua</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/59291745800</author-url><affiliation id="60104842" href="https://api.elsevier.com/content/affiliation/affiliation_id/60104842"/></author></dc:creator><dc:description><abstract xmlns="" original="y" xml:lang="eng"><publishercopyright>Â© 2024 Author(s).</publishercopyright><ce:para>We propose a masked pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water and small organic molecule systems. GNNs are pretrained by recovering the spatial information of masked-out atoms from molecules selected with certain ratios and then transferred and fine-tuned on atomic force fields. Through such pretraining, GNNs learn meaningful prior about the structural and underlying physical information of molecule systems that are useful for downstream tasks. With comprehensive experiments and ablation studies, we show that the proposed method improves both the accuracy and convergence speed of GNNs compared to their counterparts trained from scratch or with other pretraining techniques. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.</ce:para></abstract></dc:description><link href="https://api.elsevier.com/content/abstract/scopus_id/85205907710" rel="self"/><link href="https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&amp;scp=85205907710&amp;origin=inward" rel="scopus"/><link href="https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&amp;scp=85205907710&amp;origin=inward" rel="scopus-citedby"/></coredata><affiliation id="60104842" href="https://api.elsevier.com/content/affiliation/affiliation_id/60104842"><affilname>College of Engineering</affilname><affiliation-city>Pittsburgh</affiliation-city><affiliation-country>United States</affiliation-country></affiliation><affiliation id="60136640" href="https://api.elsevier.com/content/affiliation/affiliation_id/60136640"><affilname>School of Computer Science</affilname><affiliation-city>Pittsburgh</affiliation-city><affiliation-country>United States</affiliation-country></affiliation><authors><author seq="1" auid="59291745800"><ce:initials>Z.</ce:initials><ce:indexed-name>Zhang Z.</ce:indexed-name><ce:surname>Zhang</ce:surname><ce:given-name>Zehua</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Zhang Z.</ce:indexed-name><ce:surname>Zhang</ce:surname><ce:given-name>Zehua</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/59291745800</author-url><affiliation id="60104842" href="https://api.elsevier.com/content/affiliation/affiliation_id/60104842"/></author><author seq="2" auid="57375690700"><ce:initials>Z.</ce:initials><ce:indexed-name>Li Z.</ce:indexed-name><ce:surname>Li</ce:surname><ce:given-name>Zijie</ce:given-name><preferred-name><ce:initials>Z.</ce:initials><ce:indexed-name>Li Z.</ce:indexed-name><ce:surname>Li</ce:surname><ce:given-name>Zijie</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/57375690700</author-url><affiliation id="60104842" href="https://api.elsevier.com/content/affiliation/affiliation_id/60104842"/></author><author seq="3" auid="37067199500"><ce:initials>A.</ce:initials><ce:indexed-name>Barati Farimani A.</ce:indexed-name><ce:surname>Barati Farimani</ce:surname><ce:given-name>Amir</ce:given-name><preferred-name><ce:initials>A.</ce:initials><ce:indexed-name>Barati Farimani A.</ce:indexed-name><ce:surname>Barati Farimani</ce:surname><ce:given-name>Amir</ce:given-name></preferred-name><author-url>https://api.elsevier.com/content/author/author_id/37067199500</author-url><affiliation id="60104842" href="https://api.elsevier.com/content/affiliation/affiliation_id/60104842"/><affiliation id="60136640" href="https://api.elsevier.com/content/affiliation/affiliation_id/60136640"/><affiliation id="60104842" href="https://api.elsevier.com/content/affiliation/affiliation_id/60104842"/></author></authors></abstracts-retrieval-response>